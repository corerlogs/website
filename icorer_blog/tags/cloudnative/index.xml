<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cloudnative on 笔迹-工匠之芯</title><link>https://icorer.com/icorer_blog/tags/cloudnative/</link><description>Recent content in cloudnative on 笔迹-工匠之芯</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Fri, 25 Dec 2020 11:28:12 +0800</lastBuildDate><atom:link href="https://icorer.com/icorer_blog/tags/cloudnative/index.xml" rel="self" type="application/rss+xml"/><item><title>TCP长连接在K8S环境下的负载均衡分析</title><link>https://icorer.com/icorer_blog/posts/cloudnative_k8s_tcp_upstream_balance/</link><pubDate>Fri, 25 Dec 2020 11:28:12 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/cloudnative_k8s_tcp_upstream_balance/</guid><description>K8S不支持长连接的负载均衡，所以负载可能不是很均衡。如果你在使用HTTP/2，gRPC, RSockets, AMQP 或者任何长连接场景，你需要考虑客户端负载均衡。
TL;DR: Kubernetes doesn&amp;rsquo;t load balance long-lived connections, and some Pods might receive more requests than others. If you&amp;rsquo;re using HTTP/2, gRPC, RSockets, AMQP or any other long-lived connection such as a database connection, you might want to consider client-side load balancing.
Kubernetes提供了两种方便的抽象来部署应用程序：Services 和 Deployments。 Deployments描述了在任何给定时间应运行哪种类型以及多少个应用程序副本的方法。每个应用程序都部署为Pod，并为其分配了IP地址；另一方面，Services类似于负载平衡器。它们旨在将流量分配给一组Pod。
将Services视为IP地址的集合通常很有用。每次您对Services提出请求时，都会从该列表中选择一个IP地址并将其用作目的地。 如果您有两个应用程序（例如前端和后端），则可以为每个应用程序使用Deployment和Service，然后将它们部署在集群中。 当前端应用发出请求时，不需要知道有多少Pod连接到后端服务；前端应用程序也不知道后端应用程序的各个IP地址。当它想要发出请求时，该请求将发送到IP地址不变的后端服务。 但是该服务的负载平衡策略是什么？
Kubernetes Services中的负载平衡Kubernetes Services不存在，没有进程监听服务的IP地址和端口。
您可以通过访问Kubernetes集群中的任何节点并执行netstat -ntlp来检查是否存在这种情况。
甚至在任何地方都找不到IP地址,Services的IP地址由控制器管理器中的控制平面分配，并存储在数据库etcd中。然后，另一个组件将使用相同的IP地址：kube-proxy。
Kube-proxy读取所有Services的IP地址列表，并在每个节点中写入一组iptables规则。这些规则的意思是：“如果看到此Services IP地址，则改写请求并选择Pod之一作为目的地”。Services IP地址仅用作占位符-这就是为什么没有进程监听IP地址或端口的原因。 iptables是否使用轮询？不，iptables主要用于防火墙，并且其目的不是进行负载平衡。但是，您可以制定一套聪明的规则，使iptables像负载均衡器一样工作。而这正是Kubernetes中发生的事情。
如果您有三个Pod，则kube-proxy编写以下规则：
选择Pod 1作为目的地，可能性为33％。 否则，移至下一条规则 选择Pod 2作为目的地，可能性为50％。 否则，请移至以下规则 选择Pod 3作为目的地（没有可能性） 复合概率是Pod 1，Pod 2和Pod 3都有三分之一的机会被选中（33％）。</description></item><item><title>微服务治理：服务遥测之APM-SkyWalking技术应用</title><link>https://icorer.com/icorer_blog/posts/microservice_governance_apm_application/</link><pubDate>Mon, 07 Sep 2020 13:33:12 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/microservice_governance_apm_application/</guid><description>一. 背景描述微服务应用过程中，如何构建微服务的可观测性，主要从以下三个方面进行考虑：
服务日志（log） 服务指标（metric） 服务链路（trace） 这三个服务监控领域有不同的技术栈进行支撑，但是如何快速构建一个基础的服务可观测能力？尽量减少业务的侵入性、尽量多的增加业界标准的观测指标，这里我就推荐APM技术体系，在APM技术领域中SkyWalking是一个优秀的解决方案。 二. 技术结构SkyWalking 在我当前公司的落地领域中，主要围绕PHP、Go两大技术领域，JAVA生态拥有SkyWalking默认友好支撑，针对PHP、Go这两种技术栈，主要包含的APM体系技术结构如下图所示： 从技术结构图可以看出，APM技术体系主要包括以下几部分：
技术结构最底层采用了apache SkyWalking开源项目作为方案支撑。 Go生态使用Go2sky客户端进行APM数据丰富与数据包发送。 PHP生态由于自身短生命期的特征，分为PHP内核APM扩展和数据中转SideCar两部分，PHP内核扩展通过函数Hook机制完成Redis、MySQL、PDO、grpc等关键网络IO的拦截，并无感构建APM数据包结构，在RS周期发送APM数据包到SideCar，SideCar负责流转PHP内核的APM监控数据包到APM-Server上。 三. 关键领域监控APM技术生态包含内容比较多，主要的使命就是对于服务应用进行运行态监控，这里主要阐述一下几方面的监控效果：
3.1 服务指标监控服务指标监控主要包括Apdex、平均响应时间、成功率、CPM、TP数据、也包括很多的服务EndPoint数据，主要用来阐述服务健康、性能、可靠性的指标数据。 3.2 服务调用链监控微服务场景下，调用关系复杂、服务调用关系层级深，所以APM构建了服务调用链监控体系，方面研发、架构对于自己服务的调用关系有较好的可视化效果，调用链也遵循OpenTracing协议，主要效果如下所示： 3.3 微服务内核Runtime监控服务监控除了需要对于服务自身的可靠性、服务之间的调用关系进行监控之外，还需要针对服务Runtime进行拦截分析，通常的实现方式有OAP编程、内核Runtime Hook方式，Runtime监控可以很好的监控服务不同EndPoint内部的关键不稳定点的性能情况，除了PDO、Redis、Mysql、GRPC等关键IO，也可以监控长时间的cpu计算等程序行为逻辑。 主要的效果图如下： 3.4 微服务拓扑关系监控针对微服务调用关系，除了可以使用全链路Trace这种表达形式，也可以通过更具有动感效果的拓扑关系图进行描述，在拓扑关系图中可以形象的显示服务的类别、服务的流量走向、服务的当前状态、服务调用间的频率等数据。相关的效果图如下所示: 三. 总结APM技术体系对于微服务治理工作有超强的观测领域能力的弥补，增强服务的可观测程度，是微服务治理的重要工作。</description></item></channel></rss>