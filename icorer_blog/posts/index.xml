<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on 笔迹-工匠之芯</title><link>https://icorer.com/icorer_blog/posts/</link><description>Recent content in Posts on 笔迹-工匠之芯</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 14 Jun 2022 12:55:18 +0800</lastBuildDate><atom:link href="https://icorer.com/icorer_blog/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>2022 Wanxiang Blockchain Spring Hackathon - IceFireDB Won The First Prize</title><link>https://icorer.com/icorer_blog/posts/icefiredb2022wanxiang/</link><pubDate>Tue, 14 Jun 2022 12:55:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/icefiredb2022wanxiang/</guid><description>2022 Wanxiang Blockchain Spring Hackathon Wrapped up on June 12th,We are very lucky and grateful to Protocol Labs for giving us this honor. In addition to joy, we are more moved. Our efforts and innovation are recognized. Next, we will work harder to incubate our projects.
2022 Wanxiang Blockchain Spring Hackathon, an immensely successful hackathon packed with over 20 teams of passionate builders and developers wrapped up yesterday, under the theme of “Metaverse: A Shared Future on Blockchain”.</description></item><item><title>Gorilla：一个快速、可伸缩的内存时间序列数据库</title><link>https://icorer.com/icorer_blog/posts/gorilladb/</link><pubDate>Fri, 01 Apr 2022 17:26:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/gorilladb/</guid><description>摘要大规模互联网服务旨在出现意外故障时保持高可用性和高响应性。提供这种服务通常需要在大量系统上每秒钟监测和分析数千万次测量，一个特别有效的解决方案是在时间序列数据库(TSDB)中存储和查询这种测量。TSDB设计中的一个关键挑战是如何在效率、可伸缩性和可靠性之间取得平衡。在本文中，我们介绍Gorilla系统，脸书的内存TSDB。我们的见解是，监控系统的用户不太重视单个数据点，而是更重视综合分析，对于快速检测和诊断持续问题的根本原因而言，最新数据点比旧数据点更有价值。Gorilla优化了写入和读取的高可用性，即使在出现故障时也是如此，代价是可能会在写入路径上丢弃少量数据。为了提高查询效率，我们积极利用压缩技术，如增量时间戳和异或浮点值，将Gorilla的存储空间减少了10倍。这使我们能够将Gorilla的数据存储在内存中，与传统数据库(HBase)支持的时间序列数据相比，查询延迟减少了73倍，查询吞吐量提高了14倍。这种性能改进带来了新的监控和调试工具，比如时序关联搜索和更密集的可视化工具。Gorilla还可以优雅地处理从单个节点到整个区域的故障，几乎没有运营开销。
一、介绍大规模互联网服务即使在出现意外故障的情况下也能保持高可用性和对用户的响应。随着这些服务发展到支持全球客户，它们已经从运行在数百台机器上的几个系统扩展到服务数以千计的个人用户系统运行在数千台机器上，通常跨越多个地理复制的数据中心。
运行这些大规模服务的一个重要要求是准确监控底层系统的健康和性能，并在出现问题时快速识别和诊断问题。脸书使用时间序列数据库(TSDB)存储系统测量数据点，并在顶部提供快速查询功能。接下来，我们将指定监控和操作脸书需要满足的一些约束，然后描述Gorilla，这是我们新的内存TSDB，可以存储数千万个数据点(例如，CPU 负载、错误率、延迟等)。)并在几毫秒内响应对此数据的查询。
写占主导地位。我们对 TSDB 的主要要求是它应该始终可用于写入。由于我们有数百个公开数据项的系统，写入速率可能很容易超过每秒数千万个数据点。相比之下，读取速率通常要低几个数量级，因为它主要来自于观察“重要”时间序列数据的自动化系统、可视化系统或为希望诊断观察到问题的人类操作员提供仪表板。
状态转换。我们希望识别新软件发布中出现的问题、配置更改的意外副作用、网络中断以及导致重大状态转换的其他问题。因此，我们希望我们的TSDB支持短时间窗口内的细粒度聚合。在几十秒钟内显示状态转换的能力特别有价值，因为它允许自动化在问题变得广泛传播之前快速修复问题。
高可用性。即使网络分区或其他故障导致不同数据中心之间的连接断开，在任何给定数据中心内运行的系统都应该能够将数据写入本地TSDB机器，并且能够按需检索这些数据。
容错。我们希望将所有写入复制到多个区域，这样我们就可以在任何给定的数据中心或地理区域因灾难而丢失时幸存下来。
Gorilla是脸书的新TSDB，满足了这些限制。Gorilla用作进入监控系统的最新数据的直写缓存。我们的目标是确保大多数查询在几十毫秒内运行。Gorilla 的设计理念是，监控系统的用户不太重视单个数据点，而是更重视综合分析。此外，这些系统不存储任何用户数据，因此传统的 ACID保证不是TSDB的核心要求。 但是，高比例的写入必须始终成功，即使面临可能导致整个数据中心无法访问的灾难。此外，最近的数据点比旧的数据点具有更高的价值，因为直觉上，对于运营工程师来说，知道特定系统或服务现在是否被破坏比知道它是否在一个小时前被破坏更有价值，Gorilla 进行了优化，即使在出现故障的情况下也能保持高度的读写可用性，代价是可能会丢失少量数据写入路径。
高数据插入率、总数据量、实时聚合和可靠性要求带来了挑战。我们依次解决了这些问题。为了解决第一个要求，我们分析了 TSDB 操作数据存储(ODS),这是一个在脸书广泛使用的老的监控系统。我们注意到，对ODS的所有查询中，至少有85%是针对过去26小时内收集的数据。进一步的分析使我们能够确定，如果我们能够用内存中的数据库替换基于磁盘的数据库，我们可能能够为我们的用户提供最好的服务。此外，通过将这个内存中的数据库视为持久的基于磁盘的存储的缓存，我们可以实现具有基于磁盘的数据库的持久性的内存中系统的插入速度。
截至2015年春天，脸书的监控系统生成了超过20亿个独特的时间序列计数器，每秒钟增加约1200万个数据点。这代表每天超过1万亿个点。在每点16字节的情况下，产生的16TBRAM对于实际部署来说太耗费资源了。我们通过重新利用现有的基于XOR的浮点压缩方案来解决这一问题，使其以流的方式工作，从而允许我们将时间序列压缩到平均每点1.37字节，大小减少了12倍。
我们通过在不同的数据中心区域运行多个Gorilla实例并向每个实例传输数据流来满足可靠性要求，而不试图保证一致性。读取查询指向最近的可用Gorilla实例。请注意，这种设计利用了我们的观察，即在不影响数据聚合的情况下，单个数据点可能会丢失，除非Gorilla实例之间存在显著差异。Gorilla目前正在脸书的生产中运行，工程师们每天将其用于实时灭火和调试，并与Hive[27]和Scuba[3]等其他监控和分析系统结合使用，以检测和诊断问题。
二、背景和要求2.1 操作数据存储脸书的大型基础设施由分布在多个数据中心的数百个系统组成，如果没有能够跟踪其运行状况和性能的监控系统，运营和管理这些基础设施将会非常困难。业务数据储存库是脸书监测系统的一个重要部分。ODS由一个时间序列数据库(TSDB)、一个查询服务以及一个探测和警报系统组成。ODS的TSDB 构建在 HBase存储系统之上，如[26]中所述。图1显示了ODS组织方式的高级视图。来自运行在脸书主机上的服务的时间序列数据由ODS写入服务收集并写入 HBase。
ODS时间序列数据有两个消费者。第一个消费者是依赖制图系统的工程师，该系统从ODS生成图形和其他时间序列数据的直观表示，用于交互式分析。第二个消费者是我们的自动警报系统，该系统从 ODS读取计数器，将它们与健康、性能和诊断指标的预设阈值进行比较，并向oncall工程师和自动补救系统发出警报。
2.1.1 监控系统读取性能问题2013 年初，脸书的监控团队意识到其HBase时序存储系统无法扩展处理未来的读取负载。虽然交互式图表的平均读取延迟是可以接受的，但是P90的查询时间增加到了几秒钟，阻碍了我们的自动化。此外，用户正在自我审查他们的用户年龄，因为即使是几千个时间序列的中等规模查询的交互式分析也需要几十秒钟才能执行。在稀疏数据集上执行的较大查询会超时，因为HBase数据存储被调整为优先写入。虽然我们基于HBase的TSDB效率低下，但我们很快就对存储系统进行了大规模更换，因为 ODS的HBase存储拥有大约2PB 的数据[5]。脸书的数据仓库解决方案Hive也不合适，因为它的查询延迟比ODS 高几个数量级，而查询延迟和效率是我们主要关心的问题[27]。
接下来，我们将注意力转向内存缓存。ODS已经使用了一个简单的通读缓存，但它主要是针对多个仪表板共享相同时间序列的图表系统。一个特别困难的场景是当仪表板查询最近的数据点，在缓存中错过，然后发出请求直接发送到 HBase 数据存储。我们还考虑了基于独立Memcache[20]的直写缓存，但拒绝了它，因为向现有时间序列添加新数据需要一个读/写周期，从而导致Memcache服务器的流量非常高。我们需要更有效的解决方案。
2.2 Gorilla要求考虑到这些因素，我们确定了新服务的以下要求:
由一个字符串键标识的20亿个唯一的时间序列。 每分钟增加7亿个数据点(时间戳和值)。 存储数据26小时。 峰值时每秒超过40,000次查询。 读取在不到一毫秒的时间内成功。 支持15秒粒度的时间序列(每个时间序列每分钟 4 个点)。 两个内存中、不在同一位置的副本(用于灾难恢复容量)。 即使单个服务器崩溃，也始终提供读取服务。 能够快速扫描所有内存中的数据。 支持每年至少2倍的增长。 在第3节与其他 TSDB 系统进行简单比较后，我们在第4节详细介绍Gorilla的实现，首先在第4.1 节讨论其新的时间戳和数据值压缩方案。然后，我们将在第 4.4 节中描述Gorilla如何在单节点故障和区域性灾难的情况下保持高可用性。我们将在第5节描述Gorilla如何启用新工具。最后，我们在第6节描述了我们开发和部署Gorilla的经验。
三、与 TSDB 系统的比较有许多出版物详细介绍了数据挖掘技术，以有效地搜索、分类和聚类大量的时间序列数据[8,23,24]。这些系统展示了检查时间序列数据的许多用途，从聚类和分类[8,23]到异常检测[10,16] 到索引时间序列[9,12,24]。然而，很少有例子详细说明能够实时收集和存储大量时间序列数据的系统。Gorilla的设计侧重于对生产系统进行可靠的实时监控，与其他TSDB相比非常突出。Gorilla占据了一个有趣的设计空间，在面对优先于任何旧数据可用性的故障时，可用于读取和写入。
由于 Gorilla 从一开始就被设计为将所有数据存储在内存中，因此它的内存结构也不同于现有TSDB。但是，如果将Gorilla视为另一个磁盘上TSDB之前的时间序列数据内存存储的中间存储，那么Gorilla 可以用作任何 TSDB 的直写缓存(相对简单的修改)。Gorilla对摄取速度和水平扩展的关注与现有解决方案相似。
3.1 OpenTSDBOpenTSDB基于HBase[28]，非常接近我们用于长期数据的ODS HBase存储层。这两个系统依赖于相似的表结构，并且在优化和水平可伸缩性方面得出了相似的结论[26,28]。然而，我们发现支持构建高级监控工具所需的查询量需要比基于磁盘的存储所能支持的更快的查询。
与OpenTSDB不同，ODS HBase层确实为较旧的数据进行时间累积聚合以节省空间**。这导致较旧的存档数据与ODS中较新的数据相比具有较低的时间粒度**，而OpenTSDB将永远保留全分辨率数据。我们发现，更便宜的长时间查询和空间节省是值得的精度损失。</description></item><item><title>保护您的服务网格：十三项清单</title><link>https://icorer.com/icorer_blog/posts/13-item_checklist_for_securing_your_service_mesh/</link><pubDate>Thu, 17 Feb 2022 14:57:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/13-item_checklist_for_securing_your_service_mesh/</guid><description>世界各地的组织都在急于对其应用程序进行现代化改造，以便在云中 Kubernetes 编排的容器中运行。在此现代化过程中，这些公司必须计划保护其应用程序连接。当应用程序被重新平台化并重新构建为分布式微服务时，会出现新的连接模式，这些模式通常会促使构建一个或多个服务网格。
强大的服务网格可以处理南北连接（从边缘进入基于 Kubernetes 的应用程序）和东西连接（同一集群上的服务之间或不同集群之间）。但是，这些流量模式中的每一个都需要全面的安全性.
实现服务网格安全性的最佳方法是采用零信任模型，这意味着每个连接，无论其来源如何，都必须经过验证和保护。为了帮助您评估服务网格技术并实施零信任安全，我们提供了 13 个必备功能，以确保您的应用程序连接安全。这是清单：
传输层安全性（TLS 和 mTLS）提供端到端加密，以保护任何端点对之间的动态数据。它可能是最基本的组件，但令人惊讶的是，并非所有服务网格都完全支持双向 TLS。 内置 Web 应用程序防火墙 (WAF)可屏蔽入站流量以发现威胁并阻止攻击侵入您的周边。对于任何向 Internet 公开以接收传入用户和应用程序连接请求的边缘网关来说，这都是必不可少的。 数据丢失防护 (DLP)监控数据泄露或泄露，以防止数据丢失和数据泄露。如果您的应用程序以某种方式受到损害，您不希望数据泄露您的边界。 Kubernetes的机密管理集成，后者管理密码、安全令牌和加密密钥等敏感凭证。您会担心这些信息仍然被硬编码到应用程序中或以纯文本形式存储的频率。 证书管理从集中式平台控制和执行 SSL 证书以验证连接。证书轮换可能是一个痛苦的管理步骤，应该优雅地加以考虑。这应该可以扩展以支持外部权限，这意味着它将与您已经使用的企业身份和访问管理解决方案一起使用。 授权，例如使用开放策略代理 (OPA)，它将服务 API 策略定义为代码。授权是身份验证的另一面，一旦您验证了他们的身份，谁就可以访问哪些资源。 联合信任域可以安全地跨环境对用户和应用程序进行身份验证，从而在任何地方始终如一地扩展身份验证策略。如果没有这个，您将花费大量精力来尝试保持各种角色的更新和同步——并且可能会犯一些错误。 联合的基于角色的访问控制 (RBAC) 和委派向用户授予与其职责相符的权限，并且再次在任何地方始终如一地应用此权限。这些控制可以应用于管理服务网格的运营商的不同级别，也可以应用于构建在网格中运行的应用程序的开发人员。 多租户和隔离使服务网格中的用户和应用程序可以安全地共享资源。拥有 RBAC 后，您可以安全地定义谁可以接触什么，并为不同的角色有效地创建隔离的工作空间。Istio 的授权策略也可用于防止不需要的流量到达您的应用程序。 漏洞扫描和出版物发现、解决和警告系统中的任何弱点。安全性与其最薄弱的环节一样好，因此检查防御中的任何漏洞很重要。 多集群访问可观察性为整个系统的所有活动提供完整的日志聚合和可审计性。这对于事件后的实时监控和取证都很有用。对于分布式应用程序，有必要获得全局视图。许多人使用 Prometheus 和 Grafana 等开源工具来实现可观察性。 联邦信息处理标准 (FIP) 140-2意味着您的服务网格技术已经过验证，符合美国政府规定的特定严格安全标准。有许多政府法规和行业最佳实践，但 FIPS 是确定安全基准的一种常用方法。 集群中继的安全拉取模型在整个系统中安全地共享配置。这是非常微妙的，但是您要确保任何配置更改都在请求时分发到边缘，并且仅在请求时分发。 虽然严格来说不是服务网格的安全特性，但一个额外的考虑因素是企业支持的可用性和用于响应的定义服务级别协议 (SLA)。</description></item><item><title>使用区块链将零信任架构扩展到端点：最先进的审查</title><link>https://icorer.com/icorer_blog/posts/blockchain_with_zerotrust/</link><pubDate>Tue, 15 Feb 2022 23:50:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/blockchain_with_zerotrust/</guid><description>摘要为了防御当今无边界网络中的横向移动，零信任架构(ZTA)的采用势头越来越猛。有了全面的 ZTA 实施，对手不太可能从受损的端点开始通过网络传播。然而，已经经过身份验证和授权的受损端点会话可以被用来执行有限的(尽管是恶意的)活动，最终使端点成为 ZTA 的致命弱点。为了有效地检测此类攻击，已经开发了具有基于攻击场景的方法的分布式协同入侵检测系统。尽管如此，高级持续威胁(APTs)已经证明了它们绕过这种方法的能力，并且成功率很高。因此，对手可以不被发现地通过或潜在地改变检测记录机制，以实现隐蔽的存在。最近，区块链技术在网络安全领域展示了可靠的使用案例。在本文中，受基于 ZTA 和区块链的入侵检测和防御的融合的推动，我们研究了如何将 ZTA 扩展到端点。也就是说，我们对 ZTA 模型、以端点为重点的真实体系结构以及基于区块链的入侵检测系统进行了最先进的审查。我们讨论了区块链的不变性增强检测过程的潜力，并确定了开放的挑战以及潜在的解决方案和未来的方向。
一、介绍随着云计算的革命，大多数企业的资源和数据不再存储在内部。此外，最近的新冠肺炎疫情事件极大地改变了工作模式，因为大多数员工和企业不得不转向在家工作。在家工作(和远程工作)会给组织带来新的严重安全风险，因为许多“未经培训”的员工使用自己的设备连接到工作信息技术(IT)系统。云计算和远程工作是企业必须扩大其数字安全范围并适应当代趋势的例子。
在传统的基于外围的安全模型中，组织在外围的资源和资产被认为是良性的和可信的。边界通常由安全措施保护，如防火墙或入侵检测系统。这种模式在云计算和远程工作领域似乎不太有效，针对远程工作员工的几次网络攻击(例如[1-5])就表明了这一点。
信任是传统的基于边界的安全模型所依赖的基本原则。员工或合作者的设备和组织资产(即端点)通常在默认情况下是可信的，而不管其状况如何。如果攻击者能够控制这些端点中的任何一个，那么边界就会受到威胁，并且通过横向移动有可能实现对信息和数据的进一步访问。
防火墙、防病毒技术、入侵检测和防御系统(IDS/IPS)和网络应用防火墙(WAFs)，换句话说，大石墙和装甲前门，已经不足以保证现代 IT 和运营技术(OT)环境的安全[6]。基于外围的安全是多家公司采用的主要概念，尤其是当他们的数据驻留在内部数据中心时。建立在内部和外部差异基础上的传统防御模式正在过时[7]，而与此同时，威胁格局也在急剧演变[8]，最终导致基于外围的安全架构的衰落。
为了应对当今复杂的网络基础设施和当前不断发展的威胁形势，需要一种新的安全架构。ZTA 通过建立无边界的基于数字身份的边界脱颖而出，在这个边界中，数据处于安全架构的中心，破坏思维主导着威胁模型， 引领着访问控制环境、运营、托管环境、端点和互连基础架构。ZTA 提倡一种新的安全架构，默认情况下，任何设备、系统、用户或应用程序都不应该基于其在网络中的位置而受到固有的信任。相反，不管在什么地方，信任总是要赢得和验证的。然而，这并不一定意味着在ZTA 的情况下信任被消除，而是应该最小化，直到通过ZTA 信条和核心组成部分证明不是这样。
使用传统的基于边界的防御，如果坚定的攻击者能够在端点上建立经过身份验证和授权的立足点，他们仍然可以绕过 ZTA 安全健康检查。例如，操作系统内核中的潜在恶意软件可以篡改在 ZTA 环境中进行的安全检查。这最终导致绕过在 ZTA 实施的基本控制，这将允许攻击者除了横向移动之外，还执行一些以用户和设备为中心的恶意活动。因此，需要一种有效的入侵检测方法来解决端点的漏洞，这可以被视为 ZTAs 的致命弱点。
在本文中，我们旨在研究如何利用区块链的不变性增强入侵检测过程的潜力，将 ZTA 扩展到端点，以消除上述问题。为此，我们首先回顾零信任的核心原则、能力和要求。其次，我们对现有的现实世界零信任实现进行分类，并讨论它们的优缺点。第三，我们探索了区块链在开发和改进分布式协作入侵检测系统(DCIDSs)方面的潜力，该系统可以缓解ZTA 的致命弱点(即端点的脆弱性)。最后，我们讨论了开放的问题和挑战，并强调了ZTA和基于区块链的分布式入侵检测系统的潜在解决方案和研究方向。
据我们所知，这是第一个利用区块链技术成功将ZTA扩展到端点的工作。表1给出了本文中使用的主要缩写及其定义。
二、零信任（ZT）在本节中，我们简要介绍了“零信任”和 ZTA 的历史，并讨论了零信任的核心原则、核心能力、模型和现有方法，包括现实世界的实现。
2.1 零信任架构的历史2004 年的杰里科论坛提出了去周边化的想法(当时是激进的)[3]，随后发展成为更广泛的零信任概念。早在2010 年，J. Kindervag [15]就创造了“零信任”一词；然而，在此之前，网络安全领域就存在零信任概念。美国国防部和国防信息系统局(DISA)提出了一项名为“黑核”的安全战略，并于 2007 年发表[16]。Black core讨论了从基于外围的安全架构向强调保护单个交易的安全架构的过渡。
云和移动计算的广泛采用极大地促进了 ZTAs 的发展，例如，作为其中的一部分，基于身份的架构等方法慢慢获得了关注和更广泛的接受。谷歌以“BeyondCorp”的名义发布了一系列关于如何实现零信任架构的六个文档[17-22]。BeyondCorp项目倡导去边界化的概念，认为基于边界的安全控制已经不够，安全应该扩展到用户和设备。由于这个项目，谷歌放弃了传统的基于虚拟专用网络(VPNs)的远程工作方式，并设法提供了一个合理的保证，即所有公司用户都可以通过不安全和不受管理的网络访问谷歌的网络。
2.2 从传统的周边架构到ZTA作为一种理念，“零信任”假设对用户、设备、工作负载和网络流量的信任不应被隐含地授予[15]，其结果是所有实体都必须被明确地验证、认证、授权和持续监控。零信任的核心目标之一是，一旦对手成功危及用户的设备，甚至简单地窃取他们的凭据，就会严重抑制对手横向移动的能力。因此，需要相应地塑造和准备信息技术基础设施。
传统的基于外围的安全架构会创建多个信任区域[2]。并非所有区域都遵守相同的规则或相同的信任级别。事实上，如果相关组件没有明确允许，用户甚至可能无法进入下一个区域。这被称为纵深防御，正如史密斯[23]所讨论的，并在图1中描述，或者称为城堡和护城河方法[24]。请注意，在到达大型机之前，不同的区域(互联网、非军事区、可信和特权)受到各种基于外围的控制的保护，例如本地代理、虚拟专用网网关、多个防火墙和应用程序服务。在这个例子(即图1)中，大型机是一个核心银行系统，负责所有交易，因此它被完全隔离在一个特权区域中。
与传统的安全架构不同，零信任要求从内到外进行思考、构建和保护。基于谷歌[19] [20]、Jericho[3]和Kindervag[15] [25]的上述工作，有一个直接和重要的观察， 即在ZTA，一旦网络位置依赖性变得无关紧要，虚拟专用网技术就可以被消除。简而言之，虚拟专用网允许远程工作的用户(在图1中用“远程员工”表示)通过安全的加密通道连接到办公室(在图1中用“可信”表示)。但是，应该通过其他方式保护端点，因为虚拟专用网加密只处理“远程员工”和“可信”区域之间的隧道。当“远程员工”通过身份验证并成功建立隧道时，他/她会收到“可信”区域的远程网络中的一个 IP 地址。在该隧道上， 从“远程员工”到“可信”区域的流量被解封并路由，因此导致“官方”后门。此外，被称为“虚拟专用网网关”的单一入口点充当体系结构和网络的单一故障点或扼杀点。因此，如果我们开始认为网络位置无关紧要，同时应用一组适当的控制，那么如果没有进一步的依赖关系 (例如，具有传统协议的应用程序)，就可以消除虚拟专用网络。也就是说，身份验证和授权以及策略实施应该立即向网络边缘和端点靠拢。
为了反映上面的论点，我们绘制了图 2，显示了对 ZTA的引用。为了简化起见，在图中，我们只包括核心组件，例如，本地代理(LB)、远程员工、移动设备、不可信客户端和许多需要保护的服务。与基于边界的架构相比，如图 1 所示，没有区域，安全性是由内而外构建的。此外，既没有 VPN 网关，也没有防火墙来过滤网络流量， 最重要的是没有单一的入口网关。我们注意到；但是，控制平面上的策略执行点。这种 ZTA 参考不会像基于外围的架构那样产生任何瓶颈。</description></item></channel></rss>