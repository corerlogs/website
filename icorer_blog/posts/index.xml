<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on 笔迹-工匠之芯</title><link>https://icorer.com/icorer_blog/posts/</link><description>Recent content in Posts on 笔迹-工匠之芯</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 14 Jun 2022 12:55:18 +0800</lastBuildDate><atom:link href="https://icorer.com/icorer_blog/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>2022 Wanxiang Blockchain Spring Hackathon - IceFireDB Won The First Prize</title><link>https://icorer.com/icorer_blog/posts/icefiredb2022wanxiang/</link><pubDate>Tue, 14 Jun 2022 12:55:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/icefiredb2022wanxiang/</guid><description>2022 Wanxiang Blockchain Spring Hackathon Wrapped up on June 12th,We are very lucky and grateful to Protocol Labs for giving us this honor. In addition to joy, we are more moved. Our efforts and innovation are recognized. Next, we will work harder to incubate our projects.
2022 Wanxiang Blockchain Spring Hackathon, an immensely successful hackathon packed with over 20 teams of passionate builders and developers wrapped up yesterday, under the theme of “Metaverse: A Shared Future on Blockchain”.</description></item><item><title>Gorilla：一个快速、可伸缩的内存时间序列数据库</title><link>https://icorer.com/icorer_blog/posts/gorilladb/</link><pubDate>Fri, 01 Apr 2022 17:26:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/gorilladb/</guid><description>摘要大规模互联网服务旨在出现意外故障时保持高可用性和高响应性。提供这种服务通常需要在大量系统上每秒钟监测和分析数千万次测量，一个特别有效的解决方案是在时间序列数据库(TSDB)中存储和查询这种测量。TSDB设计中的一个关键挑战是如何在效率、可伸缩性和可靠性之间取得平衡。在本文中，我们介绍Gorilla系统，脸书的内存TSDB。我们的见解是，监控系统的用户不太重视单个数据点，而是更重视综合分析，对于快速检测和诊断持续问题的根本原因而言，最新数据点比旧数据点更有价值。Gorilla优化了写入和读取的高可用性，即使在出现故障时也是如此，代价是可能会在写入路径上丢弃少量数据。为了提高查询效率，我们积极利用压缩技术，如增量时间戳和异或浮点值，将Gorilla的存储空间减少了10倍。这使我们能够将Gorilla的数据存储在内存中，与传统数据库(HBase)支持的时间序列数据相比，查询延迟减少了73倍，查询吞吐量提高了14倍。这种性能改进带来了新的监控和调试工具，比如时序关联搜索和更密集的可视化工具。Gorilla还可以优雅地处理从单个节点到整个区域的故障，几乎没有运营开销。
一、介绍大规模互联网服务即使在出现意外故障的情况下也能保持高可用性和对用户的响应。随着这些服务发展到支持全球客户，它们已经从运行在数百台机器上的几个系统扩展到服务数以千计的个人用户系统运行在数千台机器上，通常跨越多个地理复制的数据中心。
运行这些大规模服务的一个重要要求是准确监控底层系统的健康和性能，并在出现问题时快速识别和诊断问题。脸书使用时间序列数据库(TSDB)存储系统测量数据点，并在顶部提供快速查询功能。接下来，我们将指定监控和操作脸书需要满足的一些约束，然后描述Gorilla，这是我们新的内存TSDB，可以存储数千万个数据点(例如，CPU 负载、错误率、延迟等)。)并在几毫秒内响应对此数据的查询。
写占主导地位。我们对 TSDB 的主要要求是它应该始终可用于写入。由于我们有数百个公开数据项的系统，写入速率可能很容易超过每秒数千万个数据点。相比之下，读取速率通常要低几个数量级，因为它主要来自于观察“重要”时间序列数据的自动化系统、可视化系统或为希望诊断观察到问题的人类操作员提供仪表板。
状态转换。我们希望识别新软件发布中出现的问题、配置更改的意外副作用、网络中断以及导致重大状态转换的其他问题。因此，我们希望我们的TSDB支持短时间窗口内的细粒度聚合。在几十秒钟内显示状态转换的能力特别有价值，因为它允许自动化在问题变得广泛传播之前快速修复问题。
高可用性。即使网络分区或其他故障导致不同数据中心之间的连接断开，在任何给定数据中心内运行的系统都应该能够将数据写入本地TSDB机器，并且能够按需检索这些数据。
容错。我们希望将所有写入复制到多个区域，这样我们就可以在任何给定的数据中心或地理区域因灾难而丢失时幸存下来。
Gorilla是脸书的新TSDB，满足了这些限制。Gorilla用作进入监控系统的最新数据的直写缓存。我们的目标是确保大多数查询在几十毫秒内运行。Gorilla 的设计理念是，监控系统的用户不太重视单个数据点，而是更重视综合分析。此外，这些系统不存储任何用户数据，因此传统的 ACID保证不是TSDB的核心要求。 但是，高比例的写入必须始终成功，即使面临可能导致整个数据中心无法访问的灾难。此外，最近的数据点比旧的数据点具有更高的价值，因为直觉上，对于运营工程师来说，知道特定系统或服务现在是否被破坏比知道它是否在一个小时前被破坏更有价值，Gorilla 进行了优化，即使在出现故障的情况下也能保持高度的读写可用性，代价是可能会丢失少量数据写入路径。
高数据插入率、总数据量、实时聚合和可靠性要求带来了挑战。我们依次解决了这些问题。为了解决第一个要求，我们分析了 TSDB 操作数据存储(ODS),这是一个在脸书广泛使用的老的监控系统。我们注意到，对ODS的所有查询中，至少有85%是针对过去26小时内收集的数据。进一步的分析使我们能够确定，如果我们能够用内存中的数据库替换基于磁盘的数据库，我们可能能够为我们的用户提供最好的服务。此外，通过将这个内存中的数据库视为持久的基于磁盘的存储的缓存，我们可以实现具有基于磁盘的数据库的持久性的内存中系统的插入速度。
截至2015年春天，脸书的监控系统生成了超过20亿个独特的时间序列计数器，每秒钟增加约1200万个数据点。这代表每天超过1万亿个点。在每点16字节的情况下，产生的16TBRAM对于实际部署来说太耗费资源了。我们通过重新利用现有的基于XOR的浮点压缩方案来解决这一问题，使其以流的方式工作，从而允许我们将时间序列压缩到平均每点1.37字节，大小减少了12倍。
我们通过在不同的数据中心区域运行多个Gorilla实例并向每个实例传输数据流来满足可靠性要求，而不试图保证一致性。读取查询指向最近的可用Gorilla实例。请注意，这种设计利用了我们的观察，即在不影响数据聚合的情况下，单个数据点可能会丢失，除非Gorilla实例之间存在显著差异。Gorilla目前正在脸书的生产中运行，工程师们每天将其用于实时灭火和调试，并与Hive[27]和Scuba[3]等其他监控和分析系统结合使用，以检测和诊断问题。
二、背景和要求2.1 操作数据存储脸书的大型基础设施由分布在多个数据中心的数百个系统组成，如果没有能够跟踪其运行状况和性能的监控系统，运营和管理这些基础设施将会非常困难。业务数据储存库是脸书监测系统的一个重要部分。ODS由一个时间序列数据库(TSDB)、一个查询服务以及一个探测和警报系统组成。ODS的TSDB 构建在 HBase存储系统之上，如[26]中所述。图1显示了ODS组织方式的高级视图。来自运行在脸书主机上的服务的时间序列数据由ODS写入服务收集并写入 HBase。
ODS时间序列数据有两个消费者。第一个消费者是依赖制图系统的工程师，该系统从ODS生成图形和其他时间序列数据的直观表示，用于交互式分析。第二个消费者是我们的自动警报系统，该系统从 ODS读取计数器，将它们与健康、性能和诊断指标的预设阈值进行比较，并向oncall工程师和自动补救系统发出警报。
2.1.1 监控系统读取性能问题2013 年初，脸书的监控团队意识到其HBase时序存储系统无法扩展处理未来的读取负载。虽然交互式图表的平均读取延迟是可以接受的，但是P90的查询时间增加到了几秒钟，阻碍了我们的自动化。此外，用户正在自我审查他们的用户年龄，因为即使是几千个时间序列的中等规模查询的交互式分析也需要几十秒钟才能执行。在稀疏数据集上执行的较大查询会超时，因为HBase数据存储被调整为优先写入。虽然我们基于HBase的TSDB效率低下，但我们很快就对存储系统进行了大规模更换，因为 ODS的HBase存储拥有大约2PB 的数据[5]。脸书的数据仓库解决方案Hive也不合适，因为它的查询延迟比ODS 高几个数量级，而查询延迟和效率是我们主要关心的问题[27]。
接下来，我们将注意力转向内存缓存。ODS已经使用了一个简单的通读缓存，但它主要是针对多个仪表板共享相同时间序列的图表系统。一个特别困难的场景是当仪表板查询最近的数据点，在缓存中错过，然后发出请求直接发送到 HBase 数据存储。我们还考虑了基于独立Memcache[20]的直写缓存，但拒绝了它，因为向现有时间序列添加新数据需要一个读/写周期，从而导致Memcache服务器的流量非常高。我们需要更有效的解决方案。
2.2 Gorilla要求考虑到这些因素，我们确定了新服务的以下要求:
由一个字符串键标识的20亿个唯一的时间序列。 每分钟增加7亿个数据点(时间戳和值)。 存储数据26小时。 峰值时每秒超过40,000次查询。 读取在不到一毫秒的时间内成功。 支持15秒粒度的时间序列(每个时间序列每分钟 4 个点)。 两个内存中、不在同一位置的副本(用于灾难恢复容量)。 即使单个服务器崩溃，也始终提供读取服务。 能够快速扫描所有内存中的数据。 支持每年至少2倍的增长。 在第3节与其他 TSDB 系统进行简单比较后，我们在第4节详细介绍Gorilla的实现，首先在第4.1 节讨论其新的时间戳和数据值压缩方案。然后，我们将在第 4.4 节中描述Gorilla如何在单节点故障和区域性灾难的情况下保持高可用性。我们将在第5节描述Gorilla如何启用新工具。最后，我们在第6节描述了我们开发和部署Gorilla的经验。
三、与 TSDB 系统的比较有许多出版物详细介绍了数据挖掘技术，以有效地搜索、分类和聚类大量的时间序列数据[8,23,24]。这些系统展示了检查时间序列数据的许多用途，从聚类和分类[8,23]到异常检测[10,16] 到索引时间序列[9,12,24]。然而，很少有例子详细说明能够实时收集和存储大量时间序列数据的系统。Gorilla的设计侧重于对生产系统进行可靠的实时监控，与其他TSDB相比非常突出。Gorilla占据了一个有趣的设计空间，在面对优先于任何旧数据可用性的故障时，可用于读取和写入。
由于 Gorilla 从一开始就被设计为将所有数据存储在内存中，因此它的内存结构也不同于现有TSDB。但是，如果将Gorilla视为另一个磁盘上TSDB之前的时间序列数据内存存储的中间存储，那么Gorilla 可以用作任何 TSDB 的直写缓存(相对简单的修改)。Gorilla对摄取速度和水平扩展的关注与现有解决方案相似。
3.1 OpenTSDBOpenTSDB基于HBase[28]，非常接近我们用于长期数据的ODS HBase存储层。这两个系统依赖于相似的表结构，并且在优化和水平可伸缩性方面得出了相似的结论[26,28]。然而，我们发现支持构建高级监控工具所需的查询量需要比基于磁盘的存储所能支持的更快的查询。
与OpenTSDB不同，ODS HBase层确实为较旧的数据进行时间累积聚合以节省空间。这导致较旧的存档数据与ODS中较新的数据相比具有较低的时间粒度，而OpenTSDB将永远保留全分辨率数据。我们发现，更便宜的长时间查询和空间节省是值得的精度损失。</description></item><item><title>保护您的服务网格：十三项清单</title><link>https://icorer.com/icorer_blog/posts/13-item_checklist_for_securing_your_service_mesh/</link><pubDate>Thu, 17 Feb 2022 14:57:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/13-item_checklist_for_securing_your_service_mesh/</guid><description>世界各地的组织都在急于对其应用程序进行现代化改造，以便在云中 Kubernetes 编排的容器中运行。在此现代化过程中，这些公司必须计划保护其应用程序连接。当应用程序被重新平台化并重新构建为分布式微服务时，会出现新的连接模式，这些模式通常会促使构建一个或多个服务网格。
强大的服务网格可以处理南北连接（从边缘进入基于 Kubernetes 的应用程序）和东西连接（同一集群上的服务之间或不同集群之间）。但是，这些流量模式中的每一个都需要全面的安全性.
实现服务网格安全性的最佳方法是采用零信任模型，这意味着每个连接，无论其来源如何，都必须经过验证和保护。为了帮助您评估服务网格技术并实施零信任安全，我们提供了 13 个必备功能，以确保您的应用程序连接安全。这是清单：
传输层安全性（TLS 和 mTLS）提供端到端加密，以保护任何端点对之间的动态数据。它可能是最基本的组件，但令人惊讶的是，并非所有服务网格都完全支持双向 TLS。 内置 Web 应用程序防火墙 (WAF)可屏蔽入站流量以发现威胁并阻止攻击侵入您的周边。对于任何向 Internet 公开以接收传入用户和应用程序连接请求的边缘网关来说，这都是必不可少的。 数据丢失防护 (DLP)监控数据泄露或泄露，以防止数据丢失和数据泄露。如果您的应用程序以某种方式受到损害，您不希望数据泄露您的边界。 Kubernetes的机密管理集成，后者管理密码、安全令牌和加密密钥等敏感凭证。您会担心这些信息仍然被硬编码到应用程序中或以纯文本形式存储的频率。 证书管理从集中式平台控制和执行 SSL 证书以验证连接。证书轮换可能是一个痛苦的管理步骤，应该优雅地加以考虑。这应该可以扩展以支持外部权限，这意味着它将与您已经使用的企业身份和访问管理解决方案一起使用。 授权，例如使用开放策略代理 (OPA)，它将服务 API 策略定义为代码。授权是身份验证的另一面，一旦您验证了他们的身份，谁就可以访问哪些资源。 联合信任域可以安全地跨环境对用户和应用程序进行身份验证，从而在任何地方始终如一地扩展身份验证策略。如果没有这个，您将花费大量精力来尝试保持各种角色的更新和同步——并且可能会犯一些错误。 联合的基于角色的访问控制 (RBAC) 和委派向用户授予与其职责相符的权限，并且再次在任何地方始终如一地应用此权限。这些控制可以应用于管理服务网格的运营商的不同级别，也可以应用于构建在网格中运行的应用程序的开发人员。 多租户和隔离使服务网格中的用户和应用程序可以安全地共享资源。拥有 RBAC 后，您可以安全地定义谁可以接触什么，并为不同的角色有效地创建隔离的工作空间。Istio 的授权策略也可用于防止不需要的流量到达您的应用程序。 漏洞扫描和出版物发现、解决和警告系统中的任何弱点。安全性与其最薄弱的环节一样好，因此检查防御中的任何漏洞很重要。 多集群访问可观察性为整个系统的所有活动提供完整的日志聚合和可审计性。这对于事件后的实时监控和取证都很有用。对于分布式应用程序，有必要获得全局视图。许多人使用 Prometheus 和 Grafana 等开源工具来实现可观察性。 联邦信息处理标准 (FIP) 140-2意味着您的服务网格技术已经过验证，符合美国政府规定的特定严格安全标准。有许多政府法规和行业最佳实践，但 FIPS 是确定安全基准的一种常用方法。 集群中继的安全拉取模型在整个系统中安全地共享配置。这是非常微妙的，但是您要确保任何配置更改都在请求时分发到边缘，并且仅在请求时分发。 虽然严格来说不是服务网格的安全特性，但一个额外的考虑因素是企业支持的可用性和用于响应的定义服务级别协议 (SLA)。</description></item><item><title>使用区块链将零信任架构扩展到端点：最先进的审查</title><link>https://icorer.com/icorer_blog/posts/blockchain_with_zerotrust/</link><pubDate>Tue, 15 Feb 2022 23:50:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/blockchain_with_zerotrust/</guid><description>摘要为了防御当今无边界网络中的横向移动，零信任架构(ZTA)的采用势头越来越猛。有了全面的 ZTA 实施，对手不太可能从受损的端点开始通过网络传播。然而，已经经过身份验证和授权的受损端点会话可以被用来执行有限的(尽管是恶意的)活动，最终使端点成为 ZTA 的致命弱点。为了有效地检测此类攻击，已经开发了具有基于攻击场景的方法的分布式协同入侵检测系统。尽管如此，高级持续威胁(APTs)已经证明了它们绕过这种方法的能力，并且成功率很高。因此，对手可以不被发现地通过或潜在地改变检测记录机制，以实现隐蔽的存在。最近，区块链技术在网络安全领域展示了可靠的使用案例。在本文中，受基于 ZTA 和区块链的入侵检测和防御的融合的推动，我们研究了如何将 ZTA 扩展到端点。也就是说，我们对 ZTA 模型、以端点为重点的真实体系结构以及基于区块链的入侵检测系统进行了最先进的审查。我们讨论了区块链的不变性增强检测过程的潜力，并确定了开放的挑战以及潜在的解决方案和未来的方向。
一、介绍随着云计算的革命，大多数企业的资源和数据不再存储在内部。此外，最近的新冠肺炎疫情事件极大地改变了工作模式，因为大多数员工和企业不得不转向在家工作。在家工作(和远程工作)会给组织带来新的严重安全风险，因为许多“未经培训”的员工使用自己的设备连接到工作信息技术(IT)系统。云计算和远程工作是企业必须扩大其数字安全范围并适应当代趋势的例子。
在传统的基于外围的安全模型中，组织在外围的资源和资产被认为是良性的和可信的。边界通常由安全措施保护，如防火墙或入侵检测系统。这种模式在云计算和远程工作领域似乎不太有效，针对远程工作员工的几次网络攻击(例如[1-5])就表明了这一点。
信任是传统的基于边界的安全模型所依赖的基本原则。员工或合作者的设备和组织资产(即端点)通常在默认情况下是可信的，而不管其状况如何。如果攻击者能够控制这些端点中的任何一个，那么边界就会受到威胁，并且通过横向移动有可能实现对信息和数据的进一步访问。
防火墙、防病毒技术、入侵检测和防御系统(IDS/IPS)和网络应用防火墙(WAFs)，换句话说，大石墙和装甲前门，已经不足以保证现代 IT 和运营技术(OT)环境的安全[6]。基于外围的安全是多家公司采用的主要概念，尤其是当他们的数据驻留在内部数据中心时。建立在内部和外部差异基础上的传统防御模式正在过时[7]，而与此同时，威胁格局也在急剧演变[8]，最终导致基于外围的安全架构的衰落。
为了应对当今复杂的网络基础设施和当前不断发展的威胁形势，需要一种新的安全架构。ZTA 通过建立无边界的基于数字身份的边界脱颖而出，在这个边界中，数据处于安全架构的中心，破坏思维主导着威胁模型， 引领着访问控制环境、运营、托管环境、端点和互连基础架构。ZTA 提倡一种新的安全架构，默认情况下，任何设备、系统、用户或应用程序都不应该基于其在网络中的位置而受到固有的信任。相反，不管在什么地方，信任总是要赢得和验证的。然而，这并不一定意味着在ZTA 的情况下信任被消除，而是应该最小化，直到通过ZTA 信条和核心组成部分证明不是这样。
使用传统的基于边界的防御，如果坚定的攻击者能够在端点上建立经过身份验证和授权的立足点，他们仍然可以绕过 ZTA 安全健康检查。例如，操作系统内核中的潜在恶意软件可以篡改在 ZTA 环境中进行的安全检查。这最终导致绕过在 ZTA 实施的基本控制，这将允许攻击者除了横向移动之外，还执行一些以用户和设备为中心的恶意活动。因此，需要一种有效的入侵检测方法来解决端点的漏洞，这可以被视为 ZTAs 的致命弱点。
在本文中，我们旨在研究如何利用区块链的不变性增强入侵检测过程的潜力，将 ZTA 扩展到端点，以消除上述问题。为此，我们首先回顾零信任的核心原则、能力和要求。其次，我们对现有的现实世界零信任实现进行分类，并讨论它们的优缺点。第三，我们探索了区块链在开发和改进分布式协作入侵检测系统(DCIDSs)方面的潜力，该系统可以缓解ZTA 的致命弱点(即端点的脆弱性)。最后，我们讨论了开放的问题和挑战，并强调了ZTA和基于区块链的分布式入侵检测系统的潜在解决方案和研究方向。
据我们所知，这是第一个利用区块链技术成功将ZTA扩展到端点的工作。表1给出了本文中使用的主要缩写及其定义。
二、零信任（ZT）在本节中，我们简要介绍了“零信任”和 ZTA 的历史，并讨论了零信任的核心原则、核心能力、模型和现有方法，包括现实世界的实现。
2.1 零信任架构的历史2004 年的杰里科论坛提出了去周边化的想法(当时是激进的)[3]，随后发展成为更广泛的零信任概念。早在2010 年，J. Kindervag [15]就创造了“零信任”一词；然而，在此之前，网络安全领域就存在零信任概念。美国国防部和国防信息系统局(DISA)提出了一项名为“黑核”的安全战略，并于 2007 年发表[16]。Black core讨论了从基于外围的安全架构向强调保护单个交易的安全架构的过渡。
云和移动计算的广泛采用极大地促进了 ZTAs 的发展，例如，作为其中的一部分，基于身份的架构等方法慢慢获得了关注和更广泛的接受。谷歌以“BeyondCorp”的名义发布了一系列关于如何实现零信任架构的六个文档[17-22]。BeyondCorp项目倡导去边界化的概念，认为基于边界的安全控制已经不够，安全应该扩展到用户和设备。由于这个项目，谷歌放弃了传统的基于虚拟专用网络(VPNs)的远程工作方式，并设法提供了一个合理的保证，即所有公司用户都可以通过不安全和不受管理的网络访问谷歌的网络。
2.2 从传统的周边架构到ZTA作为一种理念，“零信任”假设对用户、设备、工作负载和网络流量的信任不应被隐含地授予[15]，其结果是所有实体都必须被明确地验证、认证、授权和持续监控。零信任的核心目标之一是，一旦对手成功危及用户的设备，甚至简单地窃取他们的凭据，就会严重抑制对手横向移动的能力。因此，需要相应地塑造和准备信息技术基础设施。
传统的基于外围的安全架构会创建多个信任区域[2]。并非所有区域都遵守相同的规则或相同的信任级别。事实上，如果相关组件没有明确允许，用户甚至可能无法进入下一个区域。这被称为纵深防御，正如史密斯[23]所讨论的，并在图1中描述，或者称为城堡和护城河方法[24]。请注意，在到达大型机之前，不同的区域(互联网、非军事区、可信和特权)受到各种基于外围的控制的保护，例如本地代理、虚拟专用网网关、多个防火墙和应用程序服务。在这个例子(即图1)中，大型机是一个核心银行系统，负责所有交易，因此它被完全隔离在一个特权区域中。
与传统的安全架构不同，零信任要求从内到外进行思考、构建和保护。基于谷歌[19] [20]、Jericho[3]和Kindervag[15] [25]的上述工作，有一个直接和重要的观察， 即在ZTA，一旦网络位置依赖性变得无关紧要，虚拟专用网技术就可以被消除。简而言之，虚拟专用网允许远程工作的用户(在图1中用“远程员工”表示)通过安全的加密通道连接到办公室(在图1中用“可信”表示)。但是，应该通过其他方式保护端点，因为虚拟专用网加密只处理“远程员工”和“可信”区域之间的隧道。当“远程员工”通过身份验证并成功建立隧道时，他/她会收到“可信”区域的远程网络中的一个 IP 地址。在该隧道上， 从“远程员工”到“可信”区域的流量被解封并路由，因此导致“官方”后门。此外，被称为“虚拟专用网网关”的单一入口点充当体系结构和网络的单一故障点或扼杀点。因此，如果我们开始认为网络位置无关紧要，同时应用一组适当的控制，那么如果没有进一步的依赖关系 (例如，具有传统协议的应用程序)，就可以消除虚拟专用网络。也就是说，身份验证和授权以及策略实施应该立即向网络边缘和端点靠拢。
为了反映上面的论点，我们绘制了图 2，显示了对 ZTA的引用。为了简化起见，在图中，我们只包括核心组件，例如，本地代理(LB)、远程员工、移动设备、不可信客户端和许多需要保护的服务。与基于边界的架构相比，如图 1 所示，没有区域，安全性是由内而外构建的。此外，既没有 VPN 网关，也没有防火墙来过滤网络流量， 最重要的是没有单一的入口网关。我们注意到；但是，控制平面上的策略执行点。这种 ZTA 参考不会像基于外围的架构那样产生任何瓶颈。</description></item><item><title>自主网络安全-保障未来颠覆性技术</title><link>https://icorer.com/icorer_blog/posts/autonomous_network_security_securing_future_disruptive_technologies/</link><pubDate>Fri, 28 Jan 2022 21:07:07 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/autonomous_network_security_securing_future_disruptive_technologies/</guid><description>From：2021 年 IEEE ⽹络安全与弹性国际会议 (CSR)
概述这篇论文讲述自主网络安全知识的系统化。诸如物联网、人工智能和自治系统等颠覆性技术正变得越来越普通，而且通常很少或者根本没有网络安全保护，这种缺乏安全性导致的网络攻击面不断扩大。自主计算计划旨在通过使复杂计算系统自我管理来解决管理复杂计算系统的复杂性。自主系统包含应对网络攻击的属性，例如新技术的自我保护和自我修复。有许多关于自主网络安全的研究项目，采用不同的方法和目标技术，其中许多具有破坏性。本文回顾了自主计算，分析了自主网络安全的研究，并提供研究知识的系统化，本文最后确定自主网络安全方面的差距，以供未来研究。
一、介绍随着恶意⾏为者变得善于发现和利⽤⽹络和计算机系统中的缺陷[1]，对计算机系统的攻击数量随着复杂程度和严重程度的增加⽽增加。新的颠覆性技术不断被引⼊并连接到企业⽹络和互联⽹，这些技术限制了有些甚⾄没有内置的⽹络安全。其中包括⽹络物理系统/物联⽹ (CPS/IoT)、⼈⼯智能(AI) 和⾃治系统等技术。计算系统通常会在事后添加⽹络安全，⽽不是从⼀开始就设计，尤其是新的颠覆性技术，尤其是匆忙推向市场的技术。
系统需要能够以⾃我管理的⽅式对攻击和固有的弱点做出反应，⽆论是单独还是与其他系统⼀起。将安全性本地嵌⼊到软件和硬件系统中将使它们能够对攻击做出反应，并单独保护和治愈⾃⼰，并且作为⼀个群体。从本质上讲，这是⽹络安全的⾃主⽅法。
伯纳尔等⼈[2]在2019 年确定了⽹络安全⽅⾯的⼀些研究挑战，并将“软件化和虚拟化CPS/IoT系统和移动⽹络中的⾃主安全编排和执⾏”列为第⼆名，仅次于“可互操作和可扩展的安全管理异构⽣态系统”。 他们指出，需要新的⾃主和上下⽂感知安全协调器，它们可以快速、动态地编排和实施适当的防御机制。
本⽂的⽬的是调查⾃主⽹络安全研究中的空⽩，并将这些知识系统化以探索未来的研究。该论⽂基于对⾃主⽹络安全⽂献的调查和⽅法分类。这提供了对未来研究可以帮助推动⾃主⽹络安全向前发展以保护未来颠覆性技术的洞察⼒。本⽂的其余部分结构如下：第⼆部分提供了⾃主计算(AC) 的背景以及如何将⽹络安全设想为⾃主系统的⼀部分。第三部分描述了⾃主⽹络安全的过去和当前研究。⼀些研究侧重于特定应⽤领域的⾃主⽹络安全，⽽其他研究则提供可⽤于⼀系列应⽤的⾃主⽹络安全框架。第四节提供了研究分类和分类知识库的系统化，第五节讨论了基于知识系统化的⽹络安全研究的差距，第六节提供了结论性意⻅。
二、背景Kephart 和 Chess 在 2003 年的⾃主愿景论⽂[3]中将⾃主计算定义为“根据管理员的⾼级⽬标，可以⾃我管理的计算系统”。使⽤“⾃主计算”⼀词是因为⾃主神经系统管理⾝体功能，调整这些功能以满⾜⾝体需求，并为更⾼层次的认知活动释放有意识的活动。⽹络安全也是如此，它应该在维护服务的同时，根据当前的⽹络情况⾃动 调整和调整系统资源。Kephart 和 Chess 概述了⾃主系统应 具备的四个要素：
⾃我配置 ⾃我修复 ⾃我优化 ⾃我保护，也称为Self CHOP 提出了⼀个参考架构来实现能够适应不断变化的环境的Self CHOP 属性（图 1）。
Monitor、Analyze、Plan、Execute 和 Knowledge 组件统称为 MAPEK 架构。⾃主计算系统并未被设想为单⼀的独⽴系统，⽽是交互的、⾃我管理的系统，它们共同⼯作以实现其⽬标（图2）。
Self CHOP属性在[3]中定义为：
⾃配置：系统通过动态调整其资源来⾃动配置和重新配置⾃⾝的能⼒ ⾃我修复：系统从常规和意外事件中⾃动检测、诊断和修复硬件和软件的能⼒ ⾃我优化：系统和⼦组件监控其部件以检测性能下降同时不断寻求⾃我改进的能⼒ ⾃我保护：系统针对故障和恶意对系统的攻击能力能够⾃动检测和防御 自主安全CHOP属性可以为系统提供⽹络安全⽀持，以保护⾃⾝并从⽹络攻击中恢复。Kephart 将⾃我保护属性描述为保护系统免受⾃我修复属性⽆法纠正的恶意攻击或级联故障。其他Self CHOP属性也⽀持⾃保护属性，⾃修复属性在系统受到攻击后通过更新和脱落受损组件来修复系统，⾃配置属性帮助系统在禁⽤受损⼦系统后重新配置⾃⾝，⾃优化属性可以帮助系统通过攻击运⾏，然后重新配置后重新优化。
⾃主⽹络安全建⽴在⾃主视觉和参考架构的基础上，以开发⾃我管理能⼒，利⽤Self CHOP特性检测⽹络攻击并从⽹络攻击中恢复 [4]。⾃主⽹络安全不仅使⽤⾃主计算的Self CHOP特性，⽽且还互连⾃主元素，以提供⾃主元素之间的信息通信和共享，以共同应对攻击并从攻击中恢复。
三、自主网络安全方法以下是对一系列领域实施自主网络安全的方法的调查，这些领域包括智能汽车、网络物理系统/物联网、工业控制系统、关键基础设施、自组织计算、高性能计算、云计算、企业计算和其他。一些方法集中在架构上，另一些集中在被保护的领域或底层技术上。以下描述了调查的不同方法。它们按照是否使用 MAPEK 参考体系结构、非MAPEK体系结构、实现特定的自主特性、是提供自主开发支持的框架还是自主安全开发工具来分组。
3.1 MAPEK方法3.1.1 认知和自主网络安全Maymir Ducharme等人[5]在2015年描述了使用上下文分析和自主元素的分层网络，该网络从分层结构中的不同抽象层次提供系统元素的不同视图。具有更高级视图的自治元素将具有企业任务级上下文。更高级别的自治元素将根据公司功能的关键程度对这些功能进行优先排序，以便在紧急情况或网络攻击时维持运营。与安全相关的元素，如防火墙，将在层次结构中较低的抽象级别表示。
3.1.2 零信任Eidle等在2017年提出利用autonomic网络安全实现网络零信任[6]。作者开发了一个网络安全测试平台，实现了基于观察、定向、决定、行动(OODA)模型的零信任网络的各个方面，该模型与 MAPEK 参考架构相似，平台观察网络用于监控和分析攻击特征的身份验证日志。他们集成了来自多个网络设备的威胁响应，以简化威胁的检测和缓解危害。
3.1.3 自主车辆网络Le Lann在 2018年描述了未来智能汽车网络如何具有自主组织和自愈特性[7]。车辆网络可以构成由一组车辆组成的系统系统。每辆车都将是一个自主元件，与附近的其他车辆相连。车辆将被动态地连接在一起，形成“队列”，它们像车队一样一起行进，并提供群体安全。车辆可以根据车辆需求动态添加和删除。出于安全目的，加入群组需要身份验证。
3.1.4 使用大数据的分布式计算中的自主入侵响应Vierira等人在2018年使用了自主计算和大数据分析的组合来检测和响应网络攻击[8]。作者为自主入侵响应系统提供了一个参考架构，并开发了一个概念验证实现。作者将其入侵响应参考体系结构建立在 MAPEK 参考体系结构的基础上，网络流量系统日志是通过监控 MAPEK子组件收集的。</description></item><item><title>系统调用级二进制兼容的Unikernel虚拟机</title><link>https://icorer.com/icorer_blog/posts/system_call_level_binary_compatible_unikernel_virtual_machine/</link><pubDate>Mon, 24 Jan 2022 19:00:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/system_call_level_binary_compatible_unikernel_virtual_machine/</guid><description>一、背景Unikernel 是最小的单一用途虚拟机，目前在研究领域非常受欢迎，但是目前想把已有的应用程序移植到当前的unikernel环境是很困难的。HermiTux是第一个提供与Linux应用程序的系统调用级二进制兼容的unikernel，它由一个管理程序和一个模拟负载及运行时Linux ABI的轻量级内核层组成。HermiTux将应用程序开发人员从移植软件的负担中解脱出来，同时通过硬件辅助虚拟化隔离、快速启动时间和低磁盘/内存占⽤、安全性等提供单核优势。通过二进制分析重写技术及共享库替换，可以快速实现系统调用和内核模块化。
论文中展示了HermiTux的独立架构特色，在x86-64和ARM aarch64 ISA架构上展示了一个原型，针对各种云以及边缘/嵌入式部署，也展示了HermiTux对⼀系列原⽣C/C++/Fortran/Python Linux 应⽤程序的兼容性。HermiTux与其他unikernel相比，也提供了相似程度的轻量级，并且在许多情况下与Linux性能相似，它在内存和计算密集型场景下性能开销平均损失3%，其I/O性能是可以接受的。
二、HermiTux对于Linux程序兼容性的思路Unikernels在学术领域变得很流行，它以虚拟化LibOS模型为基础，这种模型也带来了很多好处，主要包括：提高安全性、性能改进、隔离性提升、降低成本等。这样的优势也增加了很多应用场景：云和边缘部署的微服务/基于Saas和Faas的软件、服务器应用程序、NFV、IOT、HPC等。尽管unikernel被视为容器领域具有吸引力的替代品，但unikernels仍然在行业中很难获得显著的牵引力，并且它们的采用率相当缓慢，主要原因是将遗留/现有的应用程序移植到unikernel模型很困难，有时候甚至是不可能的。
在大型程序中，移植复杂的代码库很困难，这是由于诸如不兼容/缺少库/函数、复杂的构建过程、缺乏开发工具（调试器/分析器）、不受支持的语言等因素存在。移植到unikernel环境，也需要程序员具备这方面的专业知识，巨大的移植负担是阻碍广泛采用unikernels最大的障碍之一。
HermiTux提出了一个新型的unikernel模型，他为常规Linux应用程序提供二进制兼容性，同时保留了unikernel的优势，它允许开发工作集中在unikernel这层。HermiTux原型是HermitCore unikernel的扩展，它能够运行原生的Linux可执行文件作为unikernel。通过提供这种基础设施，HermiTux将应用程序员的移植工作转变为unikernel层开发人员的支持工作，在这个模型下不仅可以让原生Linux应用程序透明地获得unikernel的好处，而且还可以运行以前不可移植的应用程序。使用HermiTux将遗留应用程序作为unikernel移植和运行是不存在的，HermiTux支持静态和动态链接的可执行文件、兼容多种语言（C/C++/Fortran/Python等）、编译器（GCC和LLVM）、全面优化（-O3）和剥离/混淆的二进制文件。它支持多线程和对称多处理器（SMP）、检查点/重启和迁移。
大多数现有的unikernel不提供任何类型的二进制兼容性，一些系统通过在C库级别进行接口适配来提供二进制兼容性，其作用类似于动态链接库。这可以防止它们通过系统调用来支持何种需要操作系统服务的应用程序，而无需通过C库。为了保障最大化的兼容性，HermiTux没有采用通用做法，在系统调用级别实现了所有应用程序和编译库使用的标准化接口。
三、HermiTux的挑战HermiTux应对的第一个挑战是如何提供系统调用级别的二进制兼容性？，因此HermiTux根据Linux应用程序二进制接口ABI设置执行环境并在运行时模拟OS接口。基于自定义管理程序的ELF加载程序用于在单个空间虚拟机中与最小内核一起运行Linux二进制文件。程序运行的系统调用被重定向到unikernel提供的实现。
HermiTux应对的第二个挑战是如何在**提供二进制兼容性的同时保持unikernel的好处？**有些是自然具备的好处（小磁盘/内存占用、虚拟化强制隔离），而另一些（快速的系统调用、内核模块化）在假设无法访问源代码时会带来技术挑战。为了实现这些好处，HermiTux对于静态可执行文件使用了二进制重写与分析技术，并在运行时用一个可识别的unikernel的C库替代动态链接库的可执行文件。最后HermiTux针对低磁盘/内存占用和攻击面进行了优化，与现有的unikernel一样低或更低。
由于unikernel应用案例广泛，HermiTux当前目的是兼容服务器和嵌入式虚拟化场景，因此主要支持Intel x86-64和 ARM aarch64 (ARM64) 指令集架构 (ISA) 开发。HermiTux的设计基本原则是独立于架构，但是它的实现以及我们用来恢复unikernel的二进制重写/分析技术是ISA特定的。
总体来说，HermiTux做了以下出色工作：
一种新的unikernel模型，旨在执行本机Linux可执行程序，同时保持经典的unikernel优势。 提供可以在x86-64 和 aarch64 架构上的两个原型实现。 四、关键概念阐述4.1 Unikernelsunikernel是一个应用程序，它使用必要的库和一个精简OS层静态编译成一个二进制文件，能够作为虚拟机在管理程序上执行。Unikernel符合以下条件：
单一目的：一个unikernel只包含一个应用程序 单一地址空间：由于单一目的原则，所以unikernel不需要内存保护，因此应用程序和内核共享一个地址空间，所有代码都以最高权限级别执行。 这样的模型提供了显著的好处，在安全性方面提供了unikernel之间的强隔离，虚拟机管理程序让它成为云部署的良好候选者。此外unikernel仅包含运行给定应用程序所需的必要软件。结合非常小的内核尺寸，和常规VM相比，这导致应用程序攻击面显著减少。一些unikernel也是用提供内存安全保障的语言编写的。关于性能方面，unikernel系统调用很快，因为它们是常见的函数调用，特权级别之间没有代价昂贵的用户态与内核态切换。上下文切换也很快，因为没有页表切换或TLB刷新。除了由于小内核导致代码库减少之外，unikernel OS层通常是模块化的，可以将它们配置为仅包含给定应用程序的必要功能。
所有这些好处让unikernels的应用程序域非常丰富。他们非常适合运行大多数需要高度隔离的云应用程序和需要高性能、低操作系统开销的计算密集型作业的数据中心。unikernel减少的资源使用使它们特别适合嵌入式虚拟化，随着边缘计算和物联网等范式的出现，这个领域的重要性日益增加。由于unikernels的应用领域包括服务器和嵌入式机器，因此论文主要针对intel x86-64和 aarch64架构进行模型构建。
4.2将现有应用程序移植到Unikernel移植现有软件作为unikernel运行是很困难的，特别在某些情况下无法移植程序到unikernel环境，因为所有的程序都需要重新编译与链接。一个给定的unikernel支持一组有限的内核特性和软件库，如果不支持应用程序所需的函数、库或者特定版本的库，则该程序需要进行调整。在许多情况下，缺少函数/库意味着应用程序根本无法移植，此外unikernel使用复杂的构建基础架构，将一些遗留的应用程序（大型的Makefile、autotools、cmake）移植到unikernel工具链会很麻烦，更改编译器或者构建选项也是如此。
在如此大的移植成本上，所以unikernel发展缓慢是有原因的，一种解决方案是让unikernel为常规可执行文件提供二进制兼容性，同时仍保持经典 unikernel 的优势，例如⼩代码库/占⽤空间、快速启动时间、模块化等。这种新模型允许 unikernel 开发⼈员致⼒于推⼴unikernel 层以⽀持最⼤应⽤程序的数量，并减轻应⽤程序开 发⼈员的任何移植⼯作。这种⽅法还应该⽀持调试器等开发⼯具。在这种情况下，HermiTux 允许将 Linux ⼆进制⽂件作为unikernel 运⾏，同时保持上述优势。
4.3 轻量级虚拟化设计空间轻量级虚拟化设计空间包含unikernel、面向安全的LibOS、例如Graphene，以及带有软件和硬件强化技术的容器。HermiTux不需要应用程序移植工作，并且和其他二进制兼容的系统方案不同，不同点主要包括：
作为unikernel HermiTux运行硬件强制（扩展页表）虚拟机，这是一种从根本上比软件强制隔离（容器/软件LibOS）更强的隔离机制。当前在VM中运行容器以确保安全的趋势（clear containers）加强容器隔离的努力（gVisor）都表明了这一点，这通常用作支持unikernel与容器的安全依据。 HermiTux使更广泛的应用程序能够透明地无需任何移植工作即可获得unikernel的好处，无需修改代码以及维护单独分支的潜在复杂性。鉴于unikernel提供的安全性和减少占用空间的特性，这在当今的计算机系统环境中非常有价值，软件和硬件漏洞经常成为新闻，并且数据中心架构师正在寻求增加整合和减少资源/能源消耗的方法。二进制兼容允许HermiTux成为专有软件（其源代码不可用）作为unikernel运行的唯一方法。最后，HermiTux允许软件获得VM的传统优势，例如检查点/重启/迁移，而无需大量磁盘/内存占用的相关开销。 4.4 系统调用级二进制兼容两个现有的unikernel已经生成和应用程序的二进制兼容，OSv和Lupin Linux。需要注意的是，两者都提供二进制兼容性标准C库（libc）级别，unikernel包含一个动态加载程序，它在运行时捕获对libc函数的调用，例如printf、fopen并将它重定向到内核。
这种接口方法意味着假设所有系统调用都是通过libc进行的，当考虑到各种各样的现代应用程序二进制文件时，这并不成立。我们分析了整个Debian10 x86-64存储库 (主要，贡献 和 ⾮免费） 并统计了553个ELF可执⾏⽂件，包括⾄少⼀次调⽤系统调⽤指令：这些代表不通过libc 执⾏系统调⽤程序，因此 libc 级别的⼆进制兼容unikernel 不⽀持这些程序。这种有限的libc级兼容性组织了这些系统运行相对较大范围的应用程序，这些应用程序将从座位unikernel的执行中受益匪浅。举几个例子，大量的云服务是用Go语言编写的，Go是一种无须标准C库即可执行大多数系统调用的语言。此外，由于在系统调用层面缺乏兼容性，OSv不支持最流行的HPC共享内存编程框架OpenMP，最后libc接口排除了对静态二进制文件的支持。
HermiTux代表了一种尝试，通过增加一个更加标准和一致使用的接口（系统调用级别）上进行接口来进一步推动unikernel的兼容性程度。</description></item><item><title>阿里云FAASNET无服务器容器方案</title><link>https://icorer.com/icorer_blog/posts/alibaba_cloud_faasnet_serverless_container_solution/</link><pubDate>Tue, 18 Jan 2022 21:24:16 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/alibaba_cloud_faasnet_serverless_container_solution/</guid><description>背景这篇论文中采用容器化方案来实施ServerLess的落地过程，论文内部一方面基于大量的数据统计、一方面提出来FT树结构，用来优化容器冷启动。FAASNET是第一个为FaaS优化的容器运行时提供的端到端综合解决方案，FAASNET使用轻量级、分散和自适应的函数树来避免主要平台的瓶颈。
大会地址：https://www.usenix.org/conference/atc21/presentation/wang-ao
开源地址：https://github.com/mason-leap-lab/FaaSNet
一、网络流量的峰谷比 就如上图所示，不同的应用场景下，流量的高峰和低峰的请求比例是不一样的，比如游戏、IOT场景下流量的峰谷比高于22，这种峰谷比也表明了ServerLess场景的优势。
二、容器的冷启动情况冷启动的延迟对于Faas提供商是致命的，阿里巴巴首先对于冷启动的分布情况作了调研：
对于北京地区，大约57%的镜像拉取时间超过45秒 对于上海地区，超过86%的镜像拉取时间至少需要80秒 显示超过50%和60%的函数调用请求花费至少80%和72%的整体函数启动时间来拉取容器镜像，这表明镜像拉取时间成为了大多数功能的冷启动成本。 冷启动的成本，还需要结合冷启动的间隔时间和功能持续时间来综合评价，在两个地区内部，大约49%的功能冷启动的到达时间小于1秒。
三、FAASNET技术内幕 在图3(d)中可以看出北京地区的80%函数执行时间超过1秒，上海地区80%的函数执行时间小于32.5秒，90th百分位数为36.6秒，99th百分位数为45.6秒。这种分布说明冷启动优化是必要的。
优化容器配置的性能将为降低基于容器的云功能的冷启动成本带来巨大的好处。
2.1 设计概述FAASNET将跨虚拟机的容器配置分散化和并行化，引入了名为函数树（FT）的抽象，以实现高效的容器配置规模。FAASNET将FT管理器组件和一个工作者组件整合进入FAAS调度器和虚拟机代理中，以协调FT管理，阿里的Faas平台主要包含以下几个组成部分，工作的主体组成包括：
网关：租户身份管理认证，将函数请求转发给FAAS调度器，将常规的容器镜像转换为I/O高效数据结构， 一个调度器负责为函数调用请求提供服务，将FAASNET FT管理器集成到调度器来管理函数数、简称FT，通过FT的增删API进行管理。一个FT是一个二进制的树状覆盖，它连接多个虚拟机，形成一个快速和可扩展的容器供应网络。每个虚拟机运行一个FAAS代理，负责虚拟机本地的功能管理。将一个FAASNET工作者集成到VM代理中，用于容器的供应任务。 在函数调用的路径上，如果没有足够的虚拟机、或者所有的虚拟机都很忙的情况下，调度器首先与虚拟机管理器进行通信，从空闲的虚拟机池中扩展出活动的虚拟机池。然后调度器查询其本地的FT元数据，并向FT的FAASNET工作者发起RPC请求，从而启动容器供应流程。容器运行时供应过程实际上是分散的， 并在FT尚未有容器运行的本地供应的虚拟机进行准备工作。调度器在关键路径之外，而FAASNET工作层根据需求获取函数容器层，并从分配的对等虚拟机中并行地创建容器运行时。 在函数部署路径上，网关将函数的常规容器镜像转换为I/O的有效格式，从面向租户的容器注册表中提取常规镜像，逐块压缩镜像层，创建一个包含格式相关信息的元数据文件（镜像清单），并将转换后的层及其清单分别写入阿里云内部的容器注册表和元数据存储。 2.2 FT功能树论文中针对重点强调在设计FT时做了以下选择：
FT是和函数进行绑定的，FAASNET以函数为粒度来管理FT。 FT具备解耦的数据面和控制面，FT的每个虚拟机工作者都具有等同的、简单的容器供应（数据平面）的角色，而全局树管理（控制平面）则交给调度器。 FAASNET采用平衡的二叉树结构，可以动态的适应工作负载。 这些选择结合阿里云，可以达到以下目标：
最大限度的减少容器镜像和层数据下载的I/O负载。 消除中央根节点的树状管理瓶颈和数据播种瓶颈、这里阿里内部镜像采用P2P分发，播种友好。 适应虚拟机的动态加入和离开。 以函数的粒度管理树， FAASNET为每一个至少被调用过一次但未回收的函数管理一个单独、唯一的树。图5说明一个横跨5个虚拟机的三级FT拓扑结构。函数容器镜像从书的根部虚拟机往下流，直到达到叶子节点。
平衡的二叉树，FAASNET的核心是平衡的二进制树，在二进制树中，除了根节点和叶子节点，每个树节点（宿主虚拟机）有一条传入边和两条传出边。这种设计可以有效限制每个虚拟机的并发下载操作的数量，以避免网络争用。一个有N个节点的平衡二叉树的高度为log(N)，这种关系也限制了函数镜像和层数据从顶部到底部的最多跳跃次数。树的高度会影响数据传播的效率，并且二叉树的结构可以动态变化，以适应工作负载的动态化。FAASNET把每个FT组织成一个平衡的二叉树，FT管理程序调用两个API：增加和删除，以动态地增加或缩小一个FT。
插入，FT的第一个节点会被当做根节点插入，FT管理器通过BFS（广度优先搜索）跟踪每个树节点的子节点数量，并将所有拥有0或1个子节点的节点存储在一个队列中。要插入一个新节点，FT管理器会从队列中挑选第一个节点作为新节点的父节点。
删除，调度器可能会回收闲置了一段时间的虚拟机（阿里云配置为15分钟），因此FAAS虚拟机的寿命是有限的。为了使用这种虚拟机的回收，FT管理器调用删除来回收虚拟机。删除操作也会在需要的时候重新平衡FT的结构。与二进制搜索树（如AVL、红黑树）不同，FT的节点没有可比较的键值（及其相关值）。因此，FT树的平衡算法只有当任何节点的左右子树的高度差大于1就会触发平衡操作。
2.3 FT与FAAS整合论文中的FT整合是在阿里云的FAAS环境中，主要整合了FAAS平台的调度器和虚拟机代理。阿里把FAASNET的FT管理器集成到阿里云的FAAS调度器中，并将FAASNET的VM工作者集成到阿里云的FASS-VM代理中用于调度管理FT的虚拟机。
通过FT管理者，调度器在每个虚拟机代理上启动一个FAASNET工作者，工作者负责：
为调度员的命令提供服务，执行镜像下载和容器供应的任务 管理虚拟机上的函数容器。 FT元数据管理，调度器维护一个内存映射表，记录&amp;lt;functionID,FT&amp;gt;键值对，他将一个函数ID映射到其相关的FT数据结构。一个FT数据结构管理着一组代表函数和虚拟机的内存对象，以跟踪虚拟机的地址：端口信息。调度器是分片的，是高度可用的。每个调度器分片会定期将内存中的元数据状态与运行etcd的分布式元数据服务器同步。
函数在虚拟机上的放置，为了提高效率，FAASNET允许一个虚拟机容纳属于同一个用户的多个函数。只要虚拟机有足够的内存来承载函数，一个虚拟机可能参与到多个重叠的FT的拓扑结构中。
图8显示了一个可能的FT布局的例子，为了避免网络瓶颈，FAASNET限制了一台虚拟机可以放置的函数数量，目前设置是20个。
容器供应协议，FAASNET设计了一个协议来协调调度器和容器之间的RPC通信。
调度器和FAASNET的虚拟工人，并促进容器的供应。在一个调用请求中，如果调度器发现没有足够的活动虚拟机为请求提供服务，或者当前所有虚拟机都忙于为请求提供服务，调度员会从空闲的虚拟机池中保留一个或多个新的虚拟机，然后进入容器供应流程。
当调度器将函数元数据发送给VM，VM一旦收到信息会执行两个任务。从元数据存储库加载并检查清单，获取镜像层的URL，并把URL信息持久化到VM的本地存储中。VM回复调度器表明自己已经准备好开始为请求的函数创建容器运行时，调度器收到回复后向VM发送一个创建容器的RPC请求，VM处理清单配置，并向调度器发送一个RPC表明容器已经成功创建。
FT容错，调度器定期ping虚拟机，可以快速检测虚拟机故障。如果一个虚拟机发生故障，调度器会通知FT管理器执行树平衡操作以修复FT拓扑结构。
2.4 FT设计讨论FAASNET将元数据繁重的管理任务卸载到现有的FAAS调度器上，因此每个单独节点都扮演着从其父级对等获取数据的相同角色。FT的根节点没有父级对等物，而是从注册表中获取数据。FAASNET的FT设计可以完全消除到注册中心的I/O流量，只要一个FT至少有一个活跃的虚拟机存储所请求的容器。早些时候，我们的工作负载分析显示，一个典型的FAAS应用的吞吐量将始终高于0RPS，在实践中请求突发更有可能讲一个FT规模从1到N，而不是从0到N。
另一种设计是更细粒度的层（blobs）来管理拓扑关系。在这种方法中，每个单独的层形成一个逻辑树层，属于一个函数的容器镜像的层最终可能驻留在不同的虚拟机上。注意FAASNET的FT是层树模型的一个特例。
图10中显示了一个例子，在这个例子中，一个虚拟机中存储着不同函数容器镜像的层文件，因此当许多下游的虚拟机同事从这个虚拟机获取层时，可能会出现网络瓶颈。这是因为许多重叠的层树形成了一个完全连接的、端对端的网络拓扑结构。如果虚拟机用高带宽的网络连接，全对全的拓扑结构可能会有很好的规模。然而如果每个虚拟机都收到了资源限制，全对全的拓扑结构很容易造成网络瓶颈，阿里云内部使用的是2核CPU、4G内存、1Gbps网络的小型VM。
现有的容器分配技术依靠强大的根节点来完成一系列任务，包括数据播种、元数据管理、P2P拓扑结构管理。将这些框架移植到FAAS平台上，需要额外的、专用的、分片的根节点，这将给运营商增加不必要的成本。另一方面，FAASNET的FT设计使每个虚拟机工作者的逻辑保持简单，同时所有的调度逻辑卸载到现有的调度器。这种设计自然消除了网络I/O瓶颈和根节点的瓶颈。Kraken采用了基于层的拓扑结构，具有强大根节点。
2.5 优化I/O高效的数据格式，常规的docker pull 和 docker start是低效和耗时的，因为整个容器镜像和所有层的数据都必须从远程容器注册中心下载，然后才能启动容器。为了解决这个问题，阿里云内部设计了一个新的基于块的镜像获取机制，这种机制使用了一种I/O高效的压缩数据文件格式。原始数据被分割成固定大小的块，并分别进行压缩。一个偏离表被用来记录压缩文件中每个压缩块的偏移量。
FAASNET使用相同的数据格式来管理和配置代码包。一个代码包被压缩成一个二进制文件，它被虚拟机代码提取并最终安装在一个函数容器内。FAASNET分配代码包的方式与分配容器镜像的方式相同。
按需I/O，对于不需要在启动时一次性读取所有镜像层的应用程序，基于镜像块的获取方式提供了一个懒惰的按需方式从远程存储获取细粒度的镜像层数据。一个FAASNET的VM工作者从元数据存储中下载镜像的清单文件，并在本地进行镜像加载以加载.tar镜像清单，然后它计算第一个和最后一个压缩块的索引，然后查询偏移表以找到偏移信息。最后，它读取压缩块并解压，知道读取的数据量与要求的长度一致。由于底层（远程）块存储设备的读取必须是块边界对齐，应用程序可能会读取和解压比要求的更多的数据，造成读取放大。然而，在实践中，解压算法实现的数据吞吐量比块存储或网络的数据吞吐量高的多。在我们的使用场景中，用额外的CPU开销换取降低I/O成本是有益的。
RPC和数据流，FAASNET内部建立了一个用户态、零拷贝的RPC库。这种方法利用非阻塞的TCP sendmsg和recvmsg来传输一个 struct iovec 不连续的缓冲区。RPC库把RPC头直接添加到缓冲区，以便在用户空间实现高效、零拷贝的序列化。RPC库对请求进行标记、以实现请求流水线和失序接收，类似HTTP2的多路复用。当FAASNET工作者受到一个完整的数据块时，工作者会立即将该数据块传输给下游的节点。
三、FAASNET评测3.1 实验方法使用中等规模500个虚拟机池和一个大规模的1000个虚拟机池，所有的虚拟机均使用2核CPU、4GB内存、1Gbps网络的实例类型，维护一个免费的虚拟机池，FAASNET可以保留虚拟机实例来启动云函数。这样容器配置的延迟就不包括冷启动虚拟机实例的时间，FAASNET使用512KB的块大小，用于按需取用。
系统比较，FAASNET和一下三种配置进行比较。
Kraken，Uber的基于P2P的注册系统。 baseline，阿里巴巴云函数计算目前的生产设置，使用docker pull 从集中的容器中心下载镜像。 on-demand，一个基于baseline的优化系统，但按需获取容器层数据。 DADI+P2P，阿里巴巴的DADI启动了P2P，这种方法使用一个资源受限的虚拟机作为根节点来管理P2P拓扑结构。 目的，回答以下问题：</description></item><item><title>⽤于区块链可扩展性的⾼效能 FPGA-Redis 混合 NoSQL 缓存系统</title><link>https://icorer.com/icorer_blog/posts/blockchain_fpga_redis_nosql/</link><pubDate>Tue, 25 May 2021 22:35:12 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/blockchain_fpga_redis_nosql/</guid><description>一、FPGA-Redis介绍鼓舞人心的区块链技术在加密货币以外的领域取得了很多采用和成功领域落地，因为它的好处已经被探索和成功测试。可扩展性是区块链的最大挑战之一，许多设备（轻量级节点）尤其是物联网依赖于完整的区块链服务器，因此需要减少服务器上的工作负载以获得高性能。这篇论文提出了一种高性能、高效的混合（多级）和分布式NoSQL缓存系统，用于提高区块链应用程序的可扩展（吞吐量）。我们研究了区块链中的性能瓶颈，并设计了一种高效的千兆以太网FPGA NoSQL缓存架构，该架构通过Hiredis C客户端与Redis数据库协同工作。Curl和Jansson用于连接区块链。我们为特定于区块链的高效缓存设计了一个定制的SHA-256核心。我们的结果显示，当FPGA上发生缓存命中时，性能提高了103倍。所提出的FPGA-Redis系统获得了高达4.09倍的改进，还实现了较小的FPGA面积利用率和较低的功耗。
二、概述区块链技术激励了许多人，帮助许多企业和政府改进系统，解决了信任、安全、速度、成本、效率和中心化等诸多瓶颈问题。英国政府办公室的报告确认区块链可以保护数据、降低成本并为记录提供透明度。区块链由中本聪于2008年首次提出并支持加密货币（比特币）和许多其他用于医疗保健、身份管理、网络安全等应用程序。Corda是R3的区块链（由200多家公司组成的联盟，主要是金融机构），用于增强商业交易和网络，R3一直在为企业使用和探索区块链。
尽管区块链很强大，但可扩展性（低吞吐量、高延迟、存储问题和读取性能差）是其巨大的挑战，但研究较少。与非区块链应用程序相比，区块链应用程序的吞吐量要低很多。比特币和以太坊支持每秒3-4和15-20笔交易（TPS），而Visa和PayPal分别支持1667和193TPS，另一方面，与非区块链服务器相比，区块链服务器的查询响应（读取性能）也很差。例如在我们处理约每秒96个响应的区块链系统中，查询延迟超过10毫秒。同样，Blockcypher 区块链服务器⽀持每秒3个请求。与⾮区块 链服务器相⽐，Google和YouTube分别处理每秒84,405个请求和每秒85,021次观看。糟糕的读取性能是由于区块链的结构和巨大的尺寸（比特币超过288GB）以及区块链数据存储在硬盘上的事实。与用于存储Redis等NoSQL缓存的RAM（快150,000倍）不同，硬盘具有较高的访问延迟。由于这种糟糕的读取性能，现有的区块链无法处理有效服务器所需的每秒大量客户端请求。许多轻量级节点（数以千计的物联网设备和简化验证（SPV）节点），仅依赖区块链服务器来获取区块链数据，因为其庞大的规模，它们无法存储完整的区块链。现在越来越多的轻量级客户端使用区块链并将更多的工作放在区块链服务器上，因此必须减少区块链服务器上的工作量以提升性能，从而更好地扩展区块量应用程序。
NoSQL缓存是提高和增强区块量服务器读取性能的一种有效方式。Redis、Hadoop和Memcached等NoSQL缓存如今已广泛用于大型Web数据中心，例如Yahoo、Twitter、Facebook、Youtube甚至Google，其中数百个分布式NoSQL部署缓存服务器是为了改善许多性能和可伸缩问题并节省成本。NoSQL缓存具有非常高的性能进行大规模水平扩展的优势，并且比使用更强大的CPU和内存（垂直扩展）更新现有服务器更经济。水平扩展是指使用廉价商品服务器的副本来获得更好的性能，而不是传统的垂直扩展，其中将更强大的资源添加到单个服务器使系统更加昂贵。仅苹果公司就使用了超过75000个NoSQL缓存（Cassandra）表格系列集群来存储超过10PB的数据。
分布式 NoSQL 缓存由于其⾼性能以及区块链请求（尤其是块头请求）的时间局部性，可以极⼤地提⾼区块链服务器响应的吞吐量和延迟性能。许多轻量级节点（如简化⽀付验证节点（特殊⽬的公司 )在添加新块的⼏个⼩时内。
尽管具有⾼性能，但软件 NoSQL 缓存在⾼性能时会消耗⾼功率和更多CPU 资源（在⽹络处理上）。因此，当 FPGA 发⽣缓存命中时，使⽤ FPGA来降低功耗和 CPU 资源消耗并提⾼性能。然⽽，FPGA 中的⼩尺⼨和有限的内存给可以缓存在 FPGA 上的数据量带来了缺陷和限制，从⽽通过增加 FPGA 的未命中率来影响系统性能。
本⽂研究了区块链中的性能瓶颈，并提出了⼀种⾼效的⾼性能混合分布式NoSQL FPGA-Redis 缓存系统，以减少区块链服务器的⼯作负载并提⾼其性能。我们设计并实现了⼀个千兆以太⽹ FPGA ⽹络接⼝控制器 (NIC)，该控制器包含键值存储，⽤于有效地缓存 FPGA 上的区块链数据。Redis 软件缓存和 Redis 应⽤程序内置在 Redis 服务器 PC 中，它通过 FPGA 上实现的千兆总线主控直接内存地址 (BMD) PCI Express (PCIe) 端点连接到 NIC。Redis 应⽤程序使⽤Hiredis API（实现与Redis缓存对话的Redi 的C客⼾端）。整个缓存系统通过我们的服务器应⽤程序中内置的Curl和Jansson API 连接到全节点区块链服务器。
该系统改善了FPGA NoSQL缓存内存⼩的缺点，同时以更低的功耗提⾼了 软件缓存的性能。FPGA 和 Redis 协同⼯作。Redis 通过提供另⼀个缓存层来补充 FPGA 缓存的有限内存。当在 FPGA 上未找到请求的数据（发⽣缓存未命中）时，数据从 Redis（如果缓存）⽽不是存储在主内存中的主区块链中获取。由于 Redis ⽐主存更快，因此整个系统的性能得到了提⾼。反过来，FPGA 通过处理⽹络处理来降低Redis的⾼性能和CPU资源消耗。我们只在FPGA和Redis（包括缓存）上缓存频繁的请求（即块头、确认、块⾼度、时间跨度和 Merkle根），⽽在Redis上缓存不频繁和⼤数据请求（例如块请求）只要。此外，仅当FPGA上发⽣缓存未命中时才检查Redis 缓存。</description></item><item><title>TCP长连接在K8S环境下的负载均衡分析</title><link>https://icorer.com/icorer_blog/posts/cloudnative_k8s_tcp_upstream_balance/</link><pubDate>Fri, 25 Dec 2020 11:28:12 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/cloudnative_k8s_tcp_upstream_balance/</guid><description>K8S不支持长连接的负载均衡，所以负载可能不是很均衡。如果你在使用HTTP/2，gRPC, RSockets, AMQP 或者任何长连接场景，你需要考虑客户端负载均衡。
TL;DR: Kubernetes doesn&amp;rsquo;t load balance long-lived connections, and some Pods might receive more requests than others. If you&amp;rsquo;re using HTTP/2, gRPC, RSockets, AMQP or any other long-lived connection such as a database connection, you might want to consider client-side load balancing.
Kubernetes提供了两种方便的抽象来部署应用程序：Services 和 Deployments。 Deployments描述了在任何给定时间应运行哪种类型以及多少个应用程序副本的方法。每个应用程序都部署为Pod，并为其分配了IP地址；另一方面，Services类似于负载平衡器。它们旨在将流量分配给一组Pod。
将Services视为IP地址的集合通常很有用。每次您对Services提出请求时，都会从该列表中选择一个IP地址并将其用作目的地。 如果您有两个应用程序（例如前端和后端），则可以为每个应用程序使用Deployment和Service，然后将它们部署在集群中。 当前端应用发出请求时，不需要知道有多少Pod连接到后端服务；前端应用程序也不知道后端应用程序的各个IP地址。当它想要发出请求时，该请求将发送到IP地址不变的后端服务。 但是该服务的负载平衡策略是什么？
Kubernetes Services中的负载平衡Kubernetes Services不存在，没有进程监听服务的IP地址和端口。
您可以通过访问Kubernetes集群中的任何节点并执行netstat -ntlp来检查是否存在这种情况。
甚至在任何地方都找不到IP地址,Services的IP地址由控制器管理器中的控制平面分配，并存储在数据库etcd中。然后，另一个组件将使用相同的IP地址：kube-proxy。
Kube-proxy读取所有Services的IP地址列表，并在每个节点中写入一组iptables规则。这些规则的意思是：“如果看到此Services IP地址，则改写请求并选择Pod之一作为目的地”。Services IP地址仅用作占位符-这就是为什么没有进程监听IP地址或端口的原因。 iptables是否使用轮询？不，iptables主要用于防火墙，并且其目的不是进行负载平衡。但是，您可以制定一套聪明的规则，使iptables像负载均衡器一样工作。而这正是Kubernetes中发生的事情。
如果您有三个Pod，则kube-proxy编写以下规则：
选择Pod 1作为目的地，可能性为33％。 否则，移至下一条规则 选择Pod 2作为目的地，可能性为50％。 否则，请移至以下规则 选择Pod 3作为目的地（没有可能性） 复合概率是Pod 1，Pod 2和Pod 3都有三分之一的机会被选中（33％）。</description></item><item><title>微服务治理：APM-SkyWalking-PHP内核扩展源码分析</title><link>https://icorer.com/icorer_blog/posts/microservice_skywalking_php_kernel_source_analyze/</link><pubDate>Mon, 07 Sep 2020 14:42:13 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/microservice_skywalking_php_kernel_source_analyze/</guid><description>SkyWalking APM作为服务遥测的关键技术点，为了能够更好地运用这项技术，我们需要拥有把握这项技术的底层能力。目前公司在PHP领域存活不少业务系统，针对PHP领域的APM技术，我们首先从分析这款PHP内核扩展程序下手。
一. 总体架构PHP内核在php-fpm运行模式下是短生命周期，短生命周期的脚本运行如果直接连接SkyWalking的oap-server会造成大量的性能损耗，而且php也不擅长grpc通信，因此借助mesh架构思想为PHP-FPM进程池增加一个数据SideCar，主要的结构如下图所示： 从上图可以看出，PHP内核扩展程序拦截内核运行数据（主要是关键的外部IO调用）、数据被发送给SideCar，SideCar流转数据到SkyWalking-Server，数据流还可以被SkyWalking进行分析、从而通过WebHook流转报警时间到相关后续平台里。
二. PHP内核扩展源码分析针对目前开源社区的SkyWalking-PHP内核源码进行分析，源码的分析主要包括以下几部分：
工程结构分析 关键生命周期分析 关键运行函数Hook分析 2.1 工程结构分析SkyWalking PHP内核组件工程结构比较简单，主要是站在PHP内核基础上进行扩展设计与实现，主要包含的扩展文件有：
b64.h：base64编码函数的头文件、主要包含内存分配、base64字符表、b64_encode及b64_decode、b64_decode_ex的函数声明。 decode.c：base64序列化的函数具体实现。 encode.c：base64反序列化的函数具体实现。 components.h：针对skywalking协议中的component部分进行宏定义、这部分是apm协议的一部分，例如：tomcat、httpclient、dubbo、okhttp、grpc、jedis、更多查看附录1。 php_skywalking.h：关键的内核扩展声明部分，主要包括：APM协议宏定义、Redis指令类别、memcache指令类别、ContextCarrier上下文结构体、apm拦截所需的关键函数定义（具体见附录二），apm关键函数hook定义（具体见附录三），全局变量定义（具体见附录四）。 skywalking.c：具体内核扩展实现文件，里面包含了MI-MS、RI-RS、关键函数Hook等处理逻辑。 2.2 关键生命周期分析这块将针对内核扩展关键生命周期进行分析。
2.2.1 关键生命期函数Hook定义1static void (*ori_execute_ex)(zend_execute_data *execute_data); //PHP内核原始PHP层执行流程函数指针 2static void (*ori_execute_internal)(zend_execute_data *execute_data, zval *return_value);//PHP原始内核执行函数指针 3ZEND_API void sky_execute_ex(zend_execute_data *execute_data);//skywalking针对PHP层执行函数的替换指针 4ZEND_API void sky_execute_internal(zend_execute_data *execute_data, zval *return_value);//skywalking针对原始内核执行函数的替换指针 2.2.2 php.ini配置解析周期 1PHP_INI_BEGIN() 2#if SKY_DEBUG 3 STD_PHP_INI_BOOLEAN(&amp;#34;skywalking.enable&amp;#34;, &amp;#34;1&amp;#34;, PHP_INI_ALL, OnUpdateBool, enable, zend_skywalking_globals, skywalking_globals) //读取skywalking.enable配置项 4#else 5 STD_PHP_INI_BOOLEAN(&amp;#34;skywalking.enable&amp;#34;, &amp;#34;0&amp;#34;, PHP_INI_ALL, OnUpdateBool, enable, zend_skywalking_globals, skywalking_globals) //读取skywalking.enable配置项 6#endif 7 STD_PHP_INI_ENTRY(&amp;#34;skywalking.</description></item><item><title>微服务治理：服务遥测之APM-SkyWalking技术应用</title><link>https://icorer.com/icorer_blog/posts/microservice_governance_apm_application/</link><pubDate>Mon, 07 Sep 2020 13:33:12 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/microservice_governance_apm_application/</guid><description>一. 背景描述微服务应用过程中，如何构建微服务的可观测性，主要从以下三个方面进行考虑：
服务日志（log） 服务指标（metric） 服务链路（trace） 这三个服务监控领域有不同的技术栈进行支撑，但是如何快速构建一个基础的服务可观测能力？尽量减少业务的侵入性、尽量多的增加业界标准的观测指标，这里我就推荐APM技术体系，在APM技术领域中SkyWalking是一个优秀的解决方案。 二. 技术结构SkyWalking 在我当前公司的落地领域中，主要围绕PHP、Go两大技术领域，JAVA生态拥有SkyWalking默认友好支撑，针对PHP、Go这两种技术栈，主要包含的APM体系技术结构如下图所示： 从技术结构图可以看出，APM技术体系主要包括以下几部分：
技术结构最底层采用了apache SkyWalking开源项目作为方案支撑。 Go生态使用Go2sky客户端进行APM数据丰富与数据包发送。 PHP生态由于自身短生命期的特征，分为PHP内核APM扩展和数据中转SideCar两部分，PHP内核扩展通过函数Hook机制完成Redis、MySQL、PDO、grpc等关键网络IO的拦截，并无感构建APM数据包结构，在RS周期发送APM数据包到SideCar，SideCar负责流转PHP内核的APM监控数据包到APM-Server上。 三. 关键领域监控APM技术生态包含内容比较多，主要的使命就是对于服务应用进行运行态监控，这里主要阐述一下几方面的监控效果：
3.1 服务指标监控服务指标监控主要包括Apdex、平均响应时间、成功率、CPM、TP数据、也包括很多的服务EndPoint数据，主要用来阐述服务健康、性能、可靠性的指标数据。 3.2 服务调用链监控微服务场景下，调用关系复杂、服务调用关系层级深，所以APM构建了服务调用链监控体系，方面研发、架构对于自己服务的调用关系有较好的可视化效果，调用链也遵循OpenTracing协议，主要效果如下所示： 3.3 微服务内核Runtime监控服务监控除了需要对于服务自身的可靠性、服务之间的调用关系进行监控之外，还需要针对服务Runtime进行拦截分析，通常的实现方式有OAP编程、内核Runtime Hook方式，Runtime监控可以很好的监控服务不同EndPoint内部的关键不稳定点的性能情况，除了PDO、Redis、Mysql、GRPC等关键IO，也可以监控长时间的cpu计算等程序行为逻辑。 主要的效果图如下： 3.4 微服务拓扑关系监控针对微服务调用关系，除了可以使用全链路Trace这种表达形式，也可以通过更具有动感效果的拓扑关系图进行描述，在拓扑关系图中可以形象的显示服务的类别、服务的流量走向、服务的当前状态、服务调用间的频率等数据。相关的效果图如下所示: 三. 总结APM技术体系对于微服务治理工作有超强的观测领域能力的弥补，增强服务的可观测程度，是微服务治理的重要工作。</description></item><item><title>UtahFS: Encrypted File Storage - 加密文件存储</title><link>https://icorer.com/icorer_blog/posts/utahfs_encrypted_file_storage/</link><pubDate>Sun, 14 Jun 2020 10:17:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/utahfs_encrypted_file_storage/</guid><description>加密是最强大的技术之一，每个人每天都在不知不觉中使用它。传输层加密现在已经无处不在，因为它是创建可信赖的Internet的基本工具，它可以保护通过Internet发送到目标目的地的数据。磁盘加密技术可以无所不在地保护您的数据，因为它可以防止任何窃取您设备的人也能够看到您台式机上的内容或阅读您的电子邮件。
这项技术的下一个改进是端到端加密，它是指只有最终用户才能访问其数据的系统，而没有任何中间服务提供商。这类加密的一些最流行的例子是WhatsApp和Signal等聊天应用。端到端加密显著降低了用户数据被服务提供商恶意窃取或不当处理的可能性。这是因为即使服务提供商丢失了数据，也没有人拥有解密数据的密钥！
几个月前，我意识到我的计算机上有很多敏感文件（我的日记，如果你一定知道的话），我担心会丢失，但我不喜欢将它们放入Google Drive或Dropbox之类。尽管Google和Dropbox是绝对值得信赖的公司，但它们不提供加密功能，在这种情况下，我确实希望完全控制自己的数据。
环顾四周，我很难找到符合我所有要求的东西：
会同时加密和验证目录结构，这意味着文件名是隐藏的，其他人不可能移动或重命名文件。 查看/更改大文件的一部分不需要下载并解密整个文件。 是开源的，并且有一个文档化的协议。 所以我开始建立这样一个系统！最终我把它称为“ UtahFS”，其代码在此处提供。请注意，这个系统在Cloudflare的生产中没有使用：它是我在我们的研究团队工作时构建的概念。这篇博客文章的其余部分描述了我为什么要像以前那样构建它，但是如果您想跳过它，则代码仓库中有关于实际使用它的文档。
Storage Layer(存储层)存储系统的第一个也是最重要的部分是…存储。为此，我使用对象存储，因为它是在别人的硬盘上存储数据的最便宜和最可靠的方法之一。对象存储只不过是一个由云提供程序托管的键值数据库，通常被调整为存储大约几千字节大小的值。有许多具有不同定价方案的不同提供商，例如Amazon S3，Backblaze B2和Wasabi。它们全部都能够存储TB级的数据，并且许多还提供地理冗余。
Data Layer(数据层)对我来说很重要的一个要求是，在能够读取一部分文件之前，不必下载和解密整个文件。这一点很重要的一个地方是音频和视频文件，因为它能够快速开始播放。另一个例子是ZIP文件：许多文件浏览器都具有浏览压缩档案（例如ZIP文件）的能力，而无需将其解压缩。要启用此功能，浏览器需要能够读取存档文件的特定部分，仅解压缩该部分，然后移动到其他位置。
在内部，UtahFS从不存储大于配置大小（默认为32 KB）的对象。如果文件中的数据量超过该数据量，则该文件将分成多个对象，这些对象通过跳表连接。跳表是链接列表的稍微复杂一点的版本，它允许读者通过在每个块中存储指向比指向前一跳更远的其他指针来快速移动到随机位置。
当跳表中的块不再需要时，因为文件已被删除或截断，它们将被添加到特殊的“回收站”链接列表中。例如，当需要在其他位置使用块时，可以回收垃圾列表的元素，以创建新文件或将更多数据写入现有文件的末尾。这将最大限度地重用，意味着仅当垃圾箱列表为空时才需要创建新块。一些读者可能认为这是《计算机编程艺术：第一卷，2.2.3节》中描述的链接分配策略！ 使用链接分配的根本原因是，对于大多数操作而言，这是最有效的。 而且，这是一种分配内存的方法，该方法将与我们在接下来的三个部分中讨论的加密技术最兼容。
Encryption Layer(加密层)既然我们已经讨论了如何将文件分成块并通过跳表进行连接，我们就可以讨论如何实际保护数据。这有两个方面：
第一个是机密性，它对存储提供者隐藏每个块的内容。 只需使用AES-GCM加密每个块，并使用从用户密码中获得的密钥，即可实现这一点。
该方案虽然简单，但不提供前向保密或后向安全。前向保密意味着，如果用户的设备遭到破坏，攻击者将无法读取已删除的文件。后泄露安全性意味着一旦用户的设备不再泄露，攻击者将无法读取新文件。不幸的是，提供这两种保证之一意味着在用户的设备上存储加密密钥，这些密钥需要在设备之间同步，如果丢失，将使存档无法读取。
此方案也无法防止脱机密码破解，因为攻击者可以获取任何加密的块，并一直猜测密码，直到找到有效的块为止。通过使用Argon2（这使得猜测密码更为昂贵）和建议用户选择强密码，可以在一定程度上缓解这种情况。
我肯定会在将来改进加密方案，但认为上面列出的安全属性对于初始发行版来说太困难和脆弱。
Integrity Layer(完整性层)数据保护的第二个方面是完整性，它确保存储提供程序没有更改或删除任何内容。这是通过在用户数据上构建Merkle树来实现的。Merkle树在我们关于证书透明性的博客文章中得到了深入的描述。Merkle树的根哈希值与版本号相关联，该版本号随每次更改而递增，并且根哈希值和版本号均使用从用户密码派生的密钥进行身份验证。这些数据存储在两个位置：对象存储数据库中的一个特殊密钥下，以及用户设备上的一个文件中。
每当用户想从存储提供程序读取一块数据时，他们首先请求远程存储的根目录，并检查它是否与磁盘上的相同，或者版本号是否大于磁盘上的版本号。检查版本号可防止存储提供程序将存档还原为未检测到的以前（有效）状态。然后，可以根据最新的根散列验证读取的任何数据，该散列可防止任何其他类型的修改或删除。
在此处使用Merkle树的好处与“证书透明性”的好处相同：它使我们能够验证单个数据，而无需立即下载并验证所有内容。 另一个用于数据完整性的常用工具称为消息身份验证码（Message Authentication Code，简称MAC），虽然它既简单又有效，但它无法只进行部分验证。
我们使用Merkle树不能防止的一件事是分叉，在分叉中，存储提供商向不同的用户显示不同版本的存档。然而，检测fork需要用户之间的某种流言蜚语，这已经超出了最初实现的范围。
Hiding Access Patterns(隐藏访问模式)Oblivious RAM, or ORAM,是一种用于以随机方式对随机存取存储器进行读写的加密技术，它可以从存储器本身中隐藏执行了哪个操作（读或写）以及对该操作执行到了存储器的哪一部分！在我们的例子中，“内存”是我们的对象存储提供程序，这意味着我们要向他们隐藏我们正在访问的数据片段以及访问的原因。这对于防御流量分析攻击很有价值，在这种攻击中，对UtahFS这样的系统有详细了解的对手可以查看其发出的请求，并推断加密数据的内容。例如，他们可能会看到您定期上传数据，几乎从不下载，并推断您正在存储自动备份。
ORAM最简单的实现是始终读取整个内存空间，然后使用所有新值重写整个内存空间，只要您想读取或写入单个值。一个观察内存访问模式的对手将无法判断你真正想要的值，因为你总是触摸所有东西。然而，这将是极其低效的。
我们实际使用的结构称为Path ORAM，它稍微抽象了一点这个简单的方案，使其更有效。首先，它将内存块组织成二叉树，其次，它保留一个客户端表，该表将应用程序级指针映射到二叉树中的随机叶。诀窍是允许一个值存在于任何内存块中，该内存块位于指定叶和二叉树根之间的路径上。
现在，当我们要查找指针指向的值时，我们在表中查找它的指定叶，并读取根和该叶之间路径上的所有节点。我们正在寻找的价值应该在这条路上，所以我们已经拥有了我们需要的！在没有任何其他信息的情况下，对手看到的只是我们从树上读到一条随机路径。 从树中读取的内容看起来像是一条随机路径，最终包含了我们正在寻找的数据。
但是，我们仍然需要隐藏我们是在读还是在写，并重新随机分配一些内存，以确保此查询不会与将来的其他查询相关联。 所以为了重新随机化，我们将刚读取的指针分配给新叶子，然后将值从存储在其之前的块中移到新叶子和旧叶子的父块中。（在最坏的情况下，我们可以使用根块，因为根是所有内容的父对象。）一旦将值移动到适当的块中，并完成应用程序的使用/修改，我们将对提取的所有块重新加密并将其写回内存。这将把值放在根和它的新叶之间的路径中，同时只改变我们已经获取的内存块。 这个结构很好，因为我们只需要触摸分配给二叉树中单个随机路径的内存，这是相对于内存总大小的对数工作量。但即使我们一次又一次地读同一个值，我们每次都会从树上碰到完全随机的路径！但是，额外的内存查找仍然会导致性能损失，这就是为什么ORAM支持是可选的。
Wrapping Up(结束语)在这个项目上的工作对我来说是非常有益的，因为虽然系统的许多单独的层看起来很简单，但它们是许多改进的结果，并很快形成了一些复杂的东西。在这个项目上的工作对我来说是非常有益的，因为虽然系统的许多单独的层看起来很简单，但它们是许多改进的结果，并很快形成了一些复杂的东西。
原文链接：https://blog.cloudflare.com/utahfs/ 开源地址：https://github.com/cloudflare/utahfs</description></item></channel></rss>