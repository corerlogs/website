<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>笔迹-工匠之芯</title><link>https://icorer.com/icorer_blog/</link><description>Recent content on 笔迹-工匠之芯</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 14 Jun 2022 12:55:18 +0800</lastBuildDate><atom:link href="https://icorer.com/icorer_blog/index.xml" rel="self" type="application/rss+xml"/><item><title>2022 Wanxiang Blockchain Spring Hackathon - IceFireDB Won The First Prize</title><link>https://icorer.com/icorer_blog/posts/icefiredb2022wanxiang/</link><pubDate>Tue, 14 Jun 2022 12:55:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/icefiredb2022wanxiang/</guid><description>2022 Wanxiang Blockchain Spring Hackathon Wrapped up on June 12th,We are very lucky and grateful to Protocol Labs for giving us this honor. In addition to joy, we are more moved. Our efforts and innovation are recognized. Next, we will work harder to incubate our projects.
2022 Wanxiang Blockchain Spring Hackathon, an immensely successful hackathon packed with over 20 teams of passionate builders and developers wrapped up yesterday, under the theme of “Metaverse: A Shared Future on Blockchain”.</description></item><item><title>Gorilla：一个快速、可伸缩的内存时间序列数据库</title><link>https://icorer.com/icorer_blog/posts/gorilladb/</link><pubDate>Fri, 01 Apr 2022 17:26:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/gorilladb/</guid><description>摘要大规模互联网服务旨在出现意外故障时保持高可用性和高响应性。提供这种服务通常需要在大量系统上每秒钟监测和分析数千万次测量，一个特别有效的解决方案是在时间序列数据库(TSDB)中存储和查询这种测量。TSDB设计中的一个关键挑战是如何在效率、可伸缩性和可靠性之间取得平衡。在本文中，我们介绍Gorilla系统，脸书的内存TSDB。我们的见解是，监控系统的用户不太重视单个数据点，而是更重视综合分析，对于快速检测和诊断持续问题的根本原因而言，最新数据点比旧数据点更有价值。Gorilla优化了写入和读取的高可用性，即使在出现故障时也是如此，代价是可能会在写入路径上丢弃少量数据。为了提高查询效率，我们积极利用压缩技术，如增量时间戳和异或浮点值，将Gorilla的存储空间减少了10倍。这使我们能够将Gorilla的数据存储在内存中，与传统数据库(HBase)支持的时间序列数据相比，查询延迟减少了73倍，查询吞吐量提高了14倍。这种性能改进带来了新的监控和调试工具，比如时序关联搜索和更密集的可视化工具。Gorilla还可以优雅地处理从单个节点到整个区域的故障，几乎没有运营开销。
一、介绍大规模互联网服务即使在出现意外故障的情况下也能保持高可用性和对用户的响应。随着这些服务发展到支持全球客户，它们已经从运行在数百台机器上的几个系统扩展到服务数以千计的个人用户系统运行在数千台机器上，通常跨越多个地理复制的数据中心。
运行这些大规模服务的一个重要要求是准确监控底层系统的健康和性能，并在出现问题时快速识别和诊断问题。脸书使用时间序列数据库(TSDB)存储系统测量数据点，并在顶部提供快速查询功能。接下来，我们将指定监控和操作脸书需要满足的一些约束，然后描述Gorilla，这是我们新的内存TSDB，可以存储数千万个数据点(例如，CPU 负载、错误率、延迟等)。)并在几毫秒内响应对此数据的查询。
写占主导地位。我们对 TSDB 的主要要求是它应该始终可用于写入。由于我们有数百个公开数据项的系统，写入速率可能很容易超过每秒数千万个数据点。相比之下，读取速率通常要低几个数量级，因为它主要来自于观察“重要”时间序列数据的自动化系统、可视化系统或为希望诊断观察到问题的人类操作员提供仪表板。
状态转换。我们希望识别新软件发布中出现的问题、配置更改的意外副作用、网络中断以及导致重大状态转换的其他问题。因此，我们希望我们的TSDB支持短时间窗口内的细粒度聚合。在几十秒钟内显示状态转换的能力特别有价值，因为它允许自动化在问题变得广泛传播之前快速修复问题。
高可用性。即使网络分区或其他故障导致不同数据中心之间的连接断开，在任何给定数据中心内运行的系统都应该能够将数据写入本地TSDB机器，并且能够按需检索这些数据。
容错。我们希望将所有写入复制到多个区域，这样我们就可以在任何给定的数据中心或地理区域因灾难而丢失时幸存下来。
Gorilla是脸书的新TSDB，满足了这些限制。Gorilla用作进入监控系统的最新数据的直写缓存。我们的目标是确保大多数查询在几十毫秒内运行。Gorilla 的设计理念是，监控系统的用户不太重视单个数据点，而是更重视综合分析。此外，这些系统不存储任何用户数据，因此传统的 ACID保证不是TSDB的核心要求。 但是，高比例的写入必须始终成功，即使面临可能导致整个数据中心无法访问的灾难。此外，最近的数据点比旧的数据点具有更高的价值，因为直觉上，对于运营工程师来说，知道特定系统或服务现在是否被破坏比知道它是否在一个小时前被破坏更有价值，Gorilla 进行了优化，即使在出现故障的情况下也能保持高度的读写可用性，代价是可能会丢失少量数据写入路径。
高数据插入率、总数据量、实时聚合和可靠性要求带来了挑战。我们依次解决了这些问题。为了解决第一个要求，我们分析了 TSDB 操作数据存储(ODS),这是一个在脸书广泛使用的老的监控系统。我们注意到，对ODS的所有查询中，至少有85%是针对过去26小时内收集的数据。进一步的分析使我们能够确定，如果我们能够用内存中的数据库替换基于磁盘的数据库，我们可能能够为我们的用户提供最好的服务。此外，通过将这个内存中的数据库视为持久的基于磁盘的存储的缓存，我们可以实现具有基于磁盘的数据库的持久性的内存中系统的插入速度。
截至2015年春天，脸书的监控系统生成了超过20亿个独特的时间序列计数器，每秒钟增加约1200万个数据点。这代表每天超过1万亿个点。在每点16字节的情况下，产生的16TBRAM对于实际部署来说太耗费资源了。我们通过重新利用现有的基于XOR的浮点压缩方案来解决这一问题，使其以流的方式工作，从而允许我们将时间序列压缩到平均每点1.37字节，大小减少了12倍。
我们通过在不同的数据中心区域运行多个Gorilla实例并向每个实例传输数据流来满足可靠性要求，而不试图保证一致性。读取查询指向最近的可用Gorilla实例。请注意，这种设计利用了我们的观察，即在不影响数据聚合的情况下，单个数据点可能会丢失，除非Gorilla实例之间存在显著差异。Gorilla目前正在脸书的生产中运行，工程师们每天将其用于实时灭火和调试，并与Hive[27]和Scuba[3]等其他监控和分析系统结合使用，以检测和诊断问题。
二、背景和要求2.1 操作数据存储脸书的大型基础设施由分布在多个数据中心的数百个系统组成，如果没有能够跟踪其运行状况和性能的监控系统，运营和管理这些基础设施将会非常困难。业务数据储存库是脸书监测系统的一个重要部分。ODS由一个时间序列数据库(TSDB)、一个查询服务以及一个探测和警报系统组成。ODS的TSDB 构建在 HBase存储系统之上，如[26]中所述。图1显示了ODS组织方式的高级视图。来自运行在脸书主机上的服务的时间序列数据由ODS写入服务收集并写入 HBase。
ODS时间序列数据有两个消费者。第一个消费者是依赖制图系统的工程师，该系统从ODS生成图形和其他时间序列数据的直观表示，用于交互式分析。第二个消费者是我们的自动警报系统，该系统从 ODS读取计数器，将它们与健康、性能和诊断指标的预设阈值进行比较，并向oncall工程师和自动补救系统发出警报。
2.1.1 监控系统读取性能问题2013 年初，脸书的监控团队意识到其HBase时序存储系统无法扩展处理未来的读取负载。虽然交互式图表的平均读取延迟是可以接受的，但是P90的查询时间增加到了几秒钟，阻碍了我们的自动化。此外，用户正在自我审查他们的用户年龄，因为即使是几千个时间序列的中等规模查询的交互式分析也需要几十秒钟才能执行。在稀疏数据集上执行的较大查询会超时，因为HBase数据存储被调整为优先写入。虽然我们基于HBase的TSDB效率低下，但我们很快就对存储系统进行了大规模更换，因为 ODS的HBase存储拥有大约2PB 的数据[5]。脸书的数据仓库解决方案Hive也不合适，因为它的查询延迟比ODS 高几个数量级，而查询延迟和效率是我们主要关心的问题[27]。
接下来，我们将注意力转向内存缓存。ODS已经使用了一个简单的通读缓存，但它主要是针对多个仪表板共享相同时间序列的图表系统。一个特别困难的场景是当仪表板查询最近的数据点，在缓存中错过，然后发出请求直接发送到 HBase 数据存储。我们还考虑了基于独立Memcache[20]的直写缓存，但拒绝了它，因为向现有时间序列添加新数据需要一个读/写周期，从而导致Memcache服务器的流量非常高。我们需要更有效的解决方案。
2.2 Gorilla要求考虑到这些因素，我们确定了新服务的以下要求:
由一个字符串键标识的20亿个唯一的时间序列。 每分钟增加7亿个数据点(时间戳和值)。 存储数据26小时。 峰值时每秒超过40,000次查询。 读取在不到一毫秒的时间内成功。 支持15秒粒度的时间序列(每个时间序列每分钟 4 个点)。 两个内存中、不在同一位置的副本(用于灾难恢复容量)。 即使单个服务器崩溃，也始终提供读取服务。 能够快速扫描所有内存中的数据。 支持每年至少2倍的增长。 在第3节与其他 TSDB 系统进行简单比较后，我们在第4节详细介绍Gorilla的实现，首先在第4.1 节讨论其新的时间戳和数据值压缩方案。然后，我们将在第 4.4 节中描述Gorilla如何在单节点故障和区域性灾难的情况下保持高可用性。我们将在第5节描述Gorilla如何启用新工具。最后，我们在第6节描述了我们开发和部署Gorilla的经验。
三、与 TSDB 系统的比较有许多出版物详细介绍了数据挖掘技术，以有效地搜索、分类和聚类大量的时间序列数据[8,23,24]。这些系统展示了检查时间序列数据的许多用途，从聚类和分类[8,23]到异常检测[10,16] 到索引时间序列[9,12,24]。然而，很少有例子详细说明能够实时收集和存储大量时间序列数据的系统。Gorilla的设计侧重于对生产系统进行可靠的实时监控，与其他TSDB相比非常突出。Gorilla占据了一个有趣的设计空间，在面对优先于任何旧数据可用性的故障时，可用于读取和写入。
由于 Gorilla 从一开始就被设计为将所有数据存储在内存中，因此它的内存结构也不同于现有TSDB。但是，如果将Gorilla视为另一个磁盘上TSDB之前的时间序列数据内存存储的中间存储，那么Gorilla 可以用作任何 TSDB 的直写缓存(相对简单的修改)。Gorilla对摄取速度和水平扩展的关注与现有解决方案相似。
3.1 OpenTSDBOpenTSDB基于HBase[28]，非常接近我们用于长期数据的ODS HBase存储层。这两个系统依赖于相似的表结构，并且在优化和水平可伸缩性方面得出了相似的结论[26,28]。然而，我们发现支持构建高级监控工具所需的查询量需要比基于磁盘的存储所能支持的更快的查询。
与OpenTSDB不同，ODS HBase层确实为较旧的数据进行时间累积聚合以节省空间。这导致较旧的存档数据与ODS中较新的数据相比具有较低的时间粒度，而OpenTSDB将永远保留全分辨率数据。我们发现，更便宜的长时间查询和空间节省是值得的精度损失。</description></item><item><title>保护您的服务网格：十三项清单</title><link>https://icorer.com/icorer_blog/posts/13-item_checklist_for_securing_your_service_mesh/</link><pubDate>Thu, 17 Feb 2022 14:57:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/13-item_checklist_for_securing_your_service_mesh/</guid><description>世界各地的组织都在急于对其应用程序进行现代化改造，以便在云中 Kubernetes 编排的容器中运行。在此现代化过程中，这些公司必须计划保护其应用程序连接。当应用程序被重新平台化并重新构建为分布式微服务时，会出现新的连接模式，这些模式通常会促使构建一个或多个服务网格。
强大的服务网格可以处理南北连接（从边缘进入基于 Kubernetes 的应用程序）和东西连接（同一集群上的服务之间或不同集群之间）。但是，这些流量模式中的每一个都需要全面的安全性.
实现服务网格安全性的最佳方法是采用零信任模型，这意味着每个连接，无论其来源如何，都必须经过验证和保护。为了帮助您评估服务网格技术并实施零信任安全，我们提供了 13 个必备功能，以确保您的应用程序连接安全。这是清单：
传输层安全性（TLS 和 mTLS）提供端到端加密，以保护任何端点对之间的动态数据。它可能是最基本的组件，但令人惊讶的是，并非所有服务网格都完全支持双向 TLS。 内置 Web 应用程序防火墙 (WAF)可屏蔽入站流量以发现威胁并阻止攻击侵入您的周边。对于任何向 Internet 公开以接收传入用户和应用程序连接请求的边缘网关来说，这都是必不可少的。 数据丢失防护 (DLP)监控数据泄露或泄露，以防止数据丢失和数据泄露。如果您的应用程序以某种方式受到损害，您不希望数据泄露您的边界。 Kubernetes的机密管理集成，后者管理密码、安全令牌和加密密钥等敏感凭证。您会担心这些信息仍然被硬编码到应用程序中或以纯文本形式存储的频率。 证书管理从集中式平台控制和执行 SSL 证书以验证连接。证书轮换可能是一个痛苦的管理步骤，应该优雅地加以考虑。这应该可以扩展以支持外部权限，这意味着它将与您已经使用的企业身份和访问管理解决方案一起使用。 授权，例如使用开放策略代理 (OPA)，它将服务 API 策略定义为代码。授权是身份验证的另一面，一旦您验证了他们的身份，谁就可以访问哪些资源。 联合信任域可以安全地跨环境对用户和应用程序进行身份验证，从而在任何地方始终如一地扩展身份验证策略。如果没有这个，您将花费大量精力来尝试保持各种角色的更新和同步——并且可能会犯一些错误。 联合的基于角色的访问控制 (RBAC) 和委派向用户授予与其职责相符的权限，并且再次在任何地方始终如一地应用此权限。这些控制可以应用于管理服务网格的运营商的不同级别，也可以应用于构建在网格中运行的应用程序的开发人员。 多租户和隔离使服务网格中的用户和应用程序可以安全地共享资源。拥有 RBAC 后，您可以安全地定义谁可以接触什么，并为不同的角色有效地创建隔离的工作空间。Istio 的授权策略也可用于防止不需要的流量到达您的应用程序。 漏洞扫描和出版物发现、解决和警告系统中的任何弱点。安全性与其最薄弱的环节一样好，因此检查防御中的任何漏洞很重要。 多集群访问可观察性为整个系统的所有活动提供完整的日志聚合和可审计性。这对于事件后的实时监控和取证都很有用。对于分布式应用程序，有必要获得全局视图。许多人使用 Prometheus 和 Grafana 等开源工具来实现可观察性。 联邦信息处理标准 (FIP) 140-2意味着您的服务网格技术已经过验证，符合美国政府规定的特定严格安全标准。有许多政府法规和行业最佳实践，但 FIPS 是确定安全基准的一种常用方法。 集群中继的安全拉取模型在整个系统中安全地共享配置。这是非常微妙的，但是您要确保任何配置更改都在请求时分发到边缘，并且仅在请求时分发。 虽然严格来说不是服务网格的安全特性，但一个额外的考虑因素是企业支持的可用性和用于响应的定义服务级别协议 (SLA)。</description></item><item><title>使用区块链将零信任架构扩展到端点：最先进的审查</title><link>https://icorer.com/icorer_blog/posts/blockchain_with_zerotrust/</link><pubDate>Tue, 15 Feb 2022 23:50:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/blockchain_with_zerotrust/</guid><description>摘要为了防御当今无边界网络中的横向移动，零信任架构(ZTA)的采用势头越来越猛。有了全面的 ZTA 实施，对手不太可能从受损的端点开始通过网络传播。然而，已经经过身份验证和授权的受损端点会话可以被用来执行有限的(尽管是恶意的)活动，最终使端点成为 ZTA 的致命弱点。为了有效地检测此类攻击，已经开发了具有基于攻击场景的方法的分布式协同入侵检测系统。尽管如此，高级持续威胁(APTs)已经证明了它们绕过这种方法的能力，并且成功率很高。因此，对手可以不被发现地通过或潜在地改变检测记录机制，以实现隐蔽的存在。最近，区块链技术在网络安全领域展示了可靠的使用案例。在本文中，受基于 ZTA 和区块链的入侵检测和防御的融合的推动，我们研究了如何将 ZTA 扩展到端点。也就是说，我们对 ZTA 模型、以端点为重点的真实体系结构以及基于区块链的入侵检测系统进行了最先进的审查。我们讨论了区块链的不变性增强检测过程的潜力，并确定了开放的挑战以及潜在的解决方案和未来的方向。
一、介绍随着云计算的革命，大多数企业的资源和数据不再存储在内部。此外，最近的新冠肺炎疫情事件极大地改变了工作模式，因为大多数员工和企业不得不转向在家工作。在家工作(和远程工作)会给组织带来新的严重安全风险，因为许多“未经培训”的员工使用自己的设备连接到工作信息技术(IT)系统。云计算和远程工作是企业必须扩大其数字安全范围并适应当代趋势的例子。
在传统的基于外围的安全模型中，组织在外围的资源和资产被认为是良性的和可信的。边界通常由安全措施保护，如防火墙或入侵检测系统。这种模式在云计算和远程工作领域似乎不太有效，针对远程工作员工的几次网络攻击(例如[1-5])就表明了这一点。
信任是传统的基于边界的安全模型所依赖的基本原则。员工或合作者的设备和组织资产(即端点)通常在默认情况下是可信的，而不管其状况如何。如果攻击者能够控制这些端点中的任何一个，那么边界就会受到威胁，并且通过横向移动有可能实现对信息和数据的进一步访问。
防火墙、防病毒技术、入侵检测和防御系统(IDS/IPS)和网络应用防火墙(WAFs)，换句话说，大石墙和装甲前门，已经不足以保证现代 IT 和运营技术(OT)环境的安全[6]。基于外围的安全是多家公司采用的主要概念，尤其是当他们的数据驻留在内部数据中心时。建立在内部和外部差异基础上的传统防御模式正在过时[7]，而与此同时，威胁格局也在急剧演变[8]，最终导致基于外围的安全架构的衰落。
为了应对当今复杂的网络基础设施和当前不断发展的威胁形势，需要一种新的安全架构。ZTA 通过建立无边界的基于数字身份的边界脱颖而出，在这个边界中，数据处于安全架构的中心，破坏思维主导着威胁模型， 引领着访问控制环境、运营、托管环境、端点和互连基础架构。ZTA 提倡一种新的安全架构，默认情况下，任何设备、系统、用户或应用程序都不应该基于其在网络中的位置而受到固有的信任。相反，不管在什么地方，信任总是要赢得和验证的。然而，这并不一定意味着在ZTA 的情况下信任被消除，而是应该最小化，直到通过ZTA 信条和核心组成部分证明不是这样。
使用传统的基于边界的防御，如果坚定的攻击者能够在端点上建立经过身份验证和授权的立足点，他们仍然可以绕过 ZTA 安全健康检查。例如，操作系统内核中的潜在恶意软件可以篡改在 ZTA 环境中进行的安全检查。这最终导致绕过在 ZTA 实施的基本控制，这将允许攻击者除了横向移动之外，还执行一些以用户和设备为中心的恶意活动。因此，需要一种有效的入侵检测方法来解决端点的漏洞，这可以被视为 ZTAs 的致命弱点。
在本文中，我们旨在研究如何利用区块链的不变性增强入侵检测过程的潜力，将 ZTA 扩展到端点，以消除上述问题。为此，我们首先回顾零信任的核心原则、能力和要求。其次，我们对现有的现实世界零信任实现进行分类，并讨论它们的优缺点。第三，我们探索了区块链在开发和改进分布式协作入侵检测系统(DCIDSs)方面的潜力，该系统可以缓解ZTA 的致命弱点(即端点的脆弱性)。最后，我们讨论了开放的问题和挑战，并强调了ZTA和基于区块链的分布式入侵检测系统的潜在解决方案和研究方向。
据我们所知，这是第一个利用区块链技术成功将ZTA扩展到端点的工作。表1给出了本文中使用的主要缩写及其定义。
二、零信任（ZT）在本节中，我们简要介绍了“零信任”和 ZTA 的历史，并讨论了零信任的核心原则、核心能力、模型和现有方法，包括现实世界的实现。
2.1 零信任架构的历史2004 年的杰里科论坛提出了去周边化的想法(当时是激进的)[3]，随后发展成为更广泛的零信任概念。早在2010 年，J. Kindervag [15]就创造了“零信任”一词；然而，在此之前，网络安全领域就存在零信任概念。美国国防部和国防信息系统局(DISA)提出了一项名为“黑核”的安全战略，并于 2007 年发表[16]。Black core讨论了从基于外围的安全架构向强调保护单个交易的安全架构的过渡。
云和移动计算的广泛采用极大地促进了 ZTAs 的发展，例如，作为其中的一部分，基于身份的架构等方法慢慢获得了关注和更广泛的接受。谷歌以“BeyondCorp”的名义发布了一系列关于如何实现零信任架构的六个文档[17-22]。BeyondCorp项目倡导去边界化的概念，认为基于边界的安全控制已经不够，安全应该扩展到用户和设备。由于这个项目，谷歌放弃了传统的基于虚拟专用网络(VPNs)的远程工作方式，并设法提供了一个合理的保证，即所有公司用户都可以通过不安全和不受管理的网络访问谷歌的网络。
2.2 从传统的周边架构到ZTA作为一种理念，“零信任”假设对用户、设备、工作负载和网络流量的信任不应被隐含地授予[15]，其结果是所有实体都必须被明确地验证、认证、授权和持续监控。零信任的核心目标之一是，一旦对手成功危及用户的设备，甚至简单地窃取他们的凭据，就会严重抑制对手横向移动的能力。因此，需要相应地塑造和准备信息技术基础设施。
传统的基于外围的安全架构会创建多个信任区域[2]。并非所有区域都遵守相同的规则或相同的信任级别。事实上，如果相关组件没有明确允许，用户甚至可能无法进入下一个区域。这被称为纵深防御，正如史密斯[23]所讨论的，并在图1中描述，或者称为城堡和护城河方法[24]。请注意，在到达大型机之前，不同的区域(互联网、非军事区、可信和特权)受到各种基于外围的控制的保护，例如本地代理、虚拟专用网网关、多个防火墙和应用程序服务。在这个例子(即图1)中，大型机是一个核心银行系统，负责所有交易，因此它被完全隔离在一个特权区域中。
与传统的安全架构不同，零信任要求从内到外进行思考、构建和保护。基于谷歌[19] [20]、Jericho[3]和Kindervag[15] [25]的上述工作，有一个直接和重要的观察， 即在ZTA，一旦网络位置依赖性变得无关紧要，虚拟专用网技术就可以被消除。简而言之，虚拟专用网允许远程工作的用户(在图1中用“远程员工”表示)通过安全的加密通道连接到办公室(在图1中用“可信”表示)。但是，应该通过其他方式保护端点，因为虚拟专用网加密只处理“远程员工”和“可信”区域之间的隧道。当“远程员工”通过身份验证并成功建立隧道时，他/她会收到“可信”区域的远程网络中的一个 IP 地址。在该隧道上， 从“远程员工”到“可信”区域的流量被解封并路由，因此导致“官方”后门。此外，被称为“虚拟专用网网关”的单一入口点充当体系结构和网络的单一故障点或扼杀点。因此，如果我们开始认为网络位置无关紧要，同时应用一组适当的控制，那么如果没有进一步的依赖关系 (例如，具有传统协议的应用程序)，就可以消除虚拟专用网络。也就是说，身份验证和授权以及策略实施应该立即向网络边缘和端点靠拢。
为了反映上面的论点，我们绘制了图 2，显示了对 ZTA的引用。为了简化起见，在图中，我们只包括核心组件，例如，本地代理(LB)、远程员工、移动设备、不可信客户端和许多需要保护的服务。与基于边界的架构相比，如图 1 所示，没有区域，安全性是由内而外构建的。此外，既没有 VPN 网关，也没有防火墙来过滤网络流量， 最重要的是没有单一的入口网关。我们注意到；但是，控制平面上的策略执行点。这种 ZTA 参考不会像基于外围的架构那样产生任何瓶颈。</description></item><item><title>自主网络安全-保障未来颠覆性技术</title><link>https://icorer.com/icorer_blog/posts/autonomous_network_security_securing_future_disruptive_technologies/</link><pubDate>Fri, 28 Jan 2022 21:07:07 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/autonomous_network_security_securing_future_disruptive_technologies/</guid><description>From：2021 年 IEEE ⽹络安全与弹性国际会议 (CSR)
概述这篇论文讲述自主网络安全知识的系统化。诸如物联网、人工智能和自治系统等颠覆性技术正变得越来越普通，而且通常很少或者根本没有网络安全保护，这种缺乏安全性导致的网络攻击面不断扩大。自主计算计划旨在通过使复杂计算系统自我管理来解决管理复杂计算系统的复杂性。自主系统包含应对网络攻击的属性，例如新技术的自我保护和自我修复。有许多关于自主网络安全的研究项目，采用不同的方法和目标技术，其中许多具有破坏性。本文回顾了自主计算，分析了自主网络安全的研究，并提供研究知识的系统化，本文最后确定自主网络安全方面的差距，以供未来研究。
一、介绍随着恶意⾏为者变得善于发现和利⽤⽹络和计算机系统中的缺陷[1]，对计算机系统的攻击数量随着复杂程度和严重程度的增加⽽增加。新的颠覆性技术不断被引⼊并连接到企业⽹络和互联⽹，这些技术限制了有些甚⾄没有内置的⽹络安全。其中包括⽹络物理系统/物联⽹ (CPS/IoT)、⼈⼯智能(AI) 和⾃治系统等技术。计算系统通常会在事后添加⽹络安全，⽽不是从⼀开始就设计，尤其是新的颠覆性技术，尤其是匆忙推向市场的技术。
系统需要能够以⾃我管理的⽅式对攻击和固有的弱点做出反应，⽆论是单独还是与其他系统⼀起。将安全性本地嵌⼊到软件和硬件系统中将使它们能够对攻击做出反应，并单独保护和治愈⾃⼰，并且作为⼀个群体。从本质上讲，这是⽹络安全的⾃主⽅法。
伯纳尔等⼈[2]在2019 年确定了⽹络安全⽅⾯的⼀些研究挑战，并将“软件化和虚拟化CPS/IoT系统和移动⽹络中的⾃主安全编排和执⾏”列为第⼆名，仅次于“可互操作和可扩展的安全管理异构⽣态系统”。 他们指出，需要新的⾃主和上下⽂感知安全协调器，它们可以快速、动态地编排和实施适当的防御机制。
本⽂的⽬的是调查⾃主⽹络安全研究中的空⽩，并将这些知识系统化以探索未来的研究。该论⽂基于对⾃主⽹络安全⽂献的调查和⽅法分类。这提供了对未来研究可以帮助推动⾃主⽹络安全向前发展以保护未来颠覆性技术的洞察⼒。本⽂的其余部分结构如下：第⼆部分提供了⾃主计算(AC) 的背景以及如何将⽹络安全设想为⾃主系统的⼀部分。第三部分描述了⾃主⽹络安全的过去和当前研究。⼀些研究侧重于特定应⽤领域的⾃主⽹络安全，⽽其他研究则提供可⽤于⼀系列应⽤的⾃主⽹络安全框架。第四节提供了研究分类和分类知识库的系统化，第五节讨论了基于知识系统化的⽹络安全研究的差距，第六节提供了结论性意⻅。
二、背景Kephart 和 Chess 在 2003 年的⾃主愿景论⽂[3]中将⾃主计算定义为“根据管理员的⾼级⽬标，可以⾃我管理的计算系统”。使⽤“⾃主计算”⼀词是因为⾃主神经系统管理⾝体功能，调整这些功能以满⾜⾝体需求，并为更⾼层次的认知活动释放有意识的活动。⽹络安全也是如此，它应该在维护服务的同时，根据当前的⽹络情况⾃动 调整和调整系统资源。Kephart 和 Chess 概述了⾃主系统应 具备的四个要素：
⾃我配置 ⾃我修复 ⾃我优化 ⾃我保护，也称为Self CHOP 提出了⼀个参考架构来实现能够适应不断变化的环境的Self CHOP 属性（图 1）。
Monitor、Analyze、Plan、Execute 和 Knowledge 组件统称为 MAPEK 架构。⾃主计算系统并未被设想为单⼀的独⽴系统，⽽是交互的、⾃我管理的系统，它们共同⼯作以实现其⽬标（图2）。
Self CHOP属性在[3]中定义为：
⾃配置：系统通过动态调整其资源来⾃动配置和重新配置⾃⾝的能⼒ ⾃我修复：系统从常规和意外事件中⾃动检测、诊断和修复硬件和软件的能⼒ ⾃我优化：系统和⼦组件监控其部件以检测性能下降同时不断寻求⾃我改进的能⼒ ⾃我保护：系统针对故障和恶意对系统的攻击能力能够⾃动检测和防御 自主安全CHOP属性可以为系统提供⽹络安全⽀持，以保护⾃⾝并从⽹络攻击中恢复。Kephart 将⾃我保护属性描述为保护系统免受⾃我修复属性⽆法纠正的恶意攻击或级联故障。其他Self CHOP属性也⽀持⾃保护属性，⾃修复属性在系统受到攻击后通过更新和脱落受损组件来修复系统，⾃配置属性帮助系统在禁⽤受损⼦系统后重新配置⾃⾝，⾃优化属性可以帮助系统通过攻击运⾏，然后重新配置后重新优化。
⾃主⽹络安全建⽴在⾃主视觉和参考架构的基础上，以开发⾃我管理能⼒，利⽤Self CHOP特性检测⽹络攻击并从⽹络攻击中恢复 [4]。⾃主⽹络安全不仅使⽤⾃主计算的Self CHOP特性，⽽且还互连⾃主元素，以提供⾃主元素之间的信息通信和共享，以共同应对攻击并从攻击中恢复。
三、自主网络安全方法以下是对一系列领域实施自主网络安全的方法的调查，这些领域包括智能汽车、网络物理系统/物联网、工业控制系统、关键基础设施、自组织计算、高性能计算、云计算、企业计算和其他。一些方法集中在架构上，另一些集中在被保护的领域或底层技术上。以下描述了调查的不同方法。它们按照是否使用 MAPEK 参考体系结构、非MAPEK体系结构、实现特定的自主特性、是提供自主开发支持的框架还是自主安全开发工具来分组。
3.1 MAPEK方法3.1.1 认知和自主网络安全Maymir Ducharme等人[5]在2015年描述了使用上下文分析和自主元素的分层网络，该网络从分层结构中的不同抽象层次提供系统元素的不同视图。具有更高级视图的自治元素将具有企业任务级上下文。更高级别的自治元素将根据公司功能的关键程度对这些功能进行优先排序，以便在紧急情况或网络攻击时维持运营。与安全相关的元素，如防火墙，将在层次结构中较低的抽象级别表示。
3.1.2 零信任Eidle等在2017年提出利用autonomic网络安全实现网络零信任[6]。作者开发了一个网络安全测试平台，实现了基于观察、定向、决定、行动(OODA)模型的零信任网络的各个方面，该模型与 MAPEK 参考架构相似，平台观察网络用于监控和分析攻击特征的身份验证日志。他们集成了来自多个网络设备的威胁响应，以简化威胁的检测和缓解危害。
3.1.3 自主车辆网络Le Lann在 2018年描述了未来智能汽车网络如何具有自主组织和自愈特性[7]。车辆网络可以构成由一组车辆组成的系统系统。每辆车都将是一个自主元件，与附近的其他车辆相连。车辆将被动态地连接在一起，形成“队列”，它们像车队一样一起行进，并提供群体安全。车辆可以根据车辆需求动态添加和删除。出于安全目的，加入群组需要身份验证。
3.1.4 使用大数据的分布式计算中的自主入侵响应Vierira等人在2018年使用了自主计算和大数据分析的组合来检测和响应网络攻击[8]。作者为自主入侵响应系统提供了一个参考架构，并开发了一个概念验证实现。作者将其入侵响应参考体系结构建立在 MAPEK 参考体系结构的基础上，网络流量系统日志是通过监控 MAPEK子组件收集的。</description></item><item><title>系统调用级二进制兼容的Unikernel虚拟机</title><link>https://icorer.com/icorer_blog/posts/system_call_level_binary_compatible_unikernel_virtual_machine/</link><pubDate>Mon, 24 Jan 2022 19:00:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/system_call_level_binary_compatible_unikernel_virtual_machine/</guid><description>一、背景Unikernel 是最小的单一用途虚拟机，目前在研究领域非常受欢迎，但是目前想把已有的应用程序移植到当前的unikernel环境是很困难的。HermiTux是第一个提供与Linux应用程序的系统调用级二进制兼容的unikernel，它由一个管理程序和一个模拟负载及运行时Linux ABI的轻量级内核层组成。HermiTux将应用程序开发人员从移植软件的负担中解脱出来，同时通过硬件辅助虚拟化隔离、快速启动时间和低磁盘/内存占⽤、安全性等提供单核优势。通过二进制分析重写技术及共享库替换，可以快速实现系统调用和内核模块化。
论文中展示了HermiTux的独立架构特色，在x86-64和ARM aarch64 ISA架构上展示了一个原型，针对各种云以及边缘/嵌入式部署，也展示了HermiTux对⼀系列原⽣C/C++/Fortran/Python Linux 应⽤程序的兼容性。HermiTux与其他unikernel相比，也提供了相似程度的轻量级，并且在许多情况下与Linux性能相似，它在内存和计算密集型场景下性能开销平均损失3%，其I/O性能是可以接受的。
二、HermiTux对于Linux程序兼容性的思路Unikernels在学术领域变得很流行，它以虚拟化LibOS模型为基础，这种模型也带来了很多好处，主要包括：提高安全性、性能改进、隔离性提升、降低成本等。这样的优势也增加了很多应用场景：云和边缘部署的微服务/基于Saas和Faas的软件、服务器应用程序、NFV、IOT、HPC等。尽管unikernel被视为容器领域具有吸引力的替代品，但unikernels仍然在行业中很难获得显著的牵引力，并且它们的采用率相当缓慢，主要原因是将遗留/现有的应用程序移植到unikernel模型很困难，有时候甚至是不可能的。
在大型程序中，移植复杂的代码库很困难，这是由于诸如不兼容/缺少库/函数、复杂的构建过程、缺乏开发工具（调试器/分析器）、不受支持的语言等因素存在。移植到unikernel环境，也需要程序员具备这方面的专业知识，巨大的移植负担是阻碍广泛采用unikernels最大的障碍之一。
HermiTux提出了一个新型的unikernel模型，他为常规Linux应用程序提供二进制兼容性，同时保留了unikernel的优势，它允许开发工作集中在unikernel这层。HermiTux原型是HermitCore unikernel的扩展，它能够运行原生的Linux可执行文件作为unikernel。通过提供这种基础设施，HermiTux将应用程序员的移植工作转变为unikernel层开发人员的支持工作，在这个模型下不仅可以让原生Linux应用程序透明地获得unikernel的好处，而且还可以运行以前不可移植的应用程序。使用HermiTux将遗留应用程序作为unikernel移植和运行是不存在的，HermiTux支持静态和动态链接的可执行文件、兼容多种语言（C/C++/Fortran/Python等）、编译器（GCC和LLVM）、全面优化（-O3）和剥离/混淆的二进制文件。它支持多线程和对称多处理器（SMP）、检查点/重启和迁移。
大多数现有的unikernel不提供任何类型的二进制兼容性，一些系统通过在C库级别进行接口适配来提供二进制兼容性，其作用类似于动态链接库。这可以防止它们通过系统调用来支持何种需要操作系统服务的应用程序，而无需通过C库。为了保障最大化的兼容性，HermiTux没有采用通用做法，在系统调用级别实现了所有应用程序和编译库使用的标准化接口。
三、HermiTux的挑战HermiTux应对的第一个挑战是如何提供系统调用级别的二进制兼容性？，因此HermiTux根据Linux应用程序二进制接口ABI设置执行环境并在运行时模拟OS接口。基于自定义管理程序的ELF加载程序用于在单个空间虚拟机中与最小内核一起运行Linux二进制文件。程序运行的系统调用被重定向到unikernel提供的实现。
HermiTux应对的第二个挑战是如何在**提供二进制兼容性的同时保持unikernel的好处？**有些是自然具备的好处（小磁盘/内存占用、虚拟化强制隔离），而另一些（快速的系统调用、内核模块化）在假设无法访问源代码时会带来技术挑战。为了实现这些好处，HermiTux对于静态可执行文件使用了二进制重写与分析技术，并在运行时用一个可识别的unikernel的C库替代动态链接库的可执行文件。最后HermiTux针对低磁盘/内存占用和攻击面进行了优化，与现有的unikernel一样低或更低。
由于unikernel应用案例广泛，HermiTux当前目的是兼容服务器和嵌入式虚拟化场景，因此主要支持Intel x86-64和 ARM aarch64 (ARM64) 指令集架构 (ISA) 开发。HermiTux的设计基本原则是独立于架构，但是它的实现以及我们用来恢复unikernel的二进制重写/分析技术是ISA特定的。
总体来说，HermiTux做了以下出色工作：
一种新的unikernel模型，旨在执行本机Linux可执行程序，同时保持经典的unikernel优势。 提供可以在x86-64 和 aarch64 架构上的两个原型实现。 四、关键概念阐述4.1 Unikernelsunikernel是一个应用程序，它使用必要的库和一个精简OS层静态编译成一个二进制文件，能够作为虚拟机在管理程序上执行。Unikernel符合以下条件：
单一目的：一个unikernel只包含一个应用程序 单一地址空间：由于单一目的原则，所以unikernel不需要内存保护，因此应用程序和内核共享一个地址空间，所有代码都以最高权限级别执行。 这样的模型提供了显著的好处，在安全性方面提供了unikernel之间的强隔离，虚拟机管理程序让它成为云部署的良好候选者。此外unikernel仅包含运行给定应用程序所需的必要软件。结合非常小的内核尺寸，和常规VM相比，这导致应用程序攻击面显著减少。一些unikernel也是用提供内存安全保障的语言编写的。关于性能方面，unikernel系统调用很快，因为它们是常见的函数调用，特权级别之间没有代价昂贵的用户态与内核态切换。上下文切换也很快，因为没有页表切换或TLB刷新。除了由于小内核导致代码库减少之外，unikernel OS层通常是模块化的，可以将它们配置为仅包含给定应用程序的必要功能。
所有这些好处让unikernels的应用程序域非常丰富。他们非常适合运行大多数需要高度隔离的云应用程序和需要高性能、低操作系统开销的计算密集型作业的数据中心。unikernel减少的资源使用使它们特别适合嵌入式虚拟化，随着边缘计算和物联网等范式的出现，这个领域的重要性日益增加。由于unikernels的应用领域包括服务器和嵌入式机器，因此论文主要针对intel x86-64和 aarch64架构进行模型构建。
4.2将现有应用程序移植到Unikernel移植现有软件作为unikernel运行是很困难的，特别在某些情况下无法移植程序到unikernel环境，因为所有的程序都需要重新编译与链接。一个给定的unikernel支持一组有限的内核特性和软件库，如果不支持应用程序所需的函数、库或者特定版本的库，则该程序需要进行调整。在许多情况下，缺少函数/库意味着应用程序根本无法移植，此外unikernel使用复杂的构建基础架构，将一些遗留的应用程序（大型的Makefile、autotools、cmake）移植到unikernel工具链会很麻烦，更改编译器或者构建选项也是如此。
在如此大的移植成本上，所以unikernel发展缓慢是有原因的，一种解决方案是让unikernel为常规可执行文件提供二进制兼容性，同时仍保持经典 unikernel 的优势，例如⼩代码库/占⽤空间、快速启动时间、模块化等。这种新模型允许 unikernel 开发⼈员致⼒于推⼴unikernel 层以⽀持最⼤应⽤程序的数量，并减轻应⽤程序开 发⼈员的任何移植⼯作。这种⽅法还应该⽀持调试器等开发⼯具。在这种情况下，HermiTux 允许将 Linux ⼆进制⽂件作为unikernel 运⾏，同时保持上述优势。
4.3 轻量级虚拟化设计空间轻量级虚拟化设计空间包含unikernel、面向安全的LibOS、例如Graphene，以及带有软件和硬件强化技术的容器。HermiTux不需要应用程序移植工作，并且和其他二进制兼容的系统方案不同，不同点主要包括：
作为unikernel HermiTux运行硬件强制（扩展页表）虚拟机，这是一种从根本上比软件强制隔离（容器/软件LibOS）更强的隔离机制。当前在VM中运行容器以确保安全的趋势（clear containers）加强容器隔离的努力（gVisor）都表明了这一点，这通常用作支持unikernel与容器的安全依据。 HermiTux使更广泛的应用程序能够透明地无需任何移植工作即可获得unikernel的好处，无需修改代码以及维护单独分支的潜在复杂性。鉴于unikernel提供的安全性和减少占用空间的特性，这在当今的计算机系统环境中非常有价值，软件和硬件漏洞经常成为新闻，并且数据中心架构师正在寻求增加整合和减少资源/能源消耗的方法。二进制兼容允许HermiTux成为专有软件（其源代码不可用）作为unikernel运行的唯一方法。最后，HermiTux允许软件获得VM的传统优势，例如检查点/重启/迁移，而无需大量磁盘/内存占用的相关开销。 4.4 系统调用级二进制兼容两个现有的unikernel已经生成和应用程序的二进制兼容，OSv和Lupin Linux。需要注意的是，两者都提供二进制兼容性标准C库（libc）级别，unikernel包含一个动态加载程序，它在运行时捕获对libc函数的调用，例如printf、fopen并将它重定向到内核。
这种接口方法意味着假设所有系统调用都是通过libc进行的，当考虑到各种各样的现代应用程序二进制文件时，这并不成立。我们分析了整个Debian10 x86-64存储库 (主要，贡献 和 ⾮免费） 并统计了553个ELF可执⾏⽂件，包括⾄少⼀次调⽤系统调⽤指令：这些代表不通过libc 执⾏系统调⽤程序，因此 libc 级别的⼆进制兼容unikernel 不⽀持这些程序。这种有限的libc级兼容性组织了这些系统运行相对较大范围的应用程序，这些应用程序将从座位unikernel的执行中受益匪浅。举几个例子，大量的云服务是用Go语言编写的，Go是一种无须标准C库即可执行大多数系统调用的语言。此外，由于在系统调用层面缺乏兼容性，OSv不支持最流行的HPC共享内存编程框架OpenMP，最后libc接口排除了对静态二进制文件的支持。
HermiTux代表了一种尝试，通过增加一个更加标准和一致使用的接口（系统调用级别）上进行接口来进一步推动unikernel的兼容性程度。</description></item><item><title>阿里云FAASNET无服务器容器方案</title><link>https://icorer.com/icorer_blog/posts/alibaba_cloud_faasnet_serverless_container_solution/</link><pubDate>Tue, 18 Jan 2022 21:24:16 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/alibaba_cloud_faasnet_serverless_container_solution/</guid><description>背景这篇论文中采用容器化方案来实施ServerLess的落地过程，论文内部一方面基于大量的数据统计、一方面提出来FT树结构，用来优化容器冷启动。FAASNET是第一个为FaaS优化的容器运行时提供的端到端综合解决方案，FAASNET使用轻量级、分散和自适应的函数树来避免主要平台的瓶颈。
大会地址：https://www.usenix.org/conference/atc21/presentation/wang-ao
开源地址：https://github.com/mason-leap-lab/FaaSNet
一、网络流量的峰谷比 就如上图所示，不同的应用场景下，流量的高峰和低峰的请求比例是不一样的，比如游戏、IOT场景下流量的峰谷比高于22，这种峰谷比也表明了ServerLess场景的优势。
二、容器的冷启动情况冷启动的延迟对于Faas提供商是致命的，阿里巴巴首先对于冷启动的分布情况作了调研：
对于北京地区，大约57%的镜像拉取时间超过45秒 对于上海地区，超过86%的镜像拉取时间至少需要80秒 显示超过50%和60%的函数调用请求花费至少80%和72%的整体函数启动时间来拉取容器镜像，这表明镜像拉取时间成为了大多数功能的冷启动成本。 冷启动的成本，还需要结合冷启动的间隔时间和功能持续时间来综合评价，在两个地区内部，大约49%的功能冷启动的到达时间小于1秒。
三、FAASNET技术内幕 在图3(d)中可以看出北京地区的80%函数执行时间超过1秒，上海地区80%的函数执行时间小于32.5秒，90th百分位数为36.6秒，99th百分位数为45.6秒。这种分布说明冷启动优化是必要的。
优化容器配置的性能将为降低基于容器的云功能的冷启动成本带来巨大的好处。
2.1 设计概述FAASNET将跨虚拟机的容器配置分散化和并行化，引入了名为函数树（FT）的抽象，以实现高效的容器配置规模。FAASNET将FT管理器组件和一个工作者组件整合进入FAAS调度器和虚拟机代理中，以协调FT管理，阿里的Faas平台主要包含以下几个组成部分，工作的主体组成包括：
网关：租户身份管理认证，将函数请求转发给FAAS调度器，将常规的容器镜像转换为I/O高效数据结构， 一个调度器负责为函数调用请求提供服务，将FAASNET FT管理器集成到调度器来管理函数数、简称FT，通过FT的增删API进行管理。一个FT是一个二进制的树状覆盖，它连接多个虚拟机，形成一个快速和可扩展的容器供应网络。每个虚拟机运行一个FAAS代理，负责虚拟机本地的功能管理。将一个FAASNET工作者集成到VM代理中，用于容器的供应任务。 在函数调用的路径上，如果没有足够的虚拟机、或者所有的虚拟机都很忙的情况下，调度器首先与虚拟机管理器进行通信，从空闲的虚拟机池中扩展出活动的虚拟机池。然后调度器查询其本地的FT元数据，并向FT的FAASNET工作者发起RPC请求，从而启动容器供应流程。容器运行时供应过程实际上是分散的， 并在FT尚未有容器运行的本地供应的虚拟机进行准备工作。调度器在关键路径之外，而FAASNET工作层根据需求获取函数容器层，并从分配的对等虚拟机中并行地创建容器运行时。 在函数部署路径上，网关将函数的常规容器镜像转换为I/O的有效格式，从面向租户的容器注册表中提取常规镜像，逐块压缩镜像层，创建一个包含格式相关信息的元数据文件（镜像清单），并将转换后的层及其清单分别写入阿里云内部的容器注册表和元数据存储。 2.2 FT功能树论文中针对重点强调在设计FT时做了以下选择：
FT是和函数进行绑定的，FAASNET以函数为粒度来管理FT。 FT具备解耦的数据面和控制面，FT的每个虚拟机工作者都具有等同的、简单的容器供应（数据平面）的角色，而全局树管理（控制平面）则交给调度器。 FAASNET采用平衡的二叉树结构，可以动态的适应工作负载。 这些选择结合阿里云，可以达到以下目标：
最大限度的减少容器镜像和层数据下载的I/O负载。 消除中央根节点的树状管理瓶颈和数据播种瓶颈、这里阿里内部镜像采用P2P分发，播种友好。 适应虚拟机的动态加入和离开。 以函数的粒度管理树， FAASNET为每一个至少被调用过一次但未回收的函数管理一个单独、唯一的树。图5说明一个横跨5个虚拟机的三级FT拓扑结构。函数容器镜像从书的根部虚拟机往下流，直到达到叶子节点。
平衡的二叉树，FAASNET的核心是平衡的二进制树，在二进制树中，除了根节点和叶子节点，每个树节点（宿主虚拟机）有一条传入边和两条传出边。这种设计可以有效限制每个虚拟机的并发下载操作的数量，以避免网络争用。一个有N个节点的平衡二叉树的高度为log(N)，这种关系也限制了函数镜像和层数据从顶部到底部的最多跳跃次数。树的高度会影响数据传播的效率，并且二叉树的结构可以动态变化，以适应工作负载的动态化。FAASNET把每个FT组织成一个平衡的二叉树，FT管理程序调用两个API：增加和删除，以动态地增加或缩小一个FT。
插入，FT的第一个节点会被当做根节点插入，FT管理器通过BFS（广度优先搜索）跟踪每个树节点的子节点数量，并将所有拥有0或1个子节点的节点存储在一个队列中。要插入一个新节点，FT管理器会从队列中挑选第一个节点作为新节点的父节点。
删除，调度器可能会回收闲置了一段时间的虚拟机（阿里云配置为15分钟），因此FAAS虚拟机的寿命是有限的。为了使用这种虚拟机的回收，FT管理器调用删除来回收虚拟机。删除操作也会在需要的时候重新平衡FT的结构。与二进制搜索树（如AVL、红黑树）不同，FT的节点没有可比较的键值（及其相关值）。因此，FT树的平衡算法只有当任何节点的左右子树的高度差大于1就会触发平衡操作。
2.3 FT与FAAS整合论文中的FT整合是在阿里云的FAAS环境中，主要整合了FAAS平台的调度器和虚拟机代理。阿里把FAASNET的FT管理器集成到阿里云的FAAS调度器中，并将FAASNET的VM工作者集成到阿里云的FASS-VM代理中用于调度管理FT的虚拟机。
通过FT管理者，调度器在每个虚拟机代理上启动一个FAASNET工作者，工作者负责：
为调度员的命令提供服务，执行镜像下载和容器供应的任务 管理虚拟机上的函数容器。 FT元数据管理，调度器维护一个内存映射表，记录&amp;lt;functionID,FT&amp;gt;键值对，他将一个函数ID映射到其相关的FT数据结构。一个FT数据结构管理着一组代表函数和虚拟机的内存对象，以跟踪虚拟机的地址：端口信息。调度器是分片的，是高度可用的。每个调度器分片会定期将内存中的元数据状态与运行etcd的分布式元数据服务器同步。
函数在虚拟机上的放置，为了提高效率，FAASNET允许一个虚拟机容纳属于同一个用户的多个函数。只要虚拟机有足够的内存来承载函数，一个虚拟机可能参与到多个重叠的FT的拓扑结构中。
图8显示了一个可能的FT布局的例子，为了避免网络瓶颈，FAASNET限制了一台虚拟机可以放置的函数数量，目前设置是20个。
容器供应协议，FAASNET设计了一个协议来协调调度器和容器之间的RPC通信。
调度器和FAASNET的虚拟工人，并促进容器的供应。在一个调用请求中，如果调度器发现没有足够的活动虚拟机为请求提供服务，或者当前所有虚拟机都忙于为请求提供服务，调度员会从空闲的虚拟机池中保留一个或多个新的虚拟机，然后进入容器供应流程。
当调度器将函数元数据发送给VM，VM一旦收到信息会执行两个任务。从元数据存储库加载并检查清单，获取镜像层的URL，并把URL信息持久化到VM的本地存储中。VM回复调度器表明自己已经准备好开始为请求的函数创建容器运行时，调度器收到回复后向VM发送一个创建容器的RPC请求，VM处理清单配置，并向调度器发送一个RPC表明容器已经成功创建。
FT容错，调度器定期ping虚拟机，可以快速检测虚拟机故障。如果一个虚拟机发生故障，调度器会通知FT管理器执行树平衡操作以修复FT拓扑结构。
2.4 FT设计讨论FAASNET将元数据繁重的管理任务卸载到现有的FAAS调度器上，因此每个单独节点都扮演着从其父级对等获取数据的相同角色。FT的根节点没有父级对等物，而是从注册表中获取数据。FAASNET的FT设计可以完全消除到注册中心的I/O流量，只要一个FT至少有一个活跃的虚拟机存储所请求的容器。早些时候，我们的工作负载分析显示，一个典型的FAAS应用的吞吐量将始终高于0RPS，在实践中请求突发更有可能讲一个FT规模从1到N，而不是从0到N。
另一种设计是更细粒度的层（blobs）来管理拓扑关系。在这种方法中，每个单独的层形成一个逻辑树层，属于一个函数的容器镜像的层最终可能驻留在不同的虚拟机上。注意FAASNET的FT是层树模型的一个特例。
图10中显示了一个例子，在这个例子中，一个虚拟机中存储着不同函数容器镜像的层文件，因此当许多下游的虚拟机同事从这个虚拟机获取层时，可能会出现网络瓶颈。这是因为许多重叠的层树形成了一个完全连接的、端对端的网络拓扑结构。如果虚拟机用高带宽的网络连接，全对全的拓扑结构可能会有很好的规模。然而如果每个虚拟机都收到了资源限制，全对全的拓扑结构很容易造成网络瓶颈，阿里云内部使用的是2核CPU、4G内存、1Gbps网络的小型VM。
现有的容器分配技术依靠强大的根节点来完成一系列任务，包括数据播种、元数据管理、P2P拓扑结构管理。将这些框架移植到FAAS平台上，需要额外的、专用的、分片的根节点，这将给运营商增加不必要的成本。另一方面，FAASNET的FT设计使每个虚拟机工作者的逻辑保持简单，同时所有的调度逻辑卸载到现有的调度器。这种设计自然消除了网络I/O瓶颈和根节点的瓶颈。Kraken采用了基于层的拓扑结构，具有强大根节点。
2.5 优化I/O高效的数据格式，常规的docker pull 和 docker start是低效和耗时的，因为整个容器镜像和所有层的数据都必须从远程容器注册中心下载，然后才能启动容器。为了解决这个问题，阿里云内部设计了一个新的基于块的镜像获取机制，这种机制使用了一种I/O高效的压缩数据文件格式。原始数据被分割成固定大小的块，并分别进行压缩。一个偏离表被用来记录压缩文件中每个压缩块的偏移量。
FAASNET使用相同的数据格式来管理和配置代码包。一个代码包被压缩成一个二进制文件，它被虚拟机代码提取并最终安装在一个函数容器内。FAASNET分配代码包的方式与分配容器镜像的方式相同。
按需I/O，对于不需要在启动时一次性读取所有镜像层的应用程序，基于镜像块的获取方式提供了一个懒惰的按需方式从远程存储获取细粒度的镜像层数据。一个FAASNET的VM工作者从元数据存储中下载镜像的清单文件，并在本地进行镜像加载以加载.tar镜像清单，然后它计算第一个和最后一个压缩块的索引，然后查询偏移表以找到偏移信息。最后，它读取压缩块并解压，知道读取的数据量与要求的长度一致。由于底层（远程）块存储设备的读取必须是块边界对齐，应用程序可能会读取和解压比要求的更多的数据，造成读取放大。然而，在实践中，解压算法实现的数据吞吐量比块存储或网络的数据吞吐量高的多。在我们的使用场景中，用额外的CPU开销换取降低I/O成本是有益的。
RPC和数据流，FAASNET内部建立了一个用户态、零拷贝的RPC库。这种方法利用非阻塞的TCP sendmsg和recvmsg来传输一个 struct iovec 不连续的缓冲区。RPC库把RPC头直接添加到缓冲区，以便在用户空间实现高效、零拷贝的序列化。RPC库对请求进行标记、以实现请求流水线和失序接收，类似HTTP2的多路复用。当FAASNET工作者受到一个完整的数据块时，工作者会立即将该数据块传输给下游的节点。
三、FAASNET评测3.1 实验方法使用中等规模500个虚拟机池和一个大规模的1000个虚拟机池，所有的虚拟机均使用2核CPU、4GB内存、1Gbps网络的实例类型，维护一个免费的虚拟机池，FAASNET可以保留虚拟机实例来启动云函数。这样容器配置的延迟就不包括冷启动虚拟机实例的时间，FAASNET使用512KB的块大小，用于按需取用。
系统比较，FAASNET和一下三种配置进行比较。
Kraken，Uber的基于P2P的注册系统。 baseline，阿里巴巴云函数计算目前的生产设置，使用docker pull 从集中的容器中心下载镜像。 on-demand，一个基于baseline的优化系统，但按需获取容器层数据。 DADI+P2P，阿里巴巴的DADI启动了P2P，这种方法使用一个资源受限的虚拟机作为根节点来管理P2P拓扑结构。 目的，回答以下问题：</description></item><item><title>⽤于区块链可扩展性的⾼效能 FPGA-Redis 混合 NoSQL 缓存系统</title><link>https://icorer.com/icorer_blog/posts/blockchain_fpga_redis_nosql/</link><pubDate>Tue, 25 May 2021 22:35:12 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/blockchain_fpga_redis_nosql/</guid><description>一、FPGA-Redis介绍鼓舞人心的区块链技术在加密货币以外的领域取得了很多采用和成功领域落地，因为它的好处已经被探索和成功测试。可扩展性是区块链的最大挑战之一，许多设备（轻量级节点）尤其是物联网依赖于完整的区块链服务器，因此需要减少服务器上的工作负载以获得高性能。这篇论文提出了一种高性能、高效的混合（多级）和分布式NoSQL缓存系统，用于提高区块链应用程序的可扩展（吞吐量）。我们研究了区块链中的性能瓶颈，并设计了一种高效的千兆以太网FPGA NoSQL缓存架构，该架构通过Hiredis C客户端与Redis数据库协同工作。Curl和Jansson用于连接区块链。我们为特定于区块链的高效缓存设计了一个定制的SHA-256核心。我们的结果显示，当FPGA上发生缓存命中时，性能提高了103倍。所提出的FPGA-Redis系统获得了高达4.09倍的改进，还实现了较小的FPGA面积利用率和较低的功耗。
二、概述区块链技术激励了许多人，帮助许多企业和政府改进系统，解决了信任、安全、速度、成本、效率和中心化等诸多瓶颈问题。英国政府办公室的报告确认区块链可以保护数据、降低成本并为记录提供透明度。区块链由中本聪于2008年首次提出并支持加密货币（比特币）和许多其他用于医疗保健、身份管理、网络安全等应用程序。Corda是R3的区块链（由200多家公司组成的联盟，主要是金融机构），用于增强商业交易和网络，R3一直在为企业使用和探索区块链。
尽管区块链很强大，但可扩展性（低吞吐量、高延迟、存储问题和读取性能差）是其巨大的挑战，但研究较少。与非区块链应用程序相比，区块链应用程序的吞吐量要低很多。比特币和以太坊支持每秒3-4和15-20笔交易（TPS），而Visa和PayPal分别支持1667和193TPS，另一方面，与非区块链服务器相比，区块链服务器的查询响应（读取性能）也很差。例如在我们处理约每秒96个响应的区块链系统中，查询延迟超过10毫秒。同样，Blockcypher 区块链服务器⽀持每秒3个请求。与⾮区块 链服务器相⽐，Google和YouTube分别处理每秒84,405个请求和每秒85,021次观看。糟糕的读取性能是由于区块链的结构和巨大的尺寸（比特币超过288GB）以及区块链数据存储在硬盘上的事实。与用于存储Redis等NoSQL缓存的RAM（快150,000倍）不同，硬盘具有较高的访问延迟。由于这种糟糕的读取性能，现有的区块链无法处理有效服务器所需的每秒大量客户端请求。许多轻量级节点（数以千计的物联网设备和简化验证（SPV）节点），仅依赖区块链服务器来获取区块链数据，因为其庞大的规模，它们无法存储完整的区块链。现在越来越多的轻量级客户端使用区块链并将更多的工作放在区块链服务器上，因此必须减少区块链服务器上的工作量以提升性能，从而更好地扩展区块量应用程序。
NoSQL缓存是提高和增强区块量服务器读取性能的一种有效方式。Redis、Hadoop和Memcached等NoSQL缓存如今已广泛用于大型Web数据中心，例如Yahoo、Twitter、Facebook、Youtube甚至Google，其中数百个分布式NoSQL部署缓存服务器是为了改善许多性能和可伸缩问题并节省成本。NoSQL缓存具有非常高的性能进行大规模水平扩展的优势，并且比使用更强大的CPU和内存（垂直扩展）更新现有服务器更经济。水平扩展是指使用廉价商品服务器的副本来获得更好的性能，而不是传统的垂直扩展，其中将更强大的资源添加到单个服务器使系统更加昂贵。仅苹果公司就使用了超过75000个NoSQL缓存（Cassandra）表格系列集群来存储超过10PB的数据。
分布式 NoSQL 缓存由于其⾼性能以及区块链请求（尤其是块头请求）的时间局部性，可以极⼤地提⾼区块链服务器响应的吞吐量和延迟性能。许多轻量级节点（如简化⽀付验证节点（特殊⽬的公司 )在添加新块的⼏个⼩时内。
尽管具有⾼性能，但软件 NoSQL 缓存在⾼性能时会消耗⾼功率和更多CPU 资源（在⽹络处理上）。因此，当 FPGA 发⽣缓存命中时，使⽤ FPGA来降低功耗和 CPU 资源消耗并提⾼性能。然⽽，FPGA 中的⼩尺⼨和有限的内存给可以缓存在 FPGA 上的数据量带来了缺陷和限制，从⽽通过增加 FPGA 的未命中率来影响系统性能。
本⽂研究了区块链中的性能瓶颈，并提出了⼀种⾼效的⾼性能混合分布式NoSQL FPGA-Redis 缓存系统，以减少区块链服务器的⼯作负载并提⾼其性能。我们设计并实现了⼀个千兆以太⽹ FPGA ⽹络接⼝控制器 (NIC)，该控制器包含键值存储，⽤于有效地缓存 FPGA 上的区块链数据。Redis 软件缓存和 Redis 应⽤程序内置在 Redis 服务器 PC 中，它通过 FPGA 上实现的千兆总线主控直接内存地址 (BMD) PCI Express (PCIe) 端点连接到 NIC。Redis 应⽤程序使⽤Hiredis API（实现与Redis缓存对话的Redi 的C客⼾端）。整个缓存系统通过我们的服务器应⽤程序中内置的Curl和Jansson API 连接到全节点区块链服务器。
该系统改善了FPGA NoSQL缓存内存⼩的缺点，同时以更低的功耗提⾼了 软件缓存的性能。FPGA 和 Redis 协同⼯作。Redis 通过提供另⼀个缓存层来补充 FPGA 缓存的有限内存。当在 FPGA 上未找到请求的数据（发⽣缓存未命中）时，数据从 Redis（如果缓存）⽽不是存储在主内存中的主区块链中获取。由于 Redis ⽐主存更快，因此整个系统的性能得到了提⾼。反过来，FPGA 通过处理⽹络处理来降低Redis的⾼性能和CPU资源消耗。我们只在FPGA和Redis（包括缓存）上缓存频繁的请求（即块头、确认、块⾼度、时间跨度和 Merkle根），⽽在Redis上缓存不频繁和⼤数据请求（例如块请求）只要。此外，仅当FPGA上发⽣缓存未命中时才检查Redis 缓存。</description></item><item><title>TCP长连接在K8S环境下的负载均衡分析</title><link>https://icorer.com/icorer_blog/posts/cloudnative_k8s_tcp_upstream_balance/</link><pubDate>Fri, 25 Dec 2020 11:28:12 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/cloudnative_k8s_tcp_upstream_balance/</guid><description>K8S不支持长连接的负载均衡，所以负载可能不是很均衡。如果你在使用HTTP/2，gRPC, RSockets, AMQP 或者任何长连接场景，你需要考虑客户端负载均衡。
TL;DR: Kubernetes doesn&amp;rsquo;t load balance long-lived connections, and some Pods might receive more requests than others. If you&amp;rsquo;re using HTTP/2, gRPC, RSockets, AMQP or any other long-lived connection such as a database connection, you might want to consider client-side load balancing.
Kubernetes提供了两种方便的抽象来部署应用程序：Services 和 Deployments。 Deployments描述了在任何给定时间应运行哪种类型以及多少个应用程序副本的方法。每个应用程序都部署为Pod，并为其分配了IP地址；另一方面，Services类似于负载平衡器。它们旨在将流量分配给一组Pod。
将Services视为IP地址的集合通常很有用。每次您对Services提出请求时，都会从该列表中选择一个IP地址并将其用作目的地。 如果您有两个应用程序（例如前端和后端），则可以为每个应用程序使用Deployment和Service，然后将它们部署在集群中。 当前端应用发出请求时，不需要知道有多少Pod连接到后端服务；前端应用程序也不知道后端应用程序的各个IP地址。当它想要发出请求时，该请求将发送到IP地址不变的后端服务。 但是该服务的负载平衡策略是什么？
Kubernetes Services中的负载平衡Kubernetes Services不存在，没有进程监听服务的IP地址和端口。
您可以通过访问Kubernetes集群中的任何节点并执行netstat -ntlp来检查是否存在这种情况。
甚至在任何地方都找不到IP地址,Services的IP地址由控制器管理器中的控制平面分配，并存储在数据库etcd中。然后，另一个组件将使用相同的IP地址：kube-proxy。
Kube-proxy读取所有Services的IP地址列表，并在每个节点中写入一组iptables规则。这些规则的意思是：“如果看到此Services IP地址，则改写请求并选择Pod之一作为目的地”。Services IP地址仅用作占位符-这就是为什么没有进程监听IP地址或端口的原因。 iptables是否使用轮询？不，iptables主要用于防火墙，并且其目的不是进行负载平衡。但是，您可以制定一套聪明的规则，使iptables像负载均衡器一样工作。而这正是Kubernetes中发生的事情。
如果您有三个Pod，则kube-proxy编写以下规则：
选择Pod 1作为目的地，可能性为33％。 否则，移至下一条规则 选择Pod 2作为目的地，可能性为50％。 否则，请移至以下规则 选择Pod 3作为目的地（没有可能性） 复合概率是Pod 1，Pod 2和Pod 3都有三分之一的机会被选中（33％）。</description></item><item><title>微服务治理：APM-SkyWalking-PHP内核扩展源码分析</title><link>https://icorer.com/icorer_blog/posts/microservice_skywalking_php_kernel_source_analyze/</link><pubDate>Mon, 07 Sep 2020 14:42:13 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/microservice_skywalking_php_kernel_source_analyze/</guid><description>SkyWalking APM作为服务遥测的关键技术点，为了能够更好地运用这项技术，我们需要拥有把握这项技术的底层能力。目前公司在PHP领域存活不少业务系统，针对PHP领域的APM技术，我们首先从分析这款PHP内核扩展程序下手。
一. 总体架构PHP内核在php-fpm运行模式下是短生命周期，短生命周期的脚本运行如果直接连接SkyWalking的oap-server会造成大量的性能损耗，而且php也不擅长grpc通信，因此借助mesh架构思想为PHP-FPM进程池增加一个数据SideCar，主要的结构如下图所示： 从上图可以看出，PHP内核扩展程序拦截内核运行数据（主要是关键的外部IO调用）、数据被发送给SideCar，SideCar流转数据到SkyWalking-Server，数据流还可以被SkyWalking进行分析、从而通过WebHook流转报警时间到相关后续平台里。
二. PHP内核扩展源码分析针对目前开源社区的SkyWalking-PHP内核源码进行分析，源码的分析主要包括以下几部分：
工程结构分析 关键生命周期分析 关键运行函数Hook分析 2.1 工程结构分析SkyWalking PHP内核组件工程结构比较简单，主要是站在PHP内核基础上进行扩展设计与实现，主要包含的扩展文件有：
b64.h：base64编码函数的头文件、主要包含内存分配、base64字符表、b64_encode及b64_decode、b64_decode_ex的函数声明。 decode.c：base64序列化的函数具体实现。 encode.c：base64反序列化的函数具体实现。 components.h：针对skywalking协议中的component部分进行宏定义、这部分是apm协议的一部分，例如：tomcat、httpclient、dubbo、okhttp、grpc、jedis、更多查看附录1。 php_skywalking.h：关键的内核扩展声明部分，主要包括：APM协议宏定义、Redis指令类别、memcache指令类别、ContextCarrier上下文结构体、apm拦截所需的关键函数定义（具体见附录二），apm关键函数hook定义（具体见附录三），全局变量定义（具体见附录四）。 skywalking.c：具体内核扩展实现文件，里面包含了MI-MS、RI-RS、关键函数Hook等处理逻辑。 2.2 关键生命周期分析这块将针对内核扩展关键生命周期进行分析。
2.2.1 关键生命期函数Hook定义1static void (*ori_execute_ex)(zend_execute_data *execute_data); //PHP内核原始PHP层执行流程函数指针 2static void (*ori_execute_internal)(zend_execute_data *execute_data, zval *return_value);//PHP原始内核执行函数指针 3ZEND_API void sky_execute_ex(zend_execute_data *execute_data);//skywalking针对PHP层执行函数的替换指针 4ZEND_API void sky_execute_internal(zend_execute_data *execute_data, zval *return_value);//skywalking针对原始内核执行函数的替换指针 2.2.2 php.ini配置解析周期 1PHP_INI_BEGIN() 2#if SKY_DEBUG 3 STD_PHP_INI_BOOLEAN(&amp;#34;skywalking.enable&amp;#34;, &amp;#34;1&amp;#34;, PHP_INI_ALL, OnUpdateBool, enable, zend_skywalking_globals, skywalking_globals) //读取skywalking.enable配置项 4#else 5 STD_PHP_INI_BOOLEAN(&amp;#34;skywalking.enable&amp;#34;, &amp;#34;0&amp;#34;, PHP_INI_ALL, OnUpdateBool, enable, zend_skywalking_globals, skywalking_globals) //读取skywalking.enable配置项 6#endif 7 STD_PHP_INI_ENTRY(&amp;#34;skywalking.</description></item><item><title>微服务治理：服务遥测之APM-SkyWalking技术应用</title><link>https://icorer.com/icorer_blog/posts/microservice_governance_apm_application/</link><pubDate>Mon, 07 Sep 2020 13:33:12 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/microservice_governance_apm_application/</guid><description>一. 背景描述微服务应用过程中，如何构建微服务的可观测性，主要从以下三个方面进行考虑：
服务日志（log） 服务指标（metric） 服务链路（trace） 这三个服务监控领域有不同的技术栈进行支撑，但是如何快速构建一个基础的服务可观测能力？尽量减少业务的侵入性、尽量多的增加业界标准的观测指标，这里我就推荐APM技术体系，在APM技术领域中SkyWalking是一个优秀的解决方案。 二. 技术结构SkyWalking 在我当前公司的落地领域中，主要围绕PHP、Go两大技术领域，JAVA生态拥有SkyWalking默认友好支撑，针对PHP、Go这两种技术栈，主要包含的APM体系技术结构如下图所示： 从技术结构图可以看出，APM技术体系主要包括以下几部分：
技术结构最底层采用了apache SkyWalking开源项目作为方案支撑。 Go生态使用Go2sky客户端进行APM数据丰富与数据包发送。 PHP生态由于自身短生命期的特征，分为PHP内核APM扩展和数据中转SideCar两部分，PHP内核扩展通过函数Hook机制完成Redis、MySQL、PDO、grpc等关键网络IO的拦截，并无感构建APM数据包结构，在RS周期发送APM数据包到SideCar，SideCar负责流转PHP内核的APM监控数据包到APM-Server上。 三. 关键领域监控APM技术生态包含内容比较多，主要的使命就是对于服务应用进行运行态监控，这里主要阐述一下几方面的监控效果：
3.1 服务指标监控服务指标监控主要包括Apdex、平均响应时间、成功率、CPM、TP数据、也包括很多的服务EndPoint数据，主要用来阐述服务健康、性能、可靠性的指标数据。 3.2 服务调用链监控微服务场景下，调用关系复杂、服务调用关系层级深，所以APM构建了服务调用链监控体系，方面研发、架构对于自己服务的调用关系有较好的可视化效果，调用链也遵循OpenTracing协议，主要效果如下所示： 3.3 微服务内核Runtime监控服务监控除了需要对于服务自身的可靠性、服务之间的调用关系进行监控之外，还需要针对服务Runtime进行拦截分析，通常的实现方式有OAP编程、内核Runtime Hook方式，Runtime监控可以很好的监控服务不同EndPoint内部的关键不稳定点的性能情况，除了PDO、Redis、Mysql、GRPC等关键IO，也可以监控长时间的cpu计算等程序行为逻辑。 主要的效果图如下： 3.4 微服务拓扑关系监控针对微服务调用关系，除了可以使用全链路Trace这种表达形式，也可以通过更具有动感效果的拓扑关系图进行描述，在拓扑关系图中可以形象的显示服务的类别、服务的流量走向、服务的当前状态、服务调用间的频率等数据。相关的效果图如下所示: 三. 总结APM技术体系对于微服务治理工作有超强的观测领域能力的弥补，增强服务的可观测程度，是微服务治理的重要工作。</description></item><item><title>UtahFS: Encrypted File Storage - 加密文件存储</title><link>https://icorer.com/icorer_blog/posts/utahfs_encrypted_file_storage/</link><pubDate>Sun, 14 Jun 2020 10:17:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/utahfs_encrypted_file_storage/</guid><description>加密是最强大的技术之一，每个人每天都在不知不觉中使用它。传输层加密现在已经无处不在，因为它是创建可信赖的Internet的基本工具，它可以保护通过Internet发送到目标目的地的数据。磁盘加密技术可以无所不在地保护您的数据，因为它可以防止任何窃取您设备的人也能够看到您台式机上的内容或阅读您的电子邮件。
这项技术的下一个改进是端到端加密，它是指只有最终用户才能访问其数据的系统，而没有任何中间服务提供商。这类加密的一些最流行的例子是WhatsApp和Signal等聊天应用。端到端加密显著降低了用户数据被服务提供商恶意窃取或不当处理的可能性。这是因为即使服务提供商丢失了数据，也没有人拥有解密数据的密钥！
几个月前，我意识到我的计算机上有很多敏感文件（我的日记，如果你一定知道的话），我担心会丢失，但我不喜欢将它们放入Google Drive或Dropbox之类。尽管Google和Dropbox是绝对值得信赖的公司，但它们不提供加密功能，在这种情况下，我确实希望完全控制自己的数据。
环顾四周，我很难找到符合我所有要求的东西：
会同时加密和验证目录结构，这意味着文件名是隐藏的，其他人不可能移动或重命名文件。 查看/更改大文件的一部分不需要下载并解密整个文件。 是开源的，并且有一个文档化的协议。 所以我开始建立这样一个系统！最终我把它称为“ UtahFS”，其代码在此处提供。请注意，这个系统在Cloudflare的生产中没有使用：它是我在我们的研究团队工作时构建的概念。这篇博客文章的其余部分描述了我为什么要像以前那样构建它，但是如果您想跳过它，则代码仓库中有关于实际使用它的文档。
Storage Layer(存储层)存储系统的第一个也是最重要的部分是…存储。为此，我使用对象存储，因为它是在别人的硬盘上存储数据的最便宜和最可靠的方法之一。对象存储只不过是一个由云提供程序托管的键值数据库，通常被调整为存储大约几千字节大小的值。有许多具有不同定价方案的不同提供商，例如Amazon S3，Backblaze B2和Wasabi。它们全部都能够存储TB级的数据，并且许多还提供地理冗余。
Data Layer(数据层)对我来说很重要的一个要求是，在能够读取一部分文件之前，不必下载和解密整个文件。这一点很重要的一个地方是音频和视频文件，因为它能够快速开始播放。另一个例子是ZIP文件：许多文件浏览器都具有浏览压缩档案（例如ZIP文件）的能力，而无需将其解压缩。要启用此功能，浏览器需要能够读取存档文件的特定部分，仅解压缩该部分，然后移动到其他位置。
在内部，UtahFS从不存储大于配置大小（默认为32 KB）的对象。如果文件中的数据量超过该数据量，则该文件将分成多个对象，这些对象通过跳表连接。跳表是链接列表的稍微复杂一点的版本，它允许读者通过在每个块中存储指向比指向前一跳更远的其他指针来快速移动到随机位置。
当跳表中的块不再需要时，因为文件已被删除或截断，它们将被添加到特殊的“回收站”链接列表中。例如，当需要在其他位置使用块时，可以回收垃圾列表的元素，以创建新文件或将更多数据写入现有文件的末尾。这将最大限度地重用，意味着仅当垃圾箱列表为空时才需要创建新块。一些读者可能认为这是《计算机编程艺术：第一卷，2.2.3节》中描述的链接分配策略！ 使用链接分配的根本原因是，对于大多数操作而言，这是最有效的。 而且，这是一种分配内存的方法，该方法将与我们在接下来的三个部分中讨论的加密技术最兼容。
Encryption Layer(加密层)既然我们已经讨论了如何将文件分成块并通过跳表进行连接，我们就可以讨论如何实际保护数据。这有两个方面：
第一个是机密性，它对存储提供者隐藏每个块的内容。 只需使用AES-GCM加密每个块，并使用从用户密码中获得的密钥，即可实现这一点。
该方案虽然简单，但不提供前向保密或后向安全。前向保密意味着，如果用户的设备遭到破坏，攻击者将无法读取已删除的文件。后泄露安全性意味着一旦用户的设备不再泄露，攻击者将无法读取新文件。不幸的是，提供这两种保证之一意味着在用户的设备上存储加密密钥，这些密钥需要在设备之间同步，如果丢失，将使存档无法读取。
此方案也无法防止脱机密码破解，因为攻击者可以获取任何加密的块，并一直猜测密码，直到找到有效的块为止。通过使用Argon2（这使得猜测密码更为昂贵）和建议用户选择强密码，可以在一定程度上缓解这种情况。
我肯定会在将来改进加密方案，但认为上面列出的安全属性对于初始发行版来说太困难和脆弱。
Integrity Layer(完整性层)数据保护的第二个方面是完整性，它确保存储提供程序没有更改或删除任何内容。这是通过在用户数据上构建Merkle树来实现的。Merkle树在我们关于证书透明性的博客文章中得到了深入的描述。Merkle树的根哈希值与版本号相关联，该版本号随每次更改而递增，并且根哈希值和版本号均使用从用户密码派生的密钥进行身份验证。这些数据存储在两个位置：对象存储数据库中的一个特殊密钥下，以及用户设备上的一个文件中。
每当用户想从存储提供程序读取一块数据时，他们首先请求远程存储的根目录，并检查它是否与磁盘上的相同，或者版本号是否大于磁盘上的版本号。检查版本号可防止存储提供程序将存档还原为未检测到的以前（有效）状态。然后，可以根据最新的根散列验证读取的任何数据，该散列可防止任何其他类型的修改或删除。
在此处使用Merkle树的好处与“证书透明性”的好处相同：它使我们能够验证单个数据，而无需立即下载并验证所有内容。 另一个用于数据完整性的常用工具称为消息身份验证码（Message Authentication Code，简称MAC），虽然它既简单又有效，但它无法只进行部分验证。
我们使用Merkle树不能防止的一件事是分叉，在分叉中，存储提供商向不同的用户显示不同版本的存档。然而，检测fork需要用户之间的某种流言蜚语，这已经超出了最初实现的范围。
Hiding Access Patterns(隐藏访问模式)Oblivious RAM, or ORAM,是一种用于以随机方式对随机存取存储器进行读写的加密技术，它可以从存储器本身中隐藏执行了哪个操作（读或写）以及对该操作执行到了存储器的哪一部分！在我们的例子中，“内存”是我们的对象存储提供程序，这意味着我们要向他们隐藏我们正在访问的数据片段以及访问的原因。这对于防御流量分析攻击很有价值，在这种攻击中，对UtahFS这样的系统有详细了解的对手可以查看其发出的请求，并推断加密数据的内容。例如，他们可能会看到您定期上传数据，几乎从不下载，并推断您正在存储自动备份。
ORAM最简单的实现是始终读取整个内存空间，然后使用所有新值重写整个内存空间，只要您想读取或写入单个值。一个观察内存访问模式的对手将无法判断你真正想要的值，因为你总是触摸所有东西。然而，这将是极其低效的。
我们实际使用的结构称为Path ORAM，它稍微抽象了一点这个简单的方案，使其更有效。首先，它将内存块组织成二叉树，其次，它保留一个客户端表，该表将应用程序级指针映射到二叉树中的随机叶。诀窍是允许一个值存在于任何内存块中，该内存块位于指定叶和二叉树根之间的路径上。
现在，当我们要查找指针指向的值时，我们在表中查找它的指定叶，并读取根和该叶之间路径上的所有节点。我们正在寻找的价值应该在这条路上，所以我们已经拥有了我们需要的！在没有任何其他信息的情况下，对手看到的只是我们从树上读到一条随机路径。 从树中读取的内容看起来像是一条随机路径，最终包含了我们正在寻找的数据。
但是，我们仍然需要隐藏我们是在读还是在写，并重新随机分配一些内存，以确保此查询不会与将来的其他查询相关联。 所以为了重新随机化，我们将刚读取的指针分配给新叶子，然后将值从存储在其之前的块中移到新叶子和旧叶子的父块中。（在最坏的情况下，我们可以使用根块，因为根是所有内容的父对象。）一旦将值移动到适当的块中，并完成应用程序的使用/修改，我们将对提取的所有块重新加密并将其写回内存。这将把值放在根和它的新叶之间的路径中，同时只改变我们已经获取的内存块。 这个结构很好，因为我们只需要触摸分配给二叉树中单个随机路径的内存，这是相对于内存总大小的对数工作量。但即使我们一次又一次地读同一个值，我们每次都会从树上碰到完全随机的路径！但是，额外的内存查找仍然会导致性能损失，这就是为什么ORAM支持是可选的。
Wrapping Up(结束语)在这个项目上的工作对我来说是非常有益的，因为虽然系统的许多单独的层看起来很简单，但它们是许多改进的结果，并很快形成了一些复杂的东西。在这个项目上的工作对我来说是非常有益的，因为虽然系统的许多单独的层看起来很简单，但它们是许多改进的结果，并很快形成了一些复杂的东西。
原文链接：https://blog.cloudflare.com/utahfs/ 开源地址：https://github.com/cloudflare/utahfs</description></item><item><title>云原生架构定义：12因素应用、微服务、自服务、API协作、抗脆弱性</title><link>https://icorer.com/icorer_blog/posts/cloudnative_12_factors/</link><pubDate>Fri, 15 May 2020 14:17:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/cloudnative_12_factors/</guid><description>一. 12因素应用12因素应用是一系列云原生应用架构的模式集合，最初由Heroku提出。这些模式可以用来说明什么样的应用才是云原生应用。它们关注速度、安全、通过声明式配置扩展、可横向扩展的无状态/无共享进程以及部署环境的整体松耦合。如Cloud Foundry、Heroku和Amazon ElasticBeanstalk都对部署12因素应用进行了专门的优化。
在12因素的背景下，应用（或者叫app）指的是独立可部署单元。组织中经常把一些互相协作的可部署单元称作一个应用。
1.1 代码库每个可部署app在版本控制系统中都有一个独立的代码库，可以在不同的环境中部署多个实例。
1.2 依赖App应该使用适当的工具（如Maven、Bundler、NPM）来对依赖进行显式的声明，而不该在部署环境中隐式的实现依赖。
1.3 配置配置或其他随发布环境（如部署、staging、生产）而变更的部分应当作为操作系统级的环境变量注入。
1.4 后端服务后端服务，例如数据库、消息代理应视为附加资源，并在所有环境中同等看待。
1.5 编译、发布、运行构建一个可部署的app组件并将它与配置绑定，根据这个组件/配置的组合来启动一个或者多个进程，这两个阶段是严格分离的。
1.6 进程该app执行一个或者多个无状态进程（例如master/work），它们之间不需要共享任何东西。任何需要的状态都置于后端服务（例如cache、对象存储等）。
1.7 端口绑定该应用程序是独立的，并通过端口绑定（包括HTTP）导出任何/所有服务。
1.8 并发并发通常通过水平扩展应用程序进程来实现（尽管如果需要的话进程也可以通过内部管理的线程多路复用来实现）。
1.9 可任意处置性通过快速迅速启动和优雅的终止进程，可以最大程度上的实现鲁棒性。这些方面允许快速弹性缩放、部署更改和从崩溃中恢复。
1.10 开发/生产平等通过保持开发、staging和生产环境尽可能的相同来实现持续交付和部署。
1.11 日志不管理日志文件，将日志视为事件流，允许执行环境通过集中式服务收集、聚合、索引和分析事件。
1.12 管理进程行政或管理类任务（如数据库迁移），应该在与app长期运行的相同的环境中一次性完成。
这些特性很适合快速部署应用程序，因为它们不需要对将要部署的环境做任何假定。对环境假设能够允许底层云平台使用简单而一致的机制，轻松实现自动化，快速配置新环境，并部署应用。以这种方式，十二因素应用模式能够帮我们优化应用的部署速度。
这些特性也很好地适用于突发需求，或者低成本地“丢弃”应用程序。应用程序环境本身是100％一次性的，因为任何应用程序状态，无论是内存还是持久性，都被提取到后端服务。这允许应用程序以易于自动化的非常简单和弹性的方式进行伸缩。在大多数情况下，底层平台只需将现有环境复制到所需的数目并启动进程。缩容是通过暂停正在运行的进程和删除环境来完成，无需设法地实现备份或以其他方式保存这些环境的状态。就这样，12因素应用模式帮助我们实现规模优化。
最后，应用程序的可处理性使得底层平台能够非常快速地从故障事件中恢复。
此外，将日志作为事件流处理能够极大程度上的增强应用程序运行时底层行为的可见性。
强制环境之间的等同、配置机制的一致性和后端服务管理使云平台能够为应用程序运行时架构的各个方面提供丰富的可见性。以这种方式，十二因素应用模式能够优化安全性。
二. 微服务微服务将单体业务系统分解为多个“仅做好一件事”的可独立部署的服务。这件事通常代表某项业务能力，或者最小可提供业务价值的“原子“服务单元。
微服务架构通过以下几种方式为速度、安全、可扩展性赋能：
当我们将业务领域分解为可独立部署的有限能力的环境的同时，也将相关的变更周期解耦。只要变更限于单一有限的环境，并且服务继续履行其现有合约，那么这些更改可以独立于与其他业务来进行开展和部署。结果是实现了更频繁和快速的部署，从而实现了持续的价值流动。 通过扩展部署组织本身可以加快部署。由于沟通和协调的开销，添加更多的人，往往会使软件构建变得更加苦难。 弗雷德·布鲁克斯（Fred Brooks，人月神话作者）很多年前就教导我们，在软件项目的晚期增加更多的人力将会时软件项目更加延期。 然而，我们可以通过在有限的环境中构建更多的沙箱，而不是将所有的开发者都放在同一个沙箱中。 由于学习业务领域和现有代码的认知负担减少，并建立了与较小团队的关系，因此我们添加到每个沙箱的新开发人员可以更快速地提高并变得更高效。 可以加快采用新技术的步伐。大型单体应用架构通常与对技术堆栈的长期保证有关。这些保证的存在是为了减轻采用新技术的风险。采用了错误的技术在单体架构中的代价会更高，因为这些错误可能会影响整个企业架构。如果我们可以在单个整体的范围内采用新技术，将隔离并最大限度地降低风险，就像隔离和最小运行时故障的风险一样。 微服务提供独立、高效的服务扩展。单体架构也可以扩展，但要求我们扩展所有组件，而不仅仅是那些负载较重的组件。当且仅当相关联的负载需要它时，微服务才会被缩放。 三. 自服务敏捷架构使用云原生应用架构的团队通常负责其应用的部署和持续运营。云原生应用的成功采纳者已经为团队提供了自服务平台。
正如我们创建业务能力团队为每个有界的环境构建微服务一样，我们还创建了一个能力小组，负责提供一个部署和运行这些微服务的平台。
这些平台中最大好处是为消费者提供主要的抽象层。通过基础架构即服务（IAAS），我们要求API创建虚拟服务器实例、网络和存储，然后应用各种形式的配置管理和自动化，以使我们的应用程序和支持服务能够运行。现在这种允许我们自定义应用和支持服务的平台正在不断涌现。
应用程序代码简单地以预构建的工件（可能是作为持续交付管道的一部分生成的）或Git远程的原始源代码的形式“推送”。 然后，平台构建应用程序工件，构建应用程序环境，部署应用程序，并启动必要的进程。 团队不必考虑他们的代码在哪里运行或如何到达那里，这些对用户都是透明得，因为平台会关注这些。
这样的模型同样适合于后端服务。需要数据库？ 消息队列或邮件服务器？ 只需要求平台来配合您的需求。平台现在支持各种SQL/NoSQL数据存储、消息队列、搜索引擎、缓存和其他重要的后端服务。这些服务实例然后可以“绑定”到您的应用程序，必要的凭据会自动注入到应用程序的环境中以供其使用。从而消除了大量凌乱而易出错的定制自动化。
这些平台还经常提供广泛的额外操作能力：
应用程序实例的自动化和按需扩展 应用健康管理 请求到或跨应用程序实例间的动态路由和负载均衡 日志和指标的聚合 这种工具的组合确保了能力团队能够根据敏捷原则开发和运行服务，从而实现速度，安全性和规模化。
四. 基于API的协作在云原生应用架构中，服务之间的唯一互动模式是通过已发布和版本化的API。这些API通常是具有JSON序列化的HTTP REST风格，但也可以是其他协议和序列化格式。
只要有需要，在不会破坏任何现有的API协议的前提下，团队就可以部署新的功能，而不需要与其他团队进行同步。自助服务基础设施平台的主要交互模式也是通过API，就像其他业务服务一样。供给、缩放和维护应用程序基础设施的方式不是通过提交单据，而是将这些请求提交给提供该服务的API。
通过消费者驱动的协议，可以在服务间交互的双方验证协议的合规性。服务消费者不能访问其依赖关系的私有实现细节，或者直接访问其依赖关系的数据存储。实际上，只允许有一个服务能够直接访问任何数据存储。这种强制解耦直接支持云原生的速度目标。
五.抗脆弱性Nassim Taleb在他的Antifragile（Random House）一书中介绍了抗脆弱性的概念。如果脆弱性是受到压力源的弱化或破坏的质量系统，那么与之相反呢？许多人会以稳健性或弹性作出回应——在遭受压力时不会被破坏或变弱。然而，Taleb引入了与脆弱性相反的抗脆弱性概念，或者在受到压力源时变得更强的质量系统。什么系统会这样工作？联想下人体免疫系统，当接触病原体时，其免疫力变强，隔离时较弱。我们可以像这样建立架构吗？云原生架构的采用者们已经设法构建它们了。Netflix Simian Army项目就是个例子，其中著名的子模块“混沌猴”，它将随机故障注入到生产组件中，目的是识别和消除架构中的缺陷。通过明确地寻求应用架构中的弱点，注入故障并强制进行修复，架构自然会随着时间的推移而更大程度地收敛。</description></item><item><title>HTTP/3与HTTP/2的性能比较</title><link>https://icorer.com/icorer_blog/posts/performance_comparison_between_http3_and_http2/</link><pubDate>Thu, 16 Apr 2020 15:17:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/performance_comparison_between_http3_and_http2/</guid><description>这是一篇来自cloudflare公司的博客译文，阐述了一些HTTP3与HTTP2的性能对比。 我们在去年Cloudflare的生日周宣布支持HTTP/3，它是HTTP/2的继承者。我们的目标是并且一直是帮助建立一个更好的互联网。在标准方面的合作是其中的一个重要部分，我们很幸运能在这里做到这一点。
尽管HTTP/3仍然处于草稿状态，但我们已经看到了很多用户的兴趣。到目前为止，已经有超过113000个区域激活了HTTP/3，如果您使用的是一个实验性的浏览器，那么可以使用新的协议访问这些区域！看到这么多人启用HTTP/3真是太棒了：通过HTTP/3访问真正的网站意味着浏览器有更多不同的属性可以测试。
当我们启动对HTTP/3的支持时，我们与Google合作，后者同时在Google Chrome中启动了实验性的支持。从那时起，我们看到更多的浏览器增加了实验性的支持：Firefox加入了他们的夜间版本，其他基于Chrome的浏览器，如Opera和Microsoft Edge通过底层Chrome浏览器引擎，Safari通过他们的技术预览。我们密切关注这些开发，并尽可能地与之合作；拥有一个拥有许多启用了HTTP/3的站点的大型网络，为浏览器实现者提供了一个极好的测试平台，可以用来测试代码。
那么，现在的情况如何，我们现在在哪里？IETF标准化过程将协议开发为一系列文档草稿版本，最终目的是生成一个最终草稿版本，该版本可以标记为RFC。QUIC工作组的成员在分析、实现和互操作规范方面进行协作，以便找到工作不太正常的地方。我们在支持HTTP/3的Draft-23的情况下启动了它，并一直在跟上每一个新的草案，其中27是最新的。在每一份草案中，小组都提高了QUIC定义的质量，并更接近于关于其行为方式的“粗略共识”。为了避免永久性的分析瘫痪和无休止的调整，每一个新的草案都增加了对规范提出修改的门槛。这意味着版本之间的更改更小，最终的RFC应该与我们在生产中运行的协议非常匹配。
优点HTTP/3的主要优点之一是提高了性能，特别是同时获取多个对象。使用HTTP/2，TCP连接中的任何中断（包丢失）都会阻塞所有流（行首阻塞）。因为HTTP/3是基于UDP的，如果一个数据包被丢弃，它只会中断一个流，而不是所有的流。
此外，HTTP/3还提供了0-RTT支持，这意味着在建立连接时，通过消除来自服务器的TLS确认，后续连接可以更快地启动。这意味着客户端请求数据的速度要比完整的TLS协商快得多，这意味着网站可以更早地开始加载。
下面说明数据包丢失及其影响：HTTP/2多路复用两个请求。一个请求从客户端通过HTTP/2到达服务器，请求两个资源（我们将请求及其相关的响应涂成绿色和黄色）。响应被分成多个包，唉，一个包丢失了，所以两个请求都被延迟了。
上面显示了HTTP/3复用2个请求。一个影响黄色响应的数据包丢失，而绿色的数据包运行良好。
会话启动的改进意味着到服务器的“连接”启动得更快，这意味着浏览器开始更快地查看数据。我们很好奇有多大的进步，所以我们做了一些测试。为了衡量0-RTT支持带来的改进，我们运行了一些基准测试时间到第一字节（TTFB）。平均来说，对于HTTP/3，我们看到的第一个字节出现在176ms之后，而对于HTTP/2，我们看到的是201ms，这意味着HTTP/3的性能已经提高了12.4%！
有趣的是，并不是协议的每一个方面都受草案或RFC的约束。实现选择会影响性能，例如有效的分组传输和拥塞控制算法的选择。拥塞控制是计算机和服务器用来适应过载网络的一种技术：通过丢弃数据包，传输随后会受到限制。因为QUIC是一种新的协议，要想使拥塞控制设计和实现正确，需要进行实验和调整。
为了提供安全和简单的起点，“丢失检测和拥塞控制”规范建议使用Reno算法，但允许端点选择他们可能喜欢的任何算法。 我们从New Reno开始，但我们从经验中知道，我们可以通过其他方式获得更好的性能。 我们最近已迁移到CUBIC，并且在我们的网络中，由于传输量较大且数据包丢失，CUBIC的性能比New Reno有所提高。 请继续关注，以获取更多详细信息。
对于我们现有的HTTP / 2堆栈，我们目前支持BBR v1（TCP）。 这意味着在我们的测试中，我们没有进行精确的比较，因为这些拥塞控制算法在较小传输和较大传输之间的行为会有所不同。 话虽这么说，与HTTP / 2相比，使用HTTP / 3的小型网站已经有了加速。 对于较大的区域，改进后的HTTP / 2堆栈的拥塞控制在性能上大放异彩。
对于15KB的小测试页，HTTP/3平均需要443ms来加载，而HTTP/2则需要458ms。然而，一旦我们将页面大小增加到1MB，这种优势就消失了：在我们今天的网络上，HTTP/3的速度比HTTP/2稍慢，加载速度为2.33秒，而加载速度为2.30秒。
合成基准很有趣，但是我们想知道HTTP/3在现实世界中的表现。
为了衡量，我们希望第三方可以在我们的网络上加载网站，模仿浏览器。WebPageTest是一个常用的框架，它使用漂亮的瀑布图来度量页面加载时间。为了分析后端，我们使用了 Browser Insights，以捕获我们的边缘看到的时间。然后，我们用一些自动化技术把这两部分结合在一起。
作为一个测试案例，我们决定使用这个博客来监控性能。我们配置了分布在世界各地的webgetest实例，以便通过HTTP/2和HTTP/3加载这些站点。我们还启用了HTTP/3和浏览器洞察力。因此，每当我们的测试脚本启动一个网页测试，使用支持HTTP/3的浏览器加载网页时，浏览器分析就会报告数据。冲洗并重复HTTP/2以进行比较。
下图显示了真实页面blog.cloudflare.com的页面加载时间，以比较HTTP/3和HTTP/2的性能。我们有从不同地理位置运行的这些性能度量。
如您所见，在北美，HTTP / 3性能仍落后于HTTP / 2性能，平均水平约为1-4％，在欧洲，亚洲和南美也看到了类似的结果。 我们怀疑这可能是由于拥塞算法不同所致：BBR v1上的HTTP / 2与CUBIC上的HTTP / 3不同。 将来，我们将努力在两者上支持相同的拥塞算法，以实现更准确的“苹果对苹果”比较。
结论总体而言，我们很高兴被允许推动这一标准的发展。 我们的实现效果很好，在某些情况下提供了更好的性能，并且在最坏的情况下类似于HTTP / 2。 随着标准的定稿，我们期待浏览器在主流版本中增加对HTTP / 3的支持。 对于我们而言，我们将继续支持最新的草案，同时寻找更多的方法来利用HTTP / 3获得更好的性能，无论是拥塞调整，优先级划分还是系统容量（CPU和原始网络吞吐量）。
同时，如果你想尝试一下，只需在我们的仪表板上启用HTTP/3并下载一个主要浏览器的夜间版本。关于如何启用HTTP/3的说明可以在我们的开发人员文档中找到。
附录：
原文地址：https://blog.cloudflare.com/http-3-vs-http-2/
Go QUIC库：https://github.com/lucas-clemente/quic-go</description></item><item><title>Redis6客户端缓存的相关设计</title><link>https://icorer.com/icorer_blog/posts/related-design-of-redis6-client-cache/</link><pubDate>Mon, 16 Mar 2020 13:15:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/related-design-of-redis6-client-cache/</guid><description>这篇文章翻译自Redis官方博客，这篇文章阐述了Redis6中将如何支持客户端缓存功能。
纽约Redis一天结束了，我于5:30在酒店起床，仍然与意大利时区保持同步，并立即走在曼哈顿的街道上，完全爱上了风景和美好的生活感觉。 但是我在Redis 6发行版中的感觉是，可能是最重要的功能，即新版本的Redis协议（RESP3）的采用曲线将非常缓慢，这是有充分理由的： 明智的人会在没有充分理由的情况下避免使用工具。 毕竟我为什么要这么严重地改进协议？主要有两个原因，即为客户提供更多的语义答复，并开放使用旧协议难以实现的新功能。 对我来说，最重要的功能之一就是客户端缓存。
让我们回到一年前。我来到旧金山的Redis Conf 2018，当时我坚信客户端缓存是Redis未来最重要的事情。 如果我们需要快速存储和高速缓存，那么我们需要在客户端中存储信息的子集。这是对延迟较小且规模较大的数据提供服务的想法的自然扩展。事实上，几乎所有的大公司都已经这样做了，因为这是唯一的生存之道。然而，Redis无法在此过程中协助客户。 一个幸运的巧合希望Ben Malec在Redis Conf上确切地谈论客户端缓存[1]，仅使用Redis提供的工具和许多非常聪明的想法。
[1] https://www.youtube.com/watch?v=kliQLwSikO4
本采取的方法确实打开了我的想象。 Ben为了使他的设计工作而使用了两个关键思想。首先是使用Redis Cluster的“哈希槽”概念，以将key分为16k组。这样，客户端将无需跟踪每个key的有效性，但可以将单个元数据条目用于一组key。Ben使用Pub / Sub来更改键时发送通知，因此他需要应用程序各个部分的帮助，但是该架构非常可靠。 修改key？同时发布一条使它无效的消息。 在客户端，您是否在缓存key？记住缓存每个key的时间戳，并且在接收到无效消息时，还要记住每个插槽的无效时间。 当使用给定的缓存key时，通过检查缓存的key是否具有比该key所属的插槽接收到的失效时间戳更旧的时间戳，来进行懒惰驱逐：在这种情况下，该key是陈旧数据， 必须再次询问服务器。
看完演讲之后，我意识到这是在服务器内部使用的好主意，以便允许Redis为客户端完成部分工作，并让客户端缓存更简单、更有效,所以我回家后,写了一个文档描述设计[2]。
[2] https://groups.google.com/d/msg/redis-db/xfcnYkbutDw/kTwCozpBBwAJ
但是，要使我的设计正常工作，我必须专注于将Redis协议切换到更好的协议，因此我开始编写规范，然后编写RESP3的代码，以及其他Redis 6之类的东西，例如ACL等，并且客户端缓存加入了 由于缺乏时间，我以某种方式放弃了Redis的许多构想的巨大空间。
但是我还是在纽约街头思考这个想法。 后来和会议的朋友一起去吃午餐和喝咖啡休息时间。 当我回到酒店房间时，剩下的整个晚上都是在飞机起飞前的第二天，所以我开始遵循我一年前写给小组的建议，开始编写Redis 6客户端缓存的实现。 看起来仍然很棒。
Redis服务器辅助的客户端缓存，最终称为跟踪(但我可能会改变想法)，是一个非常简单的功能，由几个关键的想法组成。
key空间被划分为“缓存槽”，但它们比Ben使用的哈希槽大得多。 我们使用CRC64输出的24位，因此有超过1600万个不同的插槽。为什么这么多?因为我认为您希望有一个拥有1亿key的服务器，而一条无效消息应该只影响客户端缓存中的几个key。Redis中无效表的内存开销是130mb:一个8字节的数组，指向16M个条目。这对我来说是可以的，如果你想要这个功能，你就要充分利用你在客户端的所有内存，所以使用130MB的服务器端是可以的;您所赢得的是一个更细粒度的失效。
客户端通过简单的命令以opt方式启用该特性：
1 CLIENT TRACKING on 服务器会回复旧的+ OK，从那一刻开始，命令表中标记为“只读”的每个命令不仅会把键返回给调用者，而且还会产生副作用 客户端到目前为止请求的所有键的缓存插槽（但只有使用只读命令的键才是，这是服务器与客户端之间的协议）。Redis存储此信息的方法很简单。每个Redis客户端都有一个唯一的ID，因此，如果客户端ID 123执行有关将key散列到插槽1、2和5的MGET，我们将获得带有以下条目的无效表：
11 -&amp;gt; [123] 22 -&amp;gt; [123] 35 -&amp;gt; [123] 但是稍后客户端ID 444也会询问插槽5中的key，因此该表将如下所示：
15 -&amp;gt; [123, 444] 现在，其他一些客户端更改了插槽5中的某些key。发生的事情是Redis将检查Invalidation Table，以发现客户端123和444都可能在该插槽上缓存了key。我们将向这两个客户端发送无效消息，因此他们可以自由地以任何形式处理该消息：要么记住上一次插槽无效的时间戳记，然后以懒惰的方式检查时间戳记（或者 如果您更喜欢此渐进式“时期”：它比较安全），然后根据比较结果将其逐出。否则，客户端可以通过获取其在此特定插槽中缓存的内容的表来直接直接回收对象。这种具有24位哈希函数的方法不是问题，因为即使缓存了数千万个key，我们也不会有很长的列表。发送无效消息后，我们可以从无效表中删除条目，这样，我们将不再向这些客户端发送无效消息，直到它们不再读取该插槽的key为止。
请注意，客户端不必真正使用hash函数的所有24位。例如，他们可能只使用20位，然后也会转移Redis发送给他们的无效消息槽。不确定这样做是否有很多好的理由，但在内存受限的系统中可能是一个想法。
如果您严格按照我所说的进行操作，您会认为相同的连接同时接收到正常的客户端响应和无效消息。对于RESP3，这是可能的，因为无效消息是作为“推送”消息类型发送的。 但是，如果客户端是阻塞客户端，而不是事件驱动的客户端，则这将变得很复杂：应用程序需要某种方式不时读取新数据，并且看起来复杂而脆弱。 在这种情况下，最好使用另一个应用程序线程和另一个客户端连接，以便接收无效消息。 因此，您可以执行以下操作：</description></item><item><title>Redis Client Side Cache - Redis客户端缓存 - RedisConf18</title><link>https://icorer.com/icorer_blog/posts/redis-client-side-cache-redis-client-side-cache-redisconf18/</link><pubDate>Sun, 15 Mar 2020 16:22:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/redis-client-side-cache-redis-client-side-cache-redisconf18/</guid><description>一. 背景描述客户端缓存是一个有意思的话题，它不是空穴来风的技术，在最新的Redis RC版本已经正式开始着手CSC方案的设计，虽然目前版本的CSC还不能真正的商用，但是市面上也有一些其他公司开始着手试探CSC相关方案的设计与实现。
目标比较有名的模型是两种：
Ben Malec paylocity公司方案 Redis6 RC方案 这两种方案并不是独立的，他们各有各的优势，paylocity公司的方案被redis团队所赞赏，并吸收了一些思路进入Redis RC版本中，Redis RC版本主要是提供了一些server端的协助，但是本质上还是没有完整的CSC方案。
二. RedisConf2018大会 Ben Malec分享这里，我们将阐述RedisConf 2018年的经典分享，这个分享围绕CSC机制的相关设计与实现，并且方案已经被广泛使用在paylocity公司，有很高的的借鉴意义。 Ben Malec的分享主要围绕如何实现一个和Redis缓存同步的本地内存缓存。
首先，我们看一下简单的网站模型，模型图如下：
接着，Ben提出很重要的缓存象限，缓存象限图如下所示：
缓存最好的应用场景就是针对更改少、请求频繁的数据读写场景。
客户端缓存，首先需要面对的问题就是 “缓存数据滞后”
这部分演讲，Ben发散思维了所有Web服务器尝使用的“文件系统观察”功能。
随后，客户端缓存会出现“跨服务器缓存数据不一致”问题。
这种问题并不是只会在不同的机器间出现，还会在同一台机器不同进程中出现。
比如在两台机器针对缓存都设置了相同的TTL生命期，但是由于机器间时间可能不同步，从而造成缓存不一致情况。更坏的情况就是，数据已经更新了，但是客户端缓存没办法及时更新，造成用户请求到旧的数据，如果再多台机器负载的情况下，极有可能出现一会新值、一会旧值得问题，这种飘忽不定的缓存返回会造成用户较差的使用体验。
接下来，Ben提出一个很重要的时间观点，服务器间想在大约相同的时间内更新相关的key，这个大约相同的时间证明这个缓存方案并不一定能够满足分布式强一致，只是在合理的时间范围内数据一致。
接下来，Ben提出第三个缓存问题，“缓存踩踏”问题
这里所说的就是如果自己完全制作一个进程内缓存，有很多需要考虑，比如启动数据加载，数据池的备份，服务器扩容过程，等等问题。
Redis可以提供简单的缓存解决方案。
Redis缓存可以很好地解决缓存一致性问题，也可以解决缓存数据滞后问题，也不会有数据践踏。
但是redis也有一些其他问题，比如每次缓存获取都需要tcp往返通信，虽然redis已经很快了，但是本地内存的访问速度仍然比网络io速度高太多。
这里，Ben提出如果在redis基础上，再增加进程内缓存，效果就会更好了。
针对这种本地缓存方案，首先提出了三个需要做的事情：
解决数据一致性问题 解决数据滞后问题，主要围绕进程内缓存和远程redis之间的滞后问题 不要让网络爆炸，要控制合理的网络通信 借助redis，我们是不是可以更好的实现这个功能呢？
上面这部分讲述了一个问题，如果我们想让机器间的数据保证一致性，如果仅仅通过广播变更的key-value，这将是致命的，因为大量的key-value将引爆网络，还有一个原因就是你广播了key-value数据，并不是所有的节点以后都会使用，这就会造成效率问题，这些问题几乎都是围绕网络，但是还没考虑网络的质量问题，比如网络质量很差的情况下，节点可能收到多组不同的改动，这些改动可能会数据践踏，但是你不知道践踏的顺序，从而造成数据的不一致问题。
因此，我们并不是广播key-value，而是只广播key，但是你也知道redis支持key数据，最大可以达到512MB，就算不是512MB，就算是1kb的数据，我们的网络就能抗住吗，所以简单的广播key是不理智的。
redis集群中采用hash槽位来进行数据分片，那么我们是否可以借鉴这种思路呢？我们不再广播key，而是广播key所计算的hash值，这样如果key的数据多么大，我们都能控制在网络上传输的数据大小。
我们放弃了广播key，而选择同步16bit的key hash槽数据，这样操作的优势明显，首先广播数据的大小被控制了，并且解决了数据一致性问题，我们只是广播hash，并没有广播数据，当某个hash出现了脏数据，它将会在下次访问时被感知并被更新。这个也有一点缺陷需要注意，因为我们借助了hash槽位，所以一个hash slot上会包含很多key，这些key中的一个被更新，则这组hash slot都将失效。
计算遍历所有的 key 吗？命中脏 slots 的话，就删除这个key？但是这样的话相当于对每一个缓存更新操作，客户端都要遍历计算一遍自己所有 key 的 slot，显然是不可接受的。
这里也是采用惰性计算的思想：客户端收到了 slot 更新的广播，只把 slot 存起来，当真正用到在此 slot 中的 key 的时候才去 Redis 更新。那么就会有这样一种情况，slot 中部分 key 更新了，部分 key 没有更新，如何区分开哪些 key 已经在 slot 更新之后更新过了呢？这里只要记一下 slot 更新的 timestamp 就可以，每一个 key-value 也带有一个 timestamp 属性。如果 key 的 timestamp 早于 slot 的 timestamp，那 key 就是需要更新的；更新之后 key 的 timestamp 就晚于 slot 的 timestamp 了。下次可以直接用。</description></item><item><title>Linux内核-内存管理: Out Of Memory Management 源码分析</title><link>https://icorer.com/icorer_blog/posts/linux-kernel-memory-management-out-of-memory-management-source-code-analysis/</link><pubDate>Sun, 08 Mar 2020 11:45:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/linux-kernel-memory-management-out-of-memory-management-source-code-analysis/</guid><description>我们这篇文章中描述了Linux内核对于 Out Of Memory Management 场景下的相关策略，接下来我们将进行Linux 5.0内核的OOM内核源码分析。
一. 关键数据结构针对源码部分，我们首先需要阐述oom_kill部分的核心数据结构。
文件路径：/linux/include/linux/oom.h
1.1 oom_control 结构体首先，我们给出具体的内核定义：
1/* 2 * Details of the page allocation that triggered the oom killer that are used to 3 * determine what should be killed. 4 */ 5struct oom_control { 6 /* Used to determine cpuset */ 7 struct zonelist *zonelist; 8 9 /* Used to determine mempolicy */ 10 nodemask_t *nodemask; 11 12 /* Memory cgroup in which oom is invoked, or NULL for global oom */ 13 struct mem_cgroup *memcg; 14 15 /* Used to determine cpuset and node locality requirement */ 16 const gfp_t gfp_mask; 17 18 /* 19 * order == -1 means the oom kill is required by sysrq, otherwise only 20 * for display purposes.</description></item><item><title>Linux内核-内存管理: Out Of Memory Management - OOM</title><link>https://icorer.com/icorer_blog/posts/linux-kernel-memory-management-out-of-memory-management-oom/</link><pubDate>Sat, 07 Mar 2020 11:49:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/linux-kernel-memory-management-out-of-memory-management-oom/</guid><description>一. 引言这篇文章，我们将要讨论的是内存不足(OOM)管理器，OOM检查是否有足够的可用内存来满足系统运行需求，如果没有足够的可用内存则进行进程kill操作。这是Linux内核虚拟内存模块中一个有争议的部分，有人建议在很多情况下删除它。所以在使用OOM的时候，首先需要确认OOM是否在待操作的Linux内核中存在，还需要确定OOM在Linux内核中的开启与关闭选项。
二. 检查可用内存（Checking Available Memory）对于某些操作，例如使用brk()扩展堆或使用mremap()重新映射地址空间，系统将检查是否有足够的可用内存来满足请求。请注意，这与当前文章后面介绍的out_of_memory()是不同部分，Linux内核的内存分配机制和内存可用度检查用于尽可能避免系统处于OOM状态。
检查可用内存时，所需的页数作为参数传递给vm_enough_memory()。除非系统管理员指定系统应超量使用内存，否则将检查可用内存的装载。为了确定有多少页面是可用的，Linux总结了以下数据位：
Total page cache (总页面缓存)：页面缓存很容易回收。 Total free pages (总空闲页面)：总可用页面，它们已经可用。 Total free swap pages(总自由交换页) ：用户空间页面可能会被换出。 Total pages managed by swapper_space(由swapper_space管理的总页面)： 尽管这将重复计算空闲交换页面。这是平衡某些情况，此选项有时保留但不使用。 Total pages used by the dentry cache(dentry缓存使用的总页面)： 这部分内存很容易被回收，主要用于vfs Total pages used by the inode cache (inode缓存使用的总页面)： 这部分内存很容易被回收，主要用于vfs-inode索引 如果在此处添加的页面总数足以满足请求，则vm_enough_memory() 将true返回给调用方。 如果返回false，则调用者知道该内存不可用，通常决定将-ENOMEM返回给用户空间。
三. 确定OOM状态（Determining OOM Status）当机器内存不足时，将回收旧的页面框架，但是尽管回收了页面，但仍可能发现即使以最高优先级进行扫描，也无法释放足够的页面来满足请求。如果无法释放页帧，则会调用out_of_memory() 以查看系统是否内存不足，是否需要终止进程。 不幸的是，系统可能没有内存不足，只需要等待IO完成或页面交换到外部存储。不幸的是，这不是因为系统具有内存，而是因为这个函数被不必要地调用，导致不必要地关闭进程。在决定终止一个进程之前，它要经过以下检查表。
是否还有足够的交换空间(nr_swap_pages&amp;gt; 0) ？如果是，则不进行OOM 从上次失败到现在已经超过5秒了吗?如果是，则不进行OOM 我们在最后一秒失败了吗?如果没有，则不进行OOM 如果在过去5秒内没有10次失败，就不进行OOM 最近5秒钟内进程是否被杀死？ 如果是，则不进行OOM 四. 选择一个进程（Selecting a Process）函数select_bad_process()负责选择要终止的进程。它通过逐步执行每个正在运行的任务并计算使用badness()函数杀死它的适合程度来做出决定。坏度的计算方法如下，请注意，平方根是使用int_sqrt()计算的整数近似值。
1badness_for_task = total_vm_for_task / (sqrt(cpu_time_in_seconds) * 2sqrt(sqrt(cpu_time_in_minutes))) 这个公式是为了选择一个使用大量内存但寿命不是很长的进程。已经运行了很长时间的进程不太可能是导致内存不足的原因，因此这个计算可能会选择一个使用了大量内存但没有运行很长时间的进程。如果该进程是根进程或具有CAP_SYS_ADMIN功能，则将坏度值除以4，因为假定根特权进程表现良好。类似地，如果它具有CAP_SYS_RAWIO功能(访问原始设备)特权，则坏度值进一步除以4，因为不希望杀死一个直接访问硬件的进程。</description></item><item><title>PHP-rdkafka 内核扩展相关源码分析</title><link>https://icorer.com/icorer_blog/posts/php-rdkafka-kernel-extension-related-source-code-analysis/</link><pubDate>Fri, 06 Mar 2020 12:41:18 +0800</pubDate><guid>https://icorer.com/icorer_blog/posts/php-rdkafka-kernel-extension-related-source-code-analysis/</guid><description>这篇文章主要针对PHP生态的的kafka组件 php-rdkafka 进行相关的内核源码分析，方便大家把握组件的相关使用，目前文章主要针对kafka生产者部分。
一. 样例PHP代码 1public function __construct($config) 2 { 3 $conf = new \RdKafka\Conf(); 4 $conf-&amp;gt;set(&amp;#39;metadata.broker.list&amp;#39;, $config[&amp;#39;brokerList&amp;#39;]); 5 $conf-&amp;gt;set(&amp;#39;message.max.bytes&amp;#39;, $config[&amp;#39;messageMaxBytes&amp;#39;]); 6 $conf-&amp;gt;set(&amp;#39;metadata.request.timeout.ms&amp;#39;, $config[&amp;#39;requestTimeout&amp;#39;]); 7 $conf-&amp;gt;set(&amp;#39;session.timeout.ms&amp;#39;, $config[&amp;#39;sessionTimeout&amp;#39;]); 8 $this-&amp;gt;producer = new \RdKafka\Producer($conf); 9 $this-&amp;gt;producer-&amp;gt;addBrokers($config[&amp;#39;brokerList&amp;#39;]); 10 } 11 12 public function sendMessage($data){ 13 $result = 1; 14 $topic = $this-&amp;gt;producer-&amp;gt;newTopic($data[0][&amp;#39;topic&amp;#39;]); 15 $topic-&amp;gt;produce(RD_KAFKA_PARTITION_UA, 0, $data[0][&amp;#39;value&amp;#39;]); 16 $this-&amp;gt;producer-&amp;gt;poll(0); 17 for ($flushRetries = 0; $flushRetries &amp;lt; 10; $flushRetries++) { 18 $result = $this-&amp;gt;producer-&amp;gt;flush(10000); 19 if (RD_KAFKA_RESP_ERR_NO_ERROR === $result) { 20 break; 21 } 22 } 23 if (RD_KAFKA_RESP_ERR_NO_ERROR !</description></item><item><title>Go的垃圾收集者之旅 [Getting to Go: The Journey of Go's Garbage Collector]</title><link>https://icorer.com/icorer_blog/posts/getting-to-go-the-journey-of-gos-garbage-collector/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://icorer.com/icorer_blog/posts/getting-to-go-the-journey-of-gos-garbage-collector/</guid><description>一 . 背景介绍这篇文章是一片演讲笔记，这是Richard L. Hudson于2018.06.18在国际内存管理研讨会(ISMM)上的演讲。
理查德·哈德森（Rick）因其在内存管理方面的工作而闻名，其中包括发明了Train，Sapphire和Mississippi Delta算法以及GC堆栈映射，这些算法能够以静态类型的语言（例如Modula-3，Java）进行垃圾收集 ，C＃和Go。 Rick目前是Google Go团队的成员，他致力于Go的垃圾回收和运行时问题。
Original URL : https://blog.golang.org/ismmkeynote
二. 演讲内容 在我们正式研究这些东西之前，我们首先需要展示一下GC在Go中看着像什么？
首先，Go程序有成千上万个堆栈，他们被Go调度器管理着并且总是在GC安全点被抢占。Go调度器将Go协程多路复用到系统线程上，希望每一个物理线程运行一个系统线程。我们通过复制堆栈和修改栈指针来管理栈及其大小。因为这些是本地操作所以很容易扩展。
接下来，我们需要讨论一个重要的内容，和传统C系统语言类似、Go也是一种“值定向”语言，而不是类似众多runtime管理型语言的“参考导向”语言。正如上面的例子展示了tar包中某一个类型如何在内存中布局存储，所有的字段均直接内嵌在Reader变量中。这使程序员可以在需要的时候更好的控制内存布局。可以把有关联值的字段进行临近分配，这样的策略有利于提高缓存的存储位置。
以值为导向有助于使用外部功能接口(有助于不同语言之间通信)，Go语言和C/C++语言能够很快的FFI (语言交互接口 ) 操作，谷歌内部有大量可用的功能、但是他们使用C++编写的。Go语言迫不及待的实现这些功能，因此Go必须使用外部功能访问接口来实现这些功能。
基于这个设计上的决定导致Go运行时必须执行一些惊人的东西，这些可能是Go和其他带有GC的计算机语言最重要的不同之处。
Go语言当然会有指针的存在，但是事实上Go甚至可以有内部指针[ interior pointer ]。这些指针可以让数据的整体具有活性，而且他们很常见。
Go语言有一套完善的预编译系统，从而一个单独的二进制运行体文件就可以包含完整的运行时环境。
运行时也不需要JIT热点重新编译，这有优点也有缺点。首先，这种模式下程序执行的可重现性要容易很多，这使得编译器改进的步伐变得更快。
可悲的是，我们没有机会像使用JITed系统那样可以反馈优化。因此，静态预编译存在上述优缺点。
Go 提供了两个旋钮用来控制GC。第一个是GCPercent，这个旋钮基本上是用来调整要使用的CPU和内存的数量，默认值为100、代表一半的堆专用于活动内存、一半的堆用来分配。当然，你可以按照你需要的比例方向就行旋钮调整。
最大堆，这个属性目前尚未发布、但已经在内部使用和评估了，这个参数允许编程人员控制最大的堆使用空间。内存不足、内存溢出(OOM)、在Go语言上很难；暂时的内存使用高峰应该通过增加CPU成本来解决，不是通过终止程序。基本上，如果GC遇到了内存压力，它应该通知应用程序应该减轻负载。当一切恢复正常之后，GC会通知应用程序让其恢复到正常负载。最大堆特性还为调度提供更多的灵活性。运行时不必总是对可用的内存量有多大的幻想，而是可以将堆的大小调整为最大堆的大小。
这结束了我们对垃圾回收器很重要的Go片段的讨论。
现在让我们来谈谈Go语言运行时以及我们如何到达这里，如何达到自己所在的位置。这句话是演讲者想表达Go运行时GC是如何一路发展的。
2014年，毫无疑问、如果Go不能以某种方式解决GC延迟问题，则Go是不会成功的。
其他新语言也会遇到同样的问题。Rust之类的语言采用了不通的解决方式，但是这里我们将讲述Go所走的道路。
为什么延迟如此的重要？
延迟是个累积量，数学对此是不能完全解释的。
99%的隔离式GC延迟服务级别目标(SLO)，例如 99% 的GC周期小于10ms，只是根本无法扩展。重要的是整个会话期间的延迟或一天中多次使用程序的延迟（这里表达的含义是：单次GC看着不重，但是无论对于单次长会话、还是长期运行的程序体，这会产生累计损害）。假设浏览一个网页的会话在一个会话中最终发出100个服务器请求，或者发出20个请求，并且一天中您有5个会话。 在这种情况下，只有37％的用户将在整个会话中获得一致的10毫秒以下体验。
正如我们所建议的那样，如果您希望这些用户中有99％的用户具有10ms以下的体验，则数学计算表明您确实需要定位4个9s或99.99％ile。
所以是2014年，杰夫·迪恩（Jeff Dean）发表了他的论文《The Tail at Scale》(规模的尾巴)，进一步探讨了这一问题。 由于它对Google的向前发展和试图以Google规模扩展产生严重影响，因此在Google周围被广泛阅读。
我们称这个问题为9s暴政。
那么，我们是如何对抗这场“暴政”的呢？
我们在2014年做了不少事情。
如果您想要10个答案，请再输入几个，然后选择前10个，这些就是您在搜索页面上输入的答案。如果请求超过50％ile，则重新发出请求或将请求转发到另一台服务器。 如果GC将要运行，请拒绝新请求或将请求转发到另一台服务器，直到完成GC。 依此类推。这段文字讲述的是类似负载均衡模式降低系统整体的响应时间。
所有这些变通办法来自非常聪明的人，他们有非常实际的问题，但他们没有解决GC延迟的根本问题。 在Google规模上，我们必须解决根本问题。 为什么？
冗余无法扩展，冗余成本很高。 它花费了新的服务器场。
我们希望能够解决这个问题，并把它看作是一个改善服务器生态系统的机会，并在这个过程中拯救一些濒临灭绝的玉米田，让一些玉米粒有机会在7月4日达到膝盖高点，让玉米更好的生长。 （这段话的意思就是，希望通过节约服务器机房成本而保护环境。）
这就是2014年的SLO，是的，的确，我在打沙袋，在团队中我是新手，这对我来说是个新过程，我不想过分承诺。(这张PPT展示了2014年的GC能力)
此外，有关其他语言的GC延迟的演讲简直令人恐惧。
最初的计划是执行无读屏障的并发复制GC。 那是长期计划。 读屏障的开销存在很多不确定性，因此Go希望避免这些屏障。
但是在2014年短期，我们必须采取行动。我们必须将所有运行时和编译器都转换为Go。它们当时是用C编写的。没有更多的C语言了，因为C语言程序员不了解GC，但是对如何复制字符串有了一个很酷的想法，因此不再有很多错误。我们需要专注与GC延迟领域的技术方案或任何东西，但是这些东西带来的性能损失必须小于编译器提供的加速。因此我们受到了限制。基本上，我们花费一年在编译器性能上的改进，可以被GC并发协程消耗完。就是这样。 我们不能放慢Go程序的速度。 这在2014年将是站不住脚的。（艰苦的2014年）</description></item></channel></rss>