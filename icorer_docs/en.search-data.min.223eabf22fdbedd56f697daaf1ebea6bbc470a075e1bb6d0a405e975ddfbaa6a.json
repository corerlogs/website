[{"id":0,"href":"/icorer_docs/doccenter/redishub/develop/icefiredb-crdt-kv/","title":"icefiredb-crdt-kv","section":"Develop","content":" icefiredb-crdt-kv # Project introduction # The IceFireDB-CRDT-KV engine can support decentralized P2P networking, data synchronization and consistency between nodes. It is a component of the IceFireDB software ecosystem, thanks to the open source of IPFS.\nFeatures # Easy access to P2P data consistency function Stable decentralized networking function Friendly program access interface Installing # go get -u github.com/IceFireDB/icefiredb-crdt-kv Example # package main import ( \u0026#34;bufio\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; icefiredb_crdt_kv \u0026#34;github.com/IceFireDB/icefiredb-crdt-kv/kv\u0026#34; badger2 \u0026#34;github.com/dgraph-io/badger\u0026#34; \u0026#34;github.com/ipfs/go-datastore/query\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) func main() { ctx := context.TODO() log := logrus.New() db, err := icefiredb_crdt_kv.NewCRDTKeyValueDB(ctx, icefiredb_crdt_kv.Config{ NodeServiceName: \u0026#34;icefiredb-crdt-kv\u0026#34;, DataSyncChannel: \u0026#34;icefiredb-crdt-kv-data\u0026#34;, NetDiscoveryChannel: \u0026#34;icefiredb-crdt-kv-net\u0026#34;, Namespace: \u0026#34;test\u0026#34;, Logger: log, }) if err != nil { panic(err) } defer db.Close() fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { text := scanner.Text() fields := strings.Fields(text) if len(fields) == 0 { fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) continue } cmd := fields[0] switch cmd { case \u0026#34;exit\u0026#34;, \u0026#34;quit\u0026#34;: return case \u0026#34;get\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } val, err := db.Get(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(string(val)) case \u0026#34;put\u0026#34;: if len(fields) \u0026lt; 3 { printVal(\u0026#34;Missing parameters\u0026#34;) continue } printVal(db.Put(ctx, []byte(fields[1]), []byte(fields[2]))) case \u0026#34;delete\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } printVal(db.Delete(ctx, []byte(fields[1]))) case \u0026#34;has\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } is, err := db.Has(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(is) case \u0026#34;list\u0026#34;: result, err := db.Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;query\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } //fmt.Println(fields[1], len(fields[1])) q := query.Query{ //Prefix: fields[1], Filters: []query.Filter{ query.FilterKeyPrefix{ Prefix: fields[1], }, }, } result, err := db.Query(ctx, q) if err != nil { printVal(err) continue } //time.Sleep(time.Second) for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;connect\u0026#34;: // 主动连接 if len(fields) \u0026lt; 2 { printVal(\u0026#34;Missing connection address\u0026#34;) continue } err = db.Connect(fields[1]) if err == nil { printVal(\u0026#34;connection succeeded!\u0026#34;) } else { printVal(err) } case \u0026#34;slist\u0026#34;: result, err := db.Store().Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;bquery\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() prefix := []byte(fields[1]) for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) case \u0026#34;blist\u0026#34;: db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() for it.Rewind(); it.Valid(); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) default: printVal(\u0026#34;\u0026#34;) } } } func printVal(v interface{}) { fmt.Printf(\u0026#34;%v\\n\u0026gt; \u0026#34;, v) } RoadMap # Optimize project structure. Encapsulates the kv engine layer for easy reference by upper-layer applications. "},{"id":1,"href":"/icorer_docs/doccenter/redistun/develop/icefiredb-crdt-kv/","title":"icefiredb-crdt-kv","section":"Develop","content":" icefiredb-crdt-kv # Project introduction # The IceFireDB-CRDT-KV engine can support decentralized P2P networking, data synchronization and consistency between nodes. It is a component of the IceFireDB software ecosystem, thanks to the open source of IPFS.\nFeatures # Easy access to P2P data consistency function Stable decentralized networking function Friendly program access interface Installing # go get -u github.com/IceFireDB/icefiredb-crdt-kv Example # package main import ( \u0026#34;bufio\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; icefiredb_crdt_kv \u0026#34;github.com/IceFireDB/icefiredb-crdt-kv/kv\u0026#34; badger2 \u0026#34;github.com/dgraph-io/badger\u0026#34; \u0026#34;github.com/ipfs/go-datastore/query\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) func main() { ctx := context.TODO() log := logrus.New() db, err := icefiredb_crdt_kv.NewCRDTKeyValueDB(ctx, icefiredb_crdt_kv.Config{ NodeServiceName: \u0026#34;icefiredb-crdt-kv\u0026#34;, DataSyncChannel: \u0026#34;icefiredb-crdt-kv-data\u0026#34;, NetDiscoveryChannel: \u0026#34;icefiredb-crdt-kv-net\u0026#34;, Namespace: \u0026#34;test\u0026#34;, Logger: log, }) if err != nil { panic(err) } defer db.Close() fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { text := scanner.Text() fields := strings.Fields(text) if len(fields) == 0 { fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) continue } cmd := fields[0] switch cmd { case \u0026#34;exit\u0026#34;, \u0026#34;quit\u0026#34;: return case \u0026#34;get\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } val, err := db.Get(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(string(val)) case \u0026#34;put\u0026#34;: if len(fields) \u0026lt; 3 { printVal(\u0026#34;Missing parameters\u0026#34;) continue } printVal(db.Put(ctx, []byte(fields[1]), []byte(fields[2]))) case \u0026#34;delete\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } printVal(db.Delete(ctx, []byte(fields[1]))) case \u0026#34;has\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } is, err := db.Has(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(is) case \u0026#34;list\u0026#34;: result, err := db.Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;query\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } //fmt.Println(fields[1], len(fields[1])) q := query.Query{ //Prefix: fields[1], Filters: []query.Filter{ query.FilterKeyPrefix{ Prefix: fields[1], }, }, } result, err := db.Query(ctx, q) if err != nil { printVal(err) continue } //time.Sleep(time.Second) for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;connect\u0026#34;: // 主动连接 if len(fields) \u0026lt; 2 { printVal(\u0026#34;Missing connection address\u0026#34;) continue } err = db.Connect(fields[1]) if err == nil { printVal(\u0026#34;connection succeeded!\u0026#34;) } else { printVal(err) } case \u0026#34;slist\u0026#34;: result, err := db.Store().Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;bquery\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() prefix := []byte(fields[1]) for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) case \u0026#34;blist\u0026#34;: db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() for it.Rewind(); it.Valid(); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) default: printVal(\u0026#34;\u0026#34;) } } } func printVal(v interface{}) { fmt.Printf(\u0026#34;%v\\n\u0026gt; \u0026#34;, v) } RoadMap # Optimize project structure. Encapsulates the kv engine layer for easy reference by upper-layer applications. "},{"id":2,"href":"/icorer_docs/doccenter/redishub/project-comparison/orbitdb/","title":"OrbitDB","section":"Project Comparison","content":" Compared with OrbitDB # OrbitDB is a serverless, distributed, peer-to-peer database.\nOrbitDB uses IPFS as its data storage and IPFS Pubsub and uses CRDTs to automatically sync databases with peers, achieving strong eventual consistency - when all updates are eventually received, all nodes will have the same state.\nIceFireDB is a database built for web3 and web2,The core mission of the project is to help applications quickly achieve decentralization,built for Data dao.\nDatabase OrbitDB IceFireDB system target P2P Databases A decentralized database platform built for Data dao. storage engine support IPFS goleveldb、badger、IPFS、CRDT、IPFS-LOG、OSS network support type P2P P2P、RAFT、NATS Data type support KV、PubSub KV、Strings、Hashes、Lists、Sorted Sets、Sets、SQL、PubSub Software integration method Software library integration Software library integration, binary software integration、web3 platform integration web3 support No smart contract plan Smart contracts are being supported、Build data dao database platform computer language used to implement Javascript Go Ecological client language Javascript Any client that supports the redis、mysql protocol Thanks OrbitDB # During the construction of IceFireDB, we learned a lot of excellent ideas from OrbitDB, and we stood on the shoulders of OrbitDB giants to move forward.\n"},{"id":3,"href":"/icorer_docs/doccenter/redistun/project-comparison/orbitdb/","title":"OrbitDB","section":"Project Comparison","content":" Compared with OrbitDB # OrbitDB is a serverless, distributed, peer-to-peer database.\nOrbitDB uses IPFS as its data storage and IPFS Pubsub and uses CRDTs to automatically sync databases with peers, achieving strong eventual consistency - when all updates are eventually received, all nodes will have the same state.\nIceFireDB is a database built for web3 and web2,The core mission of the project is to help applications quickly achieve decentralization,built for Data dao.\nDatabase OrbitDB IceFireDB system target P2P Databases A decentralized database platform built for Data dao. storage engine support IPFS goleveldb、badger、IPFS、CRDT、IPFS-LOG、OSS network support type P2P P2P、RAFT、NATS Data type support KV、PubSub KV、Strings、Hashes、Lists、Sorted Sets、Sets、SQL、PubSub Software integration method Software library integration Software library integration, binary software integration、web3 platform integration web3 support No smart contract plan Smart contracts are being supported、Build data dao database platform computer language used to implement Javascript Go Ecological client language Javascript Any client that supports the redis、mysql protocol Thanks OrbitDB # During the construction of IceFireDB, we learned a lot of excellent ideas from OrbitDB, and we stood on the shoulders of OrbitDB giants to move forward.\n"},{"id":4,"href":"/icorer_docs/doccenter/ak-2019/architecture/overview/","title":"OverView","section":"Architecture","content":" OverView # "},{"id":5,"href":"/icorer_docs/doccenter/logdarts/architecture/overview/","title":"OverView","section":"Architecture","content":" OverView # "},{"id":6,"href":"/icorer_docs/doccenter/pulseflow/architecture/overview/","title":"OverView","section":"Architecture","content":" OverView # "},{"id":7,"href":"/icorer_docs/doccenter/redishub/designs/overview/","title":"OverView","section":"Designs","content":" OverView # "},{"id":8,"href":"/icorer_docs/doccenter/redistun/designs/overview/","title":"OverView","section":"Designs","content":" OverView # "},{"id":9,"href":"/icorer_docs/doccenter/redishub/","title":"RedisHub","section":"项目总览","content":"RedisHub 是一款Redis集群中间件，帮助PHP环境平滑迁移Redis集群模式，通过中间件可以解耦业务代码和redis集群之间的关系，降低开发人员对于redis集群的理解心智。中间件实现高性能网络通信、RESP协议与Redis集群协议解析，实现集群模式下的MSET/MGET指令跨槽及并发流水吞吐，为PHP请求远程Redis集群提供更高的性能及稳定性。\n一. 总体设计 # RedisHub在组件设计上分为以下几部分:\nAgent配置解析器：负责对于配置文件进行解析（后续增加统一配置中心的支持） 通信组件: 包括UNIX本地监听组 和 远端Redis集群TCP长连接及连接池。 协议分析器: 提供稳定的Redis协议解析及组装功能组件。 协议拦截器: 主要对某些Redis命令进行拦截,调用协议插件组进行功能扩展. 协议插件组: 为协议分析器添加一系列插件,对通信进行优化和功能扩展，例如集群的mset、mget、del操作。 容灾器: 为Agent运行提供必要的安全保障,主要包括进程资源监控,迭代更新监控. 具体的组件总体架构如下图: 二. 网络代理设计 # RedisHub很重要的是网络代理部分,在网络代理方面由三部分组成.\n第一部分是对远程redis集群的连接池. 第二部分是对本地众多php-fpm客户端的UNIX请求连接管理. 第三部分是对这三端之间redis通信协议进行兼容. "},{"id":10,"href":"/icorer_docs/doccenter/redishub/develop/icefiredb-ipfs-log/","title":"icefiredb-ipfs-log","section":"Develop","content":" icefiredb-ipfs-log # Project introduction # icefiredb-ipfs-log is a distributed immutable, operation-based conflict-free replication data structure that relies on ipfs to store data and merges each peer node data based on pubsub conflict-free. You can easily implement custom data structures such as kv, event, nosql, etc. based on icefiredb-ipfs-log.\nConflict-free log replication model\nLog A Log B | | logA.append(\u0026#34;one\u0026#34;) logB.append(\u0026#34;hello\u0026#34;) | | v v +-----+ +-------+ |\u0026#34;one\u0026#34;| |\u0026#34;hello\u0026#34;| +-----+ +-------+ | | logA.append(\u0026#34;two\u0026#34;) logB.append(\u0026#34;world\u0026#34;) | | v v +-----------+ +---------------+ |\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;| |\u0026#34;hello\u0026#34;,\u0026#34;world\u0026#34;| +-----------+ +---------------+ | | | | logA.join(logB) \u0026lt;----------+ | v +---------------------------+ |\u0026#34;one\u0026#34;,\u0026#34;hello\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;world\u0026#34;| +---------------------------+ Features # Easy access to P2P \u0026amp;\u0026amp; ipfs-log data consistency function Stable decentralized networking function Friendly program access interface Installing # go get -u github.com/IceFireDB/icefiredb-ipfs-log Example # Example of building a key-value database using icefiredb-ipfs-log # memory key-value：memory-kv leveldb kv ：leveldb-kv Use of key-value databases # Detailed usage example reference\nfunc main() { ctx := context.TODO() // disk cache directory rootPath := \u0026#34;./kvdb\u0026#34; node, api, err := iflog.CreateNode(ctx, rootPath) if err != nil { panic(err) } hostAddr, _ := ma.NewMultiaddr(fmt.Sprintf(\u0026#34;/ipfs/%s\u0026#34;, node.PeerHost.ID().Pretty())) for _, a := range node.PeerHost.Addrs() { fmt.Println(a.Encapsulate(hostAddr).String()) } log := zap.NewNop() dbname := \u0026#34;iflog-event-kv\u0026#34; ev, err := iflog.NewIpfsLog(ctx, api, dbname, \u0026amp;iflog.EventOptions{ Directory: rootPath, Logger: log, }) if err != nil { panic(err) } if err := ev.AnnounceConnect(ctx, node); err != nil { panic(err) } kvdb, err := kv.NewKeyValueDB(ctx, ev, log) if err != nil { panic(err) } // Load old data from disk if err := ev.LoadDisk(ctx); err != nil { panic(err) } kvdb.Put(ctx, \u0026#34;one\u0026#34;, \u0026#34;one\u0026#34;) kvdb.Get(\u0026#34;one\u0026#34;) kvdb.Delete(ctx, \u0026#34;one\u0026#34;) } package main import ( \u0026#34;bufio\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; icefiredb_crdt_kv \u0026#34;github.com/IceFireDB/icefiredb-crdt-kv/kv\u0026#34; badger2 \u0026#34;github.com/dgraph-io/badger\u0026#34; \u0026#34;github.com/ipfs/go-datastore/query\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) func main() { ctx := context.TODO() log := logrus.New() db, err := icefiredb_crdt_kv.NewCRDTKeyValueDB(ctx, icefiredb_crdt_kv.Config{ NodeServiceName: \u0026#34;icefiredb-crdt-kv\u0026#34;, DataSyncChannel: \u0026#34;icefiredb-crdt-kv-data\u0026#34;, NetDiscoveryChannel: \u0026#34;icefiredb-crdt-kv-net\u0026#34;, Namespace: \u0026#34;test\u0026#34;, Logger: log, }) if err != nil { panic(err) } defer db.Close() fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { text := scanner.Text() fields := strings.Fields(text) if len(fields) == 0 { fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) continue } cmd := fields[0] switch cmd { case \u0026#34;exit\u0026#34;, \u0026#34;quit\u0026#34;: return case \u0026#34;get\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } val, err := db.Get(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(string(val)) case \u0026#34;put\u0026#34;: if len(fields) \u0026lt; 3 { printVal(\u0026#34;Missing parameters\u0026#34;) continue } printVal(db.Put(ctx, []byte(fields[1]), []byte(fields[2]))) case \u0026#34;delete\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } printVal(db.Delete(ctx, []byte(fields[1]))) case \u0026#34;has\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } is, err := db.Has(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(is) case \u0026#34;list\u0026#34;: result, err := db.Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;query\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } //fmt.Println(fields[1], len(fields[1])) q := query.Query{ //Prefix: fields[1], Filters: []query.Filter{ query.FilterKeyPrefix{ Prefix: fields[1], }, }, } result, err := db.Query(ctx, q) if err != nil { printVal(err) continue } //time.Sleep(time.Second) for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;connect\u0026#34;: // 主动连接 if len(fields) \u0026lt; 2 { printVal(\u0026#34;Missing connection address\u0026#34;) continue } err = db.Connect(fields[1]) if err == nil { printVal(\u0026#34;connection succeeded!\u0026#34;) } else { printVal(err) } case \u0026#34;slist\u0026#34;: result, err := db.Store().Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;bquery\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() prefix := []byte(fields[1]) for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) case \u0026#34;blist\u0026#34;: db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() for it.Rewind(); it.Valid(); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) default: printVal(\u0026#34;\u0026#34;) } } } func printVal(v interface{}) { fmt.Printf(\u0026#34;%v\\n\u0026gt; \u0026#34;, v) } Some code reference sources # go-ipfs-log License # icefiredb-ipfs-log is under the Apache 2.0 license. See the LICENSE directory for details.\n"},{"id":11,"href":"/icorer_docs/doccenter/redistun/develop/icefiredb-ipfs-log/","title":"icefiredb-ipfs-log","section":"Develop","content":" icefiredb-ipfs-log # Project introduction # icefiredb-ipfs-log is a distributed immutable, operation-based conflict-free replication data structure that relies on ipfs to store data and merges each peer node data based on pubsub conflict-free. You can easily implement custom data structures such as kv, event, nosql, etc. based on icefiredb-ipfs-log.\nConflict-free log replication model\nLog A Log B | | logA.append(\u0026#34;one\u0026#34;) logB.append(\u0026#34;hello\u0026#34;) | | v v +-----+ +-------+ |\u0026#34;one\u0026#34;| |\u0026#34;hello\u0026#34;| +-----+ +-------+ | | logA.append(\u0026#34;two\u0026#34;) logB.append(\u0026#34;world\u0026#34;) | | v v +-----------+ +---------------+ |\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;| |\u0026#34;hello\u0026#34;,\u0026#34;world\u0026#34;| +-----------+ +---------------+ | | | | logA.join(logB) \u0026lt;----------+ | v +---------------------------+ |\u0026#34;one\u0026#34;,\u0026#34;hello\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;world\u0026#34;| +---------------------------+ Features # Easy access to P2P \u0026amp;\u0026amp; ipfs-log data consistency function Stable decentralized networking function Friendly program access interface Installing # go get -u github.com/IceFireDB/icefiredb-ipfs-log Example # Example of building a key-value database using icefiredb-ipfs-log # memory key-value：memory-kv leveldb kv ：leveldb-kv Use of key-value databases # Detailed usage example reference\nfunc main() { ctx := context.TODO() // disk cache directory rootPath := \u0026#34;./kvdb\u0026#34; node, api, err := iflog.CreateNode(ctx, rootPath) if err != nil { panic(err) } hostAddr, _ := ma.NewMultiaddr(fmt.Sprintf(\u0026#34;/ipfs/%s\u0026#34;, node.PeerHost.ID().Pretty())) for _, a := range node.PeerHost.Addrs() { fmt.Println(a.Encapsulate(hostAddr).String()) } log := zap.NewNop() dbname := \u0026#34;iflog-event-kv\u0026#34; ev, err := iflog.NewIpfsLog(ctx, api, dbname, \u0026amp;iflog.EventOptions{ Directory: rootPath, Logger: log, }) if err != nil { panic(err) } if err := ev.AnnounceConnect(ctx, node); err != nil { panic(err) } kvdb, err := kv.NewKeyValueDB(ctx, ev, log) if err != nil { panic(err) } // Load old data from disk if err := ev.LoadDisk(ctx); err != nil { panic(err) } kvdb.Put(ctx, \u0026#34;one\u0026#34;, \u0026#34;one\u0026#34;) kvdb.Get(\u0026#34;one\u0026#34;) kvdb.Delete(ctx, \u0026#34;one\u0026#34;) } package main import ( \u0026#34;bufio\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; icefiredb_crdt_kv \u0026#34;github.com/IceFireDB/icefiredb-crdt-kv/kv\u0026#34; badger2 \u0026#34;github.com/dgraph-io/badger\u0026#34; \u0026#34;github.com/ipfs/go-datastore/query\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) func main() { ctx := context.TODO() log := logrus.New() db, err := icefiredb_crdt_kv.NewCRDTKeyValueDB(ctx, icefiredb_crdt_kv.Config{ NodeServiceName: \u0026#34;icefiredb-crdt-kv\u0026#34;, DataSyncChannel: \u0026#34;icefiredb-crdt-kv-data\u0026#34;, NetDiscoveryChannel: \u0026#34;icefiredb-crdt-kv-net\u0026#34;, Namespace: \u0026#34;test\u0026#34;, Logger: log, }) if err != nil { panic(err) } defer db.Close() fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { text := scanner.Text() fields := strings.Fields(text) if len(fields) == 0 { fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) continue } cmd := fields[0] switch cmd { case \u0026#34;exit\u0026#34;, \u0026#34;quit\u0026#34;: return case \u0026#34;get\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } val, err := db.Get(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(string(val)) case \u0026#34;put\u0026#34;: if len(fields) \u0026lt; 3 { printVal(\u0026#34;Missing parameters\u0026#34;) continue } printVal(db.Put(ctx, []byte(fields[1]), []byte(fields[2]))) case \u0026#34;delete\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } printVal(db.Delete(ctx, []byte(fields[1]))) case \u0026#34;has\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } is, err := db.Has(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(is) case \u0026#34;list\u0026#34;: result, err := db.Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;query\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } //fmt.Println(fields[1], len(fields[1])) q := query.Query{ //Prefix: fields[1], Filters: []query.Filter{ query.FilterKeyPrefix{ Prefix: fields[1], }, }, } result, err := db.Query(ctx, q) if err != nil { printVal(err) continue } //time.Sleep(time.Second) for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;connect\u0026#34;: // 主动连接 if len(fields) \u0026lt; 2 { printVal(\u0026#34;Missing connection address\u0026#34;) continue } err = db.Connect(fields[1]) if err == nil { printVal(\u0026#34;connection succeeded!\u0026#34;) } else { printVal(err) } case \u0026#34;slist\u0026#34;: result, err := db.Store().Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;bquery\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() prefix := []byte(fields[1]) for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) case \u0026#34;blist\u0026#34;: db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() for it.Rewind(); it.Valid(); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) default: printVal(\u0026#34;\u0026#34;) } } } func printVal(v interface{}) { fmt.Printf(\u0026#34;%v\\n\u0026gt; \u0026#34;, v) } Some code reference sources # go-ipfs-log License # icefiredb-ipfs-log is under the Apache 2.0 license. See the LICENSE directory for details.\n"},{"id":12,"href":"/icorer_docs/doccenter/ak-2019/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":13,"href":"/icorer_docs/doccenter/logdarts/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":14,"href":"/icorer_docs/doccenter/pulseflow/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":15,"href":"/icorer_docs/doccenter/redishub/designs/network/","title":"Network layer","section":"Designs","content":" Network layer design # The network layer undertakes the work of inter-node networking, inter-node data distribution, and inter-node data consistency consensus. The network layer of IceFireDB is divided into two layers according to the distance of the physical network link:\nData consistency network layer for short-distance networks. Decentralized database network layer for wide-distance network. The above two different network layers are supported by different technologies and have different requirements for data consistency sensitivity and timeliness.\nParallel cluster network # The smallest component unit of the IceFireDB network cluster is a parallel cluster network guaranteed by the RAFT algorithm, rather than a decentralized P2P network.\nThe IceFireDB parallel cluster uses the RAFT algorithm to form a data consistency layer. Each cluster has a master and multiple slave nodes. They synchronize with each other and maintain the master-slave state of the cluster. Each node stores the same data and writes data. The input is undertaken by the master node, and the master node is responsible for distributing data to the slave nodes. A RAFT cluster internally guarantees the consistency of each data write.\nDecentralized network # The data jumps out of the parallel cluster network and enters the decentralized network. The IceFireDB decentralized network mainly uses P2P technology for automatic networking, and relies on the P2P PubSub communication method to synchronize data commands.\nIceFireDB\u0026rsquo;s use of P2P networking is not only in node discovery, but also provides users with decentralized publish and subscribe middleware by bridging the PubSub network to the RESP protocol. Increase the decentralization capability of the SQLite database by bridging the PubSub network to the SQL protocol.\nDecentralized log synchronization network # Relying solely on P2P and PubSub technologies cannot meet the database requirements for data decentralized synchronization and decentralized data consistency.The current popular CRDT technology can meet the final data consistency, but it cannot guarantee the synchronization order of the data, so it will castrate some data instruction functions of IceFireDB-NoSQL, and the IPFS-LOG technology very well makes up for the functional gap of the decentralized log.\nIceFireDB-NoSQL has established a decentralized log synchronization network based on IPFS-LOG technology. The decentralized IceFireDB nodes broadcast database command logs and build orderly logs to complete the function of data decentralization and consistency.\nIceGiant Network structure # IceFireDB\u0026rsquo;s RAFT has exactly the same data set within the same group of nodes, and in a larger network, we are providing the IceGiant network structure, which aims to break up different tenant database data.\nAlthough all IceGiant nodes are in the same P2P network, the structure of the network can be decomposed according to the data set, which is the data tenant isolation area, which refers to the database area used by a specific application on top of the IceGiant protocol. Different from the traditional blockchain network, IceGiant nodes are only responsible for interacting with peers operating the same data set, and are not responsible for any data of other data sets. Peer IceGiant node sets form a high-availability data storage area.\nIn-process network IO model # IceFireDB supports the following two IO models:\nGolang classic netpoll model: goroutine-per-connection, suitable for situations where the number of connections is not a bottleneck.\nRawEpoll model: that is, Reactor mode, I/O multiplexing (I/O multiplexing) + non-blocking I/O (non-blocking I/O) mode. For scenarios where there are a large number of long links between the access layer and the gateway, it is more suitable for the RawEpoll model.\n"},{"id":16,"href":"/icorer_docs/doccenter/redistun/designs/network/","title":"Network layer","section":"Designs","content":" Network layer design # The network layer undertakes the work of inter-node networking, inter-node data distribution, and inter-node data consistency consensus. The network layer of IceFireDB is divided into two layers according to the distance of the physical network link:\nData consistency network layer for short-distance networks. Decentralized database network layer for wide-distance network. The above two different network layers are supported by different technologies and have different requirements for data consistency sensitivity and timeliness.\nParallel cluster network # The smallest component unit of the IceFireDB network cluster is a parallel cluster network guaranteed by the RAFT algorithm, rather than a decentralized P2P network.\nThe IceFireDB parallel cluster uses the RAFT algorithm to form a data consistency layer. Each cluster has a master and multiple slave nodes. They synchronize with each other and maintain the master-slave state of the cluster. Each node stores the same data and writes data. The input is undertaken by the master node, and the master node is responsible for distributing data to the slave nodes. A RAFT cluster internally guarantees the consistency of each data write.\nDecentralized network # The data jumps out of the parallel cluster network and enters the decentralized network. The IceFireDB decentralized network mainly uses P2P technology for automatic networking, and relies on the P2P PubSub communication method to synchronize data commands.\nIceFireDB\u0026rsquo;s use of P2P networking is not only in node discovery, but also provides users with decentralized publish and subscribe middleware by bridging the PubSub network to the RESP protocol. Increase the decentralization capability of the SQLite database by bridging the PubSub network to the SQL protocol.\nDecentralized log synchronization network # Relying solely on P2P and PubSub technologies cannot meet the database requirements for data decentralized synchronization and decentralized data consistency.The current popular CRDT technology can meet the final data consistency, but it cannot guarantee the synchronization order of the data, so it will castrate some data instruction functions of IceFireDB-NoSQL, and the IPFS-LOG technology very well makes up for the functional gap of the decentralized log.\nIceFireDB-NoSQL has established a decentralized log synchronization network based on IPFS-LOG technology. The decentralized IceFireDB nodes broadcast database command logs and build orderly logs to complete the function of data decentralization and consistency.\nIceGiant Network structure # IceFireDB\u0026rsquo;s RAFT has exactly the same data set within the same group of nodes, and in a larger network, we are providing the IceGiant network structure, which aims to break up different tenant database data.\nAlthough all IceGiant nodes are in the same P2P network, the structure of the network can be decomposed according to the data set, which is the data tenant isolation area, which refers to the database area used by a specific application on top of the IceGiant protocol. Different from the traditional blockchain network, IceGiant nodes are only responsible for interacting with peers operating the same data set, and are not responsible for any data of other data sets. Peer IceGiant node sets form a high-availability data storage area.\nIn-process network IO model # IceFireDB supports the following two IO models:\nGolang classic netpoll model: goroutine-per-connection, suitable for situations where the number of connections is not a bottleneck.\nRawEpoll model: that is, Reactor mode, I/O multiplexing (I/O multiplexing) + non-blocking I/O (non-blocking I/O) mode. For scenarios where there are a large number of long links between the access layer and the gateway, it is more suitable for the RawEpoll model.\n"},{"id":17,"href":"/icorer_docs/doccenter/redishub/project-comparison/threaddb/","title":"ThreadDB","section":"Project Comparison","content":" Compared with ThreadDB # ThreadDB is a serverless, distributed, peer-to-peer database.\nIceFireDB is a database built for web3 and web2,The core mission of the project is to help applications quickly achieve decentralization,built for Data dao.\nDatabase ThreadDB IceFireDB system target P2P Databases A decentralized database platform built for Data dao. storage engine support IPFS goleveldb、badger、IPFS、CRDT、IPFS-LOG、OSS network support type P2P P2P、RAFT、NATS Data type support SQL KV、Strings、Hashes、Lists、Sorted Sets、Sets、SQL、PubSub Software integration method Binary software integration Software library integration, binary software integration、web3 platform integration web3 support No smart contract plan Smart contracts are being supported、Build data dao database platform computer language used to implement Go Go Ecological client language Go Any client that supports the redis、mysql protocol Thanks ThreadDB # Thanks to ThreadDB for letting us see the excellent implementation of decentralized SQL database.\n"},{"id":18,"href":"/icorer_docs/doccenter/redistun/project-comparison/threaddb/","title":"ThreadDB","section":"Project Comparison","content":" Compared with ThreadDB # ThreadDB is a serverless, distributed, peer-to-peer database.\nIceFireDB is a database built for web3 and web2,The core mission of the project is to help applications quickly achieve decentralization,built for Data dao.\nDatabase ThreadDB IceFireDB system target P2P Databases A decentralized database platform built for Data dao. storage engine support IPFS goleveldb、badger、IPFS、CRDT、IPFS-LOG、OSS network support type P2P P2P、RAFT、NATS Data type support SQL KV、Strings、Hashes、Lists、Sorted Sets、Sets、SQL、PubSub Software integration method Binary software integration Software library integration, binary software integration、web3 platform integration web3 support No smart contract plan Smart contracts are being supported、Build data dao database platform computer language used to implement Go Go Ecological client language Go Any client that supports the redis、mysql protocol Thanks ThreadDB # Thanks to ThreadDB for letting us see the excellent implementation of decentralized SQL database.\n"},{"id":19,"href":"/icorer_docs/doccenter/redishub/develop/icefiredb_proxy/","title":"icefiredb-proxy","section":"Develop","content":" IceFireDB-Proxy # Project introduction # IceFireDB-Proxy is a high-performance, high-availability, and user-friendly Resp protocol cluster proxy solution. It is supporting P2P networking and is a network component in the IceFireDB ecosystem.\nFeatures # Complete data source mode support: stand-alone, cluster mode Rich command support Excellent cluster state management and failover Excellent traffic control policies: Traffic read/write separation and multi-tenant data isolation Excellent command telemetry features Bottom-fishing use of mind and base abilities that are closer to cloud native Supports P2P automatic networking, and Proxy helps traditional Redis databases achieve data decentralization. New framework for faster network, will be upgraded soon. redhub Architecture # Network Communication Model # Installing # 1. Install Go 2. git clone https://github.com/IceFireDB/IceFireDB-Proxy.git $GOPATH/src/github.com/IceFireDB/IceFireDB-Proxy 3. cd $GOPATH/src/github.com/IceFireDB/IceFireDB-Proxy 4. make Usage # Run a binary file directly, if you need to run in the background can be added to the systemd system management\n./bin/Icefiredb-proxy -c ./config/config.yaml Command support # String # APPEND BITCOUNT BITPOS DECR DECRBY DEL EXISTS GET GETBIT SETBIT GETRANGE GETSET INCR INCRBY MGET MSET SET SETEX SETEXAT SETRANGE EXPIRE EXPIREAT TTL Set # SADD SCARD SETBIT SISMEMBER SMEMBERS SPOP SRANDMEMBER SREM SSCAN List # LINDEX LINSERT LLEN LPOP LPUSH LPUSHX LRANGE LREM LSET LTRIM RPOP RPUSH RPUSHX Hash # HDEL HEXISTS HGET HGETALL HINCRBY HINCRBYFLOAT HKEYS HLEN HMGET HMSET HSCAN HSET HSETNX HSTRLEN HVALS Sorted Sets # ZADD ZCARD ZCOUNT ZINCRBY ZLEXCOUNT ZPOPMAX ZPOPMIN ZLEXCOUNT ZRANGE ZRANGEBYLEX ZRANGEBYSCORE ZRANK ZREM ZREMRANGEBYLEX ZREMRANGEBYRANK ZREMRANGEBYSCORE ZREVRANGE ZREVRANGEBYLEX ZREVRANGEBYSCORE ZREVRANK ZSCAN ZSCORE Stream # XACK XADD XCLAIM XDEL XLEN XINFO XPENDING XRANGE XREADGROUP XREVRANGE XTRIM XGROUP Others # COMMAND PING QUIT "},{"id":20,"href":"/icorer_docs/doccenter/redistun/develop/icefiredb_proxy/","title":"icefiredb-proxy","section":"Develop","content":" IceFireDB-Proxy # Project introduction # IceFireDB-Proxy is a high-performance, high-availability, and user-friendly Resp protocol cluster proxy solution. It is supporting P2P networking and is a network component in the IceFireDB ecosystem.\nFeatures # Complete data source mode support: stand-alone, cluster mode Rich command support Excellent cluster state management and failover Excellent traffic control policies: Traffic read/write separation and multi-tenant data isolation Excellent command telemetry features Bottom-fishing use of mind and base abilities that are closer to cloud native Supports P2P automatic networking, and Proxy helps traditional Redis databases achieve data decentralization. New framework for faster network, will be upgraded soon. redhub Architecture # Network Communication Model # Installing # 1. Install Go 2. git clone https://github.com/IceFireDB/IceFireDB-Proxy.git $GOPATH/src/github.com/IceFireDB/IceFireDB-Proxy 3. cd $GOPATH/src/github.com/IceFireDB/IceFireDB-Proxy 4. make Usage # Run a binary file directly, if you need to run in the background can be added to the systemd system management\n./bin/Icefiredb-proxy -c ./config/config.yaml Command support # String # APPEND BITCOUNT BITPOS DECR DECRBY DEL EXISTS GET GETBIT SETBIT GETRANGE GETSET INCR INCRBY MGET MSET SET SETEX SETEXAT SETRANGE EXPIRE EXPIREAT TTL Set # SADD SCARD SETBIT SISMEMBER SMEMBERS SPOP SRANDMEMBER SREM SSCAN List # LINDEX LINSERT LLEN LPOP LPUSH LPUSHX LRANGE LREM LSET LTRIM RPOP RPUSH RPUSHX Hash # HDEL HEXISTS HGET HGETALL HINCRBY HINCRBYFLOAT HKEYS HLEN HMGET HMSET HSCAN HSET HSETNX HSTRLEN HVALS Sorted Sets # ZADD ZCARD ZCOUNT ZINCRBY ZLEXCOUNT ZPOPMAX ZPOPMIN ZLEXCOUNT ZRANGE ZRANGEBYLEX ZRANGEBYSCORE ZRANK ZREM ZREMRANGEBYLEX ZREMRANGEBYRANK ZREMRANGEBYSCORE ZREVRANGE ZREVRANGEBYLEX ZREVRANGEBYSCORE ZREVRANK ZSCAN ZSCORE Stream # XACK XADD XCLAIM XDEL XLEN XINFO XPENDING XRANGE XREADGROUP XREVRANGE XTRIM XGROUP Others # COMMAND PING QUIT "},{"id":21,"href":"/icorer_docs/doccenter/ak-2019/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":22,"href":"/icorer_docs/doccenter/logdarts/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":23,"href":"/icorer_docs/doccenter/pulseflow/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":24,"href":"/icorer_docs/doccenter/redishub/designs/storage/","title":"Storage layer","section":"Designs","content":" Storage layer design # The storage layer is responsible for data storage, and the data storage here includes different storage media of web2 and web3. For web2, the storage media we face includes disk, OSS, and for web3, the storage media we face includes IPFS, blockchain, and smart contracts.Currently, the storage types supported by IceFireDB mainly include the following.\nEngine type describe Driver directory LevelDB LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values. Default Badger BadgerDB is an embeddable, persistent and fast key-value (KV) database written in pure Go. Badger OSS Object storage is a technology that stores and manages data in an unstructured format called objects. OSS IPFS IPFS (the InterPlanetary File System) is a hypermedia distribution protocol addressed by content and identities. It enables the creation of completely distributed applications, and in doing so aims to make the web faster, safer, and more open. IPFS CRDT-KV The IceFireDB-CRDT-KV engine can support decentralized P2P networking, data synchronization and consistency between nodes. It is a component of the IceFireDB software ecosystem, thanks to the open source of IPFS. CRDT-KV IPFS-LOG icefiredb-ipfs-log is a distributed immutable, operation-based conflict-free replication data structure that relies on ipfs to store data and merges each peer node data based on pubsub conflict-free. You can easily implement custom data structures such as kv, event, nosql, etc. based on icefiredb-ipfs-log. IPFS-LOG OrbitDB OrbitDB is a serverless, distributed, peer-to-peer database. OrbitDB uses IPFS as its data storage and IPFS Pubsub to automatically sync databases with peers. OrbitDB Storage model # The NoSQL storage layer of each individual IceGiant mainly includes the codec layer and the underlying KV storage layer. the underlying KV engine currently supports levelDB, badgerDB, IPFS and OSS, and the main data storage includes two ways:\ninstruction broadcast model based on IPFS-LOG\\CRDT_KV\\OrbitDB\nNative data storage model based on LevelDB\\Badger\\OSS\\IPFS\nMultiple IceFireDB nodes will be divided into groups according to data sets, and each group will form a highly available storage area structure.\nNoSQL storage engine # The core of each node is the database engine. By default, IceGiant node integrates KV storage engines such as levelDB, badgerDB, IPFS, OSS, etc., and implements the protocol coding layer of NoSQL on the KV storage relationship. Currently, the data storage of NoSQL mainly includes the following two ways:\nInstruction broadcast model # Based on ipfs-log,crdt and libp2p(pubsub), an immutable and operation-based conflict-free replication data model for distributed systems is implemented. Based on ipfs-log, various data structures such as event and kv are encapsulated, and multi-node database instruction broadcast is implemented based on this engine;At that bottom of IceFireDB, we abstract the variable kv engine base on badgerdb and leveldb. any node will broadcast the whole network when it is writing instruction, and the bottom driver of IceFireDB of each node will execute the broadcast instruction to ensure the final consistency of data.\nLog A Log B | | logA.append(\u0026#34;one\u0026#34;) logB.append(\u0026#34;hello\u0026#34;) | | v v +-----+ +-------+ |\u0026#34;one\u0026#34;| |\u0026#34;hello\u0026#34;| +-----+ +-------+ | | logA.append(\u0026#34;two\u0026#34;) logB.append(\u0026#34;world\u0026#34;) | | v v +-----------+ +---------------+ |\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;| |\u0026#34;hello\u0026#34;,\u0026#34;world\u0026#34;| +-----------+ +---------------+ | | | | logA.join(logB) \u0026lt;----------+ | v +---------------------------+ |\u0026#34;one\u0026#34;,\u0026#34;hello\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;world\u0026#34;| +---------------------------+ Full storage model # In addition to the first implementation mode, we are also building the structure of the second type of data, so that the complete data will grow on ipfs. At first, there is an ipfs driver in the IceFireDB driver layer, which will encode and process the upper-level commands into a unified kv data structure, store and change the value, and the generated new cid will be connected with key. However, at present, there is no key broadcast network with multiple nodes and data synchronization. When we connect with the broadcast network, we can build a data model originally grown on LevelDB\\Badger\\OSS\\IPFS.\n+-------------------------------------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv、list、hash、set | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ IceGiant Synchronizer # The storage layer of IceFireDB not only includes a complete storage server, but IceGiant Synchronizer, which is currently under construction, also belongs to the ecological software layer of the storage layer.\nIceGiant Synchronizer is an application directly above the database engine. All incoming database requests pass through IceGiant Synchronizer, which determines whether the requests should be processed, whether data writes should be propagated to other parts of the network, and whether local data should be written and customer requests should be responded to.\nIceGiant Synchronizer can also provide data write aggregation function, allowing multiple data requests to be merged and written into a single network storage request. It also allows users to cross-mix data sets between different nodes, encouraging further data decentralization, while keeping the operation overhead low.\n"},{"id":25,"href":"/icorer_docs/doccenter/redistun/designs/storage/","title":"Storage layer","section":"Designs","content":" Storage layer design # The storage layer is responsible for data storage, and the data storage here includes different storage media of web2 and web3. For web2, the storage media we face includes disk, OSS, and for web3, the storage media we face includes IPFS, blockchain, and smart contracts.Currently, the storage types supported by IceFireDB mainly include the following.\nEngine type describe Driver directory LevelDB LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values. Default Badger BadgerDB is an embeddable, persistent and fast key-value (KV) database written in pure Go. Badger OSS Object storage is a technology that stores and manages data in an unstructured format called objects. OSS IPFS IPFS (the InterPlanetary File System) is a hypermedia distribution protocol addressed by content and identities. It enables the creation of completely distributed applications, and in doing so aims to make the web faster, safer, and more open. IPFS CRDT-KV The IceFireDB-CRDT-KV engine can support decentralized P2P networking, data synchronization and consistency between nodes. It is a component of the IceFireDB software ecosystem, thanks to the open source of IPFS. CRDT-KV IPFS-LOG icefiredb-ipfs-log is a distributed immutable, operation-based conflict-free replication data structure that relies on ipfs to store data and merges each peer node data based on pubsub conflict-free. You can easily implement custom data structures such as kv, event, nosql, etc. based on icefiredb-ipfs-log. IPFS-LOG OrbitDB OrbitDB is a serverless, distributed, peer-to-peer database. OrbitDB uses IPFS as its data storage and IPFS Pubsub to automatically sync databases with peers. OrbitDB Storage model # The NoSQL storage layer of each individual IceGiant mainly includes the codec layer and the underlying KV storage layer. the underlying KV engine currently supports levelDB, badgerDB, IPFS and OSS, and the main data storage includes two ways:\ninstruction broadcast model based on IPFS-LOG\\CRDT_KV\\OrbitDB\nNative data storage model based on LevelDB\\Badger\\OSS\\IPFS\nMultiple IceFireDB nodes will be divided into groups according to data sets, and each group will form a highly available storage area structure.\nNoSQL storage engine # The core of each node is the database engine. By default, IceGiant node integrates KV storage engines such as levelDB, badgerDB, IPFS, OSS, etc., and implements the protocol coding layer of NoSQL on the KV storage relationship. Currently, the data storage of NoSQL mainly includes the following two ways:\nInstruction broadcast model # Based on ipfs-log,crdt and libp2p(pubsub), an immutable and operation-based conflict-free replication data model for distributed systems is implemented. Based on ipfs-log, various data structures such as event and kv are encapsulated, and multi-node database instruction broadcast is implemented based on this engine;At that bottom of IceFireDB, we abstract the variable kv engine base on badgerdb and leveldb. any node will broadcast the whole network when it is writing instruction, and the bottom driver of IceFireDB of each node will execute the broadcast instruction to ensure the final consistency of data.\nLog A Log B | | logA.append(\u0026#34;one\u0026#34;) logB.append(\u0026#34;hello\u0026#34;) | | v v +-----+ +-------+ |\u0026#34;one\u0026#34;| |\u0026#34;hello\u0026#34;| +-----+ +-------+ | | logA.append(\u0026#34;two\u0026#34;) logB.append(\u0026#34;world\u0026#34;) | | v v +-----------+ +---------------+ |\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;| |\u0026#34;hello\u0026#34;,\u0026#34;world\u0026#34;| +-----------+ +---------------+ | | | | logA.join(logB) \u0026lt;----------+ | v +---------------------------+ |\u0026#34;one\u0026#34;,\u0026#34;hello\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;world\u0026#34;| +---------------------------+ Full storage model # In addition to the first implementation mode, we are also building the structure of the second type of data, so that the complete data will grow on ipfs. At first, there is an ipfs driver in the IceFireDB driver layer, which will encode and process the upper-level commands into a unified kv data structure, store and change the value, and the generated new cid will be connected with key. However, at present, there is no key broadcast network with multiple nodes and data synchronization. When we connect with the broadcast network, we can build a data model originally grown on LevelDB\\Badger\\OSS\\IPFS.\n+-------------------------------------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv、list、hash、set | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ IceGiant Synchronizer # The storage layer of IceFireDB not only includes a complete storage server, but IceGiant Synchronizer, which is currently under construction, also belongs to the ecological software layer of the storage layer.\nIceGiant Synchronizer is an application directly above the database engine. All incoming database requests pass through IceGiant Synchronizer, which determines whether the requests should be processed, whether data writes should be propagated to other parts of the network, and whether local data should be written and customer requests should be responded to.\nIceGiant Synchronizer can also provide data write aggregation function, allowing multiple data requests to be merged and written into a single network storage request. It also allows users to cross-mix data sets between different nodes, encouraging further data decentralization, while keeping the operation overhead low.\n"},{"id":26,"href":"/icorer_docs/doccenter/ak-2019/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":27,"href":"/icorer_docs/doccenter/logdarts/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":28,"href":"/icorer_docs/doccenter/pulseflow/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":29,"href":"/icorer_docs/doccenter/redishub/designs/protocol/","title":"Protocol layer","section":"Designs","content":" Protocol layer design # A good access method of the application can accelerate the growth of the application ecology, and a good protocol design can reduce the transformation cost of the stock application, so the protocol layer is an important component of the IceFireDB software stack. The communication protocol of IceFireDB-NoSQL fully integrates the Redis RESP protocol, which mainly includes the following two parts of the protocol:\nData control protocol: Complete support for RESP clients, supporting functional requirements for database data access.\nCluster control protocol: Satisfy the client\u0026rsquo;s command protocol for controlling the nodes of the IceFireDB cluster and viewing the status and availability status of the cluster nodes.\n+-+---------------+----+---------------+----+---------------+-+ | Transport | | +---------------+ +---------------+ +---------------+ | | | Cluster | | Cluster | | Cluster | | | | communication | | communication | | communication | | | +---------------+ +---------------+ +---------------+ | +-+---------------+----+-------^-------+----+---------------+-+ | +------------------------------v------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +-------------------------------------------------------------+ As can be seen from the figure above, the cluster control protocol is located above the data read and write protocol. The client (redis cluster client, IceFireDB-Proxy) obtains the master-slave structure of the cluster nodes according to the cluster status, and selects the relevant master and slave nodes Perform data read and write operations. For the request traffic carried by IceFireDB, it will enter the request processing cycle. During the request processing cycle, it will analyze and optimize the client request.\nProtocol advantages # At present, the web2 ecology of RESP protocol clients is very rich, and mainstream computer languages have been fully covered. IceFireDB is firstly compatible with the RESP NoSQL protocol, which can quickly meet the needs of existing application systems to access IceFireDB-NoSQL.\nAs a decentralized database, IceFireDB is compatible with the RESP protocol, and can hide high technical intelligence from the user layer. Users do not need to understand P2P, RAFT, CRDT, IPFS-LOG and other technologies, and only need to follow the RESP protocol to choose the appropriate client for application You can operate the database to read and write, and quickly meet the access and transformation of the business system.\nData control protocol # Strings Hashes Lists Sets Sorted Sets APPEND HSET RPUSH SADD ZADD BITCOUNT HGET LPOP SCARD ZCARD BITOP HDEL LINDEX SDIFF ZCOUNT BITPOS HEXISTS LPUSH SDIFFSTORE ZREM DECR HGETALL RPOP SINTER ZCLEAR DECRBY HINCRBY LRANGE SINTERSTORE ZRANK DEL HKEYS LSET SISMEMBER ZRANGE EXISTS HLEN LLEN SMEMBERS ZREVRANGE GET HMGET RPOPLPUSH SREM ZSCORE GETBIT HMSET LCLEAR SUNION ZINCRBY SETBIT HSETEX LCLEAR SUNIONSTORE ZREVRANK GETRANGE HSTRLEN LMCLEAR SCLEAR ZRANGEBYSCORE GETSET HVALS LEXPIRE SMCLEAR ZREVRANGEBYSCORE INCR HCLEAR LEXPIREAT SEXPIRE ZREMRANGEBYSCORE EXISTS HMCLEAR LKEYEXISTS SEXPIRE ZREMRANGEBYRANK GET HEXPIRE LTRIM SEXPIREAT GETBIT HEXPIREAT LTTL STTL SETBIT HKEYEXIST SPERSIST GETRANGE HTTL SKEYEXISTS GETSET INCRBY GET MGET MSET SET SETEX SETEXAT SETRANGE EXPIRE EXPIREAT TTL Cluster control protocol # IceFireDB-NoSQL integrates some Redis cluster status instructions in the RAFT parallel database scenario.\nVERSION # show the application version MACHINE # show information about the state machine RAFT LEADER # show the address of the current raft leader RAFT INFO [pattern] # show information about the raft server and cluster RAFT SERVER LIST # show all servers in cluster RAFT SERVER ADD id address # add a server to cluster RAFT SERVER REMOVE id # remove a server from the cluster RAFT SNAPSHOT NOW # make a snapshot of the data RAFT SNAPSHOT LIST # show a list of all snapshots on server RAFT SNAPSHOT FILE id # show the file path of a snapshot on server RAFT SNAPSHOT READ id [RANGE start end] # download all or part of a snapshot "},{"id":30,"href":"/icorer_docs/doccenter/redistun/designs/protocol/","title":"Protocol layer","section":"Designs","content":" Protocol layer design # A good access method of the application can accelerate the growth of the application ecology, and a good protocol design can reduce the transformation cost of the stock application, so the protocol layer is an important component of the IceFireDB software stack. The communication protocol of IceFireDB-NoSQL fully integrates the Redis RESP protocol, which mainly includes the following two parts of the protocol:\nData control protocol: Complete support for RESP clients, supporting functional requirements for database data access.\nCluster control protocol: Satisfy the client\u0026rsquo;s command protocol for controlling the nodes of the IceFireDB cluster and viewing the status and availability status of the cluster nodes.\n+-+---------------+----+---------------+----+---------------+-+ | Transport | | +---------------+ +---------------+ +---------------+ | | | Cluster | | Cluster | | Cluster | | | | communication | | communication | | communication | | | +---------------+ +---------------+ +---------------+ | +-+---------------+----+-------^-------+----+---------------+-+ | +------------------------------v------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +-------------------------------------------------------------+ As can be seen from the figure above, the cluster control protocol is located above the data read and write protocol. The client (redis cluster client, IceFireDB-Proxy) obtains the master-slave structure of the cluster nodes according to the cluster status, and selects the relevant master and slave nodes Perform data read and write operations. For the request traffic carried by IceFireDB, it will enter the request processing cycle. During the request processing cycle, it will analyze and optimize the client request.\nProtocol advantages # At present, the web2 ecology of RESP protocol clients is very rich, and mainstream computer languages have been fully covered. IceFireDB is firstly compatible with the RESP NoSQL protocol, which can quickly meet the needs of existing application systems to access IceFireDB-NoSQL.\nAs a decentralized database, IceFireDB is compatible with the RESP protocol, and can hide high technical intelligence from the user layer. Users do not need to understand P2P, RAFT, CRDT, IPFS-LOG and other technologies, and only need to follow the RESP protocol to choose the appropriate client for application You can operate the database to read and write, and quickly meet the access and transformation of the business system.\nData control protocol # Strings Hashes Lists Sets Sorted Sets APPEND HSET RPUSH SADD ZADD BITCOUNT HGET LPOP SCARD ZCARD BITOP HDEL LINDEX SDIFF ZCOUNT BITPOS HEXISTS LPUSH SDIFFSTORE ZREM DECR HGETALL RPOP SINTER ZCLEAR DECRBY HINCRBY LRANGE SINTERSTORE ZRANK DEL HKEYS LSET SISMEMBER ZRANGE EXISTS HLEN LLEN SMEMBERS ZREVRANGE GET HMGET RPOPLPUSH SREM ZSCORE GETBIT HMSET LCLEAR SUNION ZINCRBY SETBIT HSETEX LCLEAR SUNIONSTORE ZREVRANK GETRANGE HSTRLEN LMCLEAR SCLEAR ZRANGEBYSCORE GETSET HVALS LEXPIRE SMCLEAR ZREVRANGEBYSCORE INCR HCLEAR LEXPIREAT SEXPIRE ZREMRANGEBYSCORE EXISTS HMCLEAR LKEYEXISTS SEXPIRE ZREMRANGEBYRANK GET HEXPIRE LTRIM SEXPIREAT GETBIT HEXPIREAT LTTL STTL SETBIT HKEYEXIST SPERSIST GETRANGE HTTL SKEYEXISTS GETSET INCRBY GET MGET MSET SET SETEX SETEXAT SETRANGE EXPIRE EXPIREAT TTL Cluster control protocol # IceFireDB-NoSQL integrates some Redis cluster status instructions in the RAFT parallel database scenario.\nVERSION # show the application version MACHINE # show information about the state machine RAFT LEADER # show the address of the current raft leader RAFT INFO [pattern] # show information about the raft server and cluster RAFT SERVER LIST # show all servers in cluster RAFT SERVER ADD id address # add a server to cluster RAFT SERVER REMOVE id # remove a server from the cluster RAFT SNAPSHOT NOW # make a snapshot of the data RAFT SNAPSHOT LIST # show a list of all snapshots on server RAFT SNAPSHOT FILE id # show the file path of a snapshot on server RAFT SNAPSHOT READ id [RANGE start end] # download all or part of a snapshot "},{"id":31,"href":"/icorer_docs/doccenter/redishub/develop/redhub/","title":"redhub-frame","section":"Develop","content":" redhub-frame # Project introduction # High-performance Redis-Server multi-threaded framework, based on RawEpoll model.\nFeatures # Ultra high performance Fully multi-threaded support Low CPU resource consumption Compatible with redis protocol Create a Redis compatible server with RawEpoll model in Go Installing # go get -u github.com/IceFireDB/redhub Example # Here is a simple framework usage example,support the following redis commands:\nSET key value GET key DEL key PING QUIT You can run this example in terminal:\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;net/http\u0026#34; _ \u0026#34;net/http/pprof\u0026#34; \u0026#34;github.com/IceFireDB/redhub\u0026#34; \u0026#34;github.com/IceFireDB/redhub/pkg/resp\u0026#34; ) func main() { var mu sync.RWMutex var items = make(map[string][]byte) var network string var addr string var multicore bool var reusePort bool var pprofDebug bool var pprofAddr string flag.StringVar(\u0026amp;network, \u0026#34;network\u0026#34;, \u0026#34;tcp\u0026#34;, \u0026#34;server network (default \\\u0026#34;tcp\\\u0026#34;)\u0026#34;) flag.StringVar(\u0026amp;addr, \u0026#34;addr\u0026#34;, \u0026#34;127.0.0.1:6380\u0026#34;, \u0026#34;server addr (default \\\u0026#34;:6380\\\u0026#34;)\u0026#34;) flag.BoolVar(\u0026amp;multicore, \u0026#34;multicore\u0026#34;, true, \u0026#34;multicore\u0026#34;) flag.BoolVar(\u0026amp;reusePort, \u0026#34;reusePort\u0026#34;, false, \u0026#34;reusePort\u0026#34;) flag.BoolVar(\u0026amp;pprofDebug, \u0026#34;pprofDebug\u0026#34;, false, \u0026#34;open pprof\u0026#34;) flag.StringVar(\u0026amp;pprofAddr, \u0026#34;pprofAddr\u0026#34;, \u0026#34;:8888\u0026#34;, \u0026#34;pprof address\u0026#34;) flag.Parse() if pprofDebug { go func() { http.ListenAndServe(pprofAddr, nil) }() } protoAddr := fmt.Sprintf(\u0026#34;%s://%s\u0026#34;, network, addr) option := redhub.Options{ Multicore: multicore, ReusePort: reusePort, } rh := redhub.NewRedHub( func(c *redhub.Conn) (out []byte, action redhub.Action) { return }, func(c *redhub.Conn, err error) (action redhub.Action) { return }, func(cmd resp.Command, out []byte) ([]byte, redhub.Action) { var status redhub.Action switch strings.ToLower(string(cmd.Args[0])) { default: out = resp.AppendError(out, \u0026#34;ERR unknown command \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39;\u0026#34;) case \u0026#34;ping\u0026#34;: out = resp.AppendString(out, \u0026#34;PONG\u0026#34;) case \u0026#34;quit\u0026#34;: out = resp.AppendString(out, \u0026#34;OK\u0026#34;) status = redhub.Close case \u0026#34;set\u0026#34;: if len(cmd.Args) != 3 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.Lock() items[string(cmd.Args[1])] = cmd.Args[2] mu.Unlock() out = resp.AppendString(out, \u0026#34;OK\u0026#34;) case \u0026#34;get\u0026#34;: if len(cmd.Args) != 2 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.RLock() val, ok := items[string(cmd.Args[1])] mu.RUnlock() if !ok { out = resp.AppendNull(out) } else { out = resp.AppendBulk(out, val) } case \u0026#34;del\u0026#34;: if len(cmd.Args) != 2 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.Lock() _, ok := items[string(cmd.Args[1])] delete(items, string(cmd.Args[1])) mu.Unlock() if !ok { out = resp.AppendInt(out, 0) } else { out = resp.AppendInt(out, 1) } case \u0026#34;config\u0026#34;: // This simple (blank) response is only here to allow for the // redis-benchmark command to work with this example. out = resp.AppendArray(out, 2) out = resp.AppendBulk(out, cmd.Args[2]) out = resp.AppendBulkString(out, \u0026#34;\u0026#34;) } return out, status }, ) log.Printf(\u0026#34;started redhub server at %s\u0026#34;, addr) err := redhub.ListendAndServe(protoAddr, option, rh) if err != nil { log.Fatal(err) } } Benchmarks # Machine information OS : Debian Buster 10.6 64bit CPU : 8 CPU cores Memory : 64.0 GiB Go Version : go1.16.5 linux/amd64 【Redis-server5.0.3】 Single-threaded, no disk persistence. # $ ./redis-server --port 6380 --appendonly no $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2306060.50 requests per second GET: 3096742.25 requests per second 【Redis-server6.2.5】 Single-threaded, no disk persistence. # $ ./redis-server --port 6380 --appendonly no $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2076325.75 requests per second GET: 2652801.50 requests per second 【Redis-server6.2.5】 Multi-threaded, no disk persistence. # io-threads-do-reads yes io-threads 8 $ ./redis-server redis.conf $ redis-benchmark -h 127.0.0.1 -p 6379 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 1944692.88 requests per second GET: 2375184.00 requests per second 【RedCon】 Multi-threaded, no disk persistence # $ go run example/clone.go $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2332742.25 requests per second GET: 14654162.00 requests per second 【RedHub】 Multi-threaded, no disk persistence # $ go run example/server.go $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 4087305.00 requests per second GET: 16490765.00 requests per second "},{"id":32,"href":"/icorer_docs/doccenter/redistun/develop/redhub/","title":"redhub-frame","section":"Develop","content":" redhub-frame # Project introduction # High-performance Redis-Server multi-threaded framework, based on RawEpoll model.\nFeatures # Ultra high performance Fully multi-threaded support Low CPU resource consumption Compatible with redis protocol Create a Redis compatible server with RawEpoll model in Go Installing # go get -u github.com/IceFireDB/redhub Example # Here is a simple framework usage example,support the following redis commands:\nSET key value GET key DEL key PING QUIT You can run this example in terminal:\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;net/http\u0026#34; _ \u0026#34;net/http/pprof\u0026#34; \u0026#34;github.com/IceFireDB/redhub\u0026#34; \u0026#34;github.com/IceFireDB/redhub/pkg/resp\u0026#34; ) func main() { var mu sync.RWMutex var items = make(map[string][]byte) var network string var addr string var multicore bool var reusePort bool var pprofDebug bool var pprofAddr string flag.StringVar(\u0026amp;network, \u0026#34;network\u0026#34;, \u0026#34;tcp\u0026#34;, \u0026#34;server network (default \\\u0026#34;tcp\\\u0026#34;)\u0026#34;) flag.StringVar(\u0026amp;addr, \u0026#34;addr\u0026#34;, \u0026#34;127.0.0.1:6380\u0026#34;, \u0026#34;server addr (default \\\u0026#34;:6380\\\u0026#34;)\u0026#34;) flag.BoolVar(\u0026amp;multicore, \u0026#34;multicore\u0026#34;, true, \u0026#34;multicore\u0026#34;) flag.BoolVar(\u0026amp;reusePort, \u0026#34;reusePort\u0026#34;, false, \u0026#34;reusePort\u0026#34;) flag.BoolVar(\u0026amp;pprofDebug, \u0026#34;pprofDebug\u0026#34;, false, \u0026#34;open pprof\u0026#34;) flag.StringVar(\u0026amp;pprofAddr, \u0026#34;pprofAddr\u0026#34;, \u0026#34;:8888\u0026#34;, \u0026#34;pprof address\u0026#34;) flag.Parse() if pprofDebug { go func() { http.ListenAndServe(pprofAddr, nil) }() } protoAddr := fmt.Sprintf(\u0026#34;%s://%s\u0026#34;, network, addr) option := redhub.Options{ Multicore: multicore, ReusePort: reusePort, } rh := redhub.NewRedHub( func(c *redhub.Conn) (out []byte, action redhub.Action) { return }, func(c *redhub.Conn, err error) (action redhub.Action) { return }, func(cmd resp.Command, out []byte) ([]byte, redhub.Action) { var status redhub.Action switch strings.ToLower(string(cmd.Args[0])) { default: out = resp.AppendError(out, \u0026#34;ERR unknown command \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39;\u0026#34;) case \u0026#34;ping\u0026#34;: out = resp.AppendString(out, \u0026#34;PONG\u0026#34;) case \u0026#34;quit\u0026#34;: out = resp.AppendString(out, \u0026#34;OK\u0026#34;) status = redhub.Close case \u0026#34;set\u0026#34;: if len(cmd.Args) != 3 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.Lock() items[string(cmd.Args[1])] = cmd.Args[2] mu.Unlock() out = resp.AppendString(out, \u0026#34;OK\u0026#34;) case \u0026#34;get\u0026#34;: if len(cmd.Args) != 2 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.RLock() val, ok := items[string(cmd.Args[1])] mu.RUnlock() if !ok { out = resp.AppendNull(out) } else { out = resp.AppendBulk(out, val) } case \u0026#34;del\u0026#34;: if len(cmd.Args) != 2 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.Lock() _, ok := items[string(cmd.Args[1])] delete(items, string(cmd.Args[1])) mu.Unlock() if !ok { out = resp.AppendInt(out, 0) } else { out = resp.AppendInt(out, 1) } case \u0026#34;config\u0026#34;: // This simple (blank) response is only here to allow for the // redis-benchmark command to work with this example. out = resp.AppendArray(out, 2) out = resp.AppendBulk(out, cmd.Args[2]) out = resp.AppendBulkString(out, \u0026#34;\u0026#34;) } return out, status }, ) log.Printf(\u0026#34;started redhub server at %s\u0026#34;, addr) err := redhub.ListendAndServe(protoAddr, option, rh) if err != nil { log.Fatal(err) } } Benchmarks # Machine information OS : Debian Buster 10.6 64bit CPU : 8 CPU cores Memory : 64.0 GiB Go Version : go1.16.5 linux/amd64 【Redis-server5.0.3】 Single-threaded, no disk persistence. # $ ./redis-server --port 6380 --appendonly no $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2306060.50 requests per second GET: 3096742.25 requests per second 【Redis-server6.2.5】 Single-threaded, no disk persistence. # $ ./redis-server --port 6380 --appendonly no $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2076325.75 requests per second GET: 2652801.50 requests per second 【Redis-server6.2.5】 Multi-threaded, no disk persistence. # io-threads-do-reads yes io-threads 8 $ ./redis-server redis.conf $ redis-benchmark -h 127.0.0.1 -p 6379 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 1944692.88 requests per second GET: 2375184.00 requests per second 【RedCon】 Multi-threaded, no disk persistence # $ go run example/clone.go $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2332742.25 requests per second GET: 14654162.00 requests per second 【RedHub】 Multi-threaded, no disk persistence # $ go run example/server.go $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 4087305.00 requests per second GET: 16490765.00 requests per second "},{"id":33,"href":"/icorer_docs/doccenter/pulseflow/architecture/codec/","title":"Codec","section":"Architecture","content":" Codec Engine Details # "},{"id":34,"href":"/icorer_docs/doccenter/redishub/designs/codec/","title":"Codec layer","section":"Designs","content":" Codec layer design # The codec layer is the glue of the IceFireDB data expression layer, because the bottom layer of IceFireDB supports many storage engines, including centralized storage such as web2 disk, OSS, leveldb, and badger, as well as web3\u0026rsquo;s IPFS, crdt-kv, and IPFS-LOG For this kind of decentralized storage, the storage interface provided by any kind of storage is simple and not standardized. The codec layer of IceFireDB-NoSQL is abstracted through a unified driver layer, and by encoding and decoding many instruction semantics into a KV model, a richer data expression layer is built to support more data scenarios, such as Strings\\Hashs\\Sets\\Lists \\Sorted Sets.\n+-------------------------------------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +---+--------------------------+--------------------------+---+ | +------------------------------v------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv、list、hash、set | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ As an important glue layer, the codec layer connects the network layer, request layer, and KV storage layer.\nCodec advantages # In the decentralized database scenario, although the community also has a decentralized storage solution similar to the KV model, the KV model cannot meet the complex use of upper-level applications. We believe that rich data structures need to be supported to support the decentralized development of applications, including NoSQL data models (such as strings\\hashs\\lists\\sets\\sorted sets data structures). Just as the prosperity of the web2 application ecology is inseparable from the contribution of database infrastructure such as memcached, redis, and mysql, IceFireDB provides richer data types and can support data decentralization in more complex scenarios.\nThe encoding and decoding layer encodes data commands and parameters to meet the conversion of many rich instruction data models to KV models. Under the function of this conversion layer, the complex shielding layer of the data model layer and the storage engine layer is effectively constructed.\nThe data instruction layer does not need to care about the underlying storage engine, so it can adapt to various storage engine drivers with the help of the codec layer.\nThe underlying storage engine continues to provide a simple data operation interface (put\\get\\del\\iterator), without direct coupling and customization with the data presentation layer, and the abstract storage driver layer of the codec layer maintains the easy work of the two layers.\nAny problem in computer science can be solved by another layer of indirection.\nCodec Layer value # In addition to adding a richer data model and isolating the complexity of the request layer and storage layer, the codec layer of IceFireDB also has the following functions:\nImprove data access speed: A memory buffer layer is added to the encoding layer to effectively improve the performance of data access. Calculate the middle layer：The support of complex data structures often requires a certain amount of calculation. At present, the coding layer mainly performs CPU calculations, and subsequent calculations can be combined with GPU and FPGA, which can expand high-performance data calculations and add more possibilities for decentralized databases. blockchain/web3 connection layer: At present, whether it is a web2 or web3 database scenario, data access is the focus of attention, but with the development of blockchain and web3, trusted computing of data and non-tamperable proofs are becoming more and more important. The codec layer of IceFireDB is playing an important role , can be used to combine the blockchain data structure to build web3 side chain infrastructure, reducing the calculation and storage burden of the blockchain layer. "},{"id":35,"href":"/icorer_docs/doccenter/redistun/designs/codec/","title":"Codec layer","section":"Designs","content":" Codec layer design # The codec layer is the glue of the IceFireDB data expression layer, because the bottom layer of IceFireDB supports many storage engines, including centralized storage such as web2 disk, OSS, leveldb, and badger, as well as web3\u0026rsquo;s IPFS, crdt-kv, and IPFS-LOG For this kind of decentralized storage, the storage interface provided by any kind of storage is simple and not standardized. The codec layer of IceFireDB-NoSQL is abstracted through a unified driver layer, and by encoding and decoding many instruction semantics into a KV model, a richer data expression layer is built to support more data scenarios, such as Strings\\Hashs\\Sets\\Lists \\Sorted Sets.\n+-------------------------------------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +---+--------------------------+--------------------------+---+ | +------------------------------v------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv、list、hash、set | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ As an important glue layer, the codec layer connects the network layer, request layer, and KV storage layer.\nCodec advantages # In the decentralized database scenario, although the community also has a decentralized storage solution similar to the KV model, the KV model cannot meet the complex use of upper-level applications. We believe that rich data structures need to be supported to support the decentralized development of applications, including NoSQL data models (such as strings\\hashs\\lists\\sets\\sorted sets data structures). Just as the prosperity of the web2 application ecology is inseparable from the contribution of database infrastructure such as memcached, redis, and mysql, IceFireDB provides richer data types and can support data decentralization in more complex scenarios.\nThe encoding and decoding layer encodes data commands and parameters to meet the conversion of many rich instruction data models to KV models. Under the function of this conversion layer, the complex shielding layer of the data model layer and the storage engine layer is effectively constructed.\nThe data instruction layer does not need to care about the underlying storage engine, so it can adapt to various storage engine drivers with the help of the codec layer.\nThe underlying storage engine continues to provide a simple data operation interface (put\\get\\del\\iterator), without direct coupling and customization with the data presentation layer, and the abstract storage driver layer of the codec layer maintains the easy work of the two layers.\nAny problem in computer science can be solved by another layer of indirection.\nCodec Layer value # In addition to adding a richer data model and isolating the complexity of the request layer and storage layer, the codec layer of IceFireDB also has the following functions:\nImprove data access speed: A memory buffer layer is added to the encoding layer to effectively improve the performance of data access. Calculate the middle layer：The support of complex data structures often requires a certain amount of calculation. At present, the coding layer mainly performs CPU calculations, and subsequent calculations can be combined with GPU and FPGA, which can expand high-performance data calculations and add more possibilities for decentralized databases. blockchain/web3 connection layer: At present, whether it is a web2 or web3 database scenario, data access is the focus of attention, but with the development of blockchain and web3, trusted computing of data and non-tamperable proofs are becoming more and more important. The codec layer of IceFireDB is playing an important role , can be used to combine the blockchain data structure to build web3 side chain infrastructure, reducing the calculation and storage burden of the blockchain layer. "},{"id":36,"href":"/icorer_docs/doccenter/redishub/quick_start/","title":"Quick Start","section":"RedisHub","content":" Quick Start Guide for the IceFireDB Database # This guide walks you through the quickest way to get started with IceFireDB. For non-production environments, you can deploy your IceFireDB database by either of the following methods:\nDeploy a local test cluster，Simulate production deployment on a single machine Deploy a local test cluster # Scenario: Quickly deploy a local IceFireDB cluster for testing using a single macOS or Linux server. As a distributed system, in the same availability zone network, a basic IceFireDB cluster usually consists of 3 IceFireDB instances.\n1.Download and compile the program # git clone https://github.com/IceFireDB/IceFireDB.git IceFireDB-NoSQL cd IceFireDB-NoSQL make \u0026amp;\u0026amp; ls ./bin/IceFireDB If the following message is displayed, you have build IceFireDB-NoSQL successfully:\nif [ ! -d \u0026#34;./bin/\u0026#34; ]; then \\ mkdir bin; \\ fi go build -ldflags \u0026#34;-s -w -X \\\u0026#34;main.BuildVersion=1c102f3\\\u0026#34; -X \\\u0026#34;main.BuildDate=2022-11-21 06:17:29\\\u0026#34;\u0026#34; -o bin/IceFireDB . ./bin/IceFireDB 2.Declare the global environment variable # IceFireDB implements many engines at the bottom, mainly including the following categories. The choice of the bottom engine is initialized through cmd variables.\nEngine type cmd key cmd value LevelDB storage-backend goleveldb Badger storage-backend badger IPFS storage-backend ipfs CRDT-KV storage-backend crdt IPFS-LOG storage-backend ipfs-log OrbitDB storage-backend orbitdb OSS storage-backend oss 3.Start the cluster in the current session # mkdir 6001 \u0026amp;\u0026amp; mkdir 6002 \u0026amp;\u0026amp; mkdir 6003 cp ./bin/IceFireDB ./6001 cp ./bin/IceFireDB ./6002 cp ./bin/IceFireDB ./6003 # start node1 /pwd/IceFireDB-NoSQL/6001/IceFireDB -storage-backend ipfs-log -n 1 -a 127.0.0.1:6001 --openreads # start node2 /pwd/IceFireDB-NoSQL/6002/IceFireDB -storage-backend ipfs-log -n 2 -a 127.0.0.1:6002 -j 127.0.0.1:6001 --openreads # start node3 /pwd/IceFireDB-NoSQL/6003/IceFireDB -storage-backend ipfs-log -n 3 -a 127.0.0.1:6003 -j 127.0.0.1:6001 --openreads In the same network availability zone, multiple IceFireDB instances can be added to the same raft network, and the same raft network exposes the standard Redis cluster access interface to meet the access requirements of the Redis client.\n4.Start a new session to access IceFireDB # The above steps start three IceFireDB nodes and form a highly available network with each other.We can use redis-cli to observe the cluster status\nsudo apt-get -y install redis-tools redis-cli cluster nodes We execute the cluster nodes command in the redis-cli terminal, and we can view the cluster status as follows:\n127.0.0.1:6002\u0026gt; cluster nodes 356a192b7913b04c54574d18c28d46e6395428ab 127.0.0.1:6001@6001 slave 77de68daecd823babbb58edb1c8e14d7106e83bb 0 0 connected 0-16383 da4b9237bacccdf19c0760cab7aec4a8359010b0 127.0.0.1:6002@6002 slave 77de68daecd823babbb58edb1c8e14d7106e83bb 0 0 connected 0-16383 77de68daecd823babbb58edb1c8e14d7106e83bb 127.0.0.1:6003@6003 master - 0 0 connected 0-16383 We use redis-cli for data read and write tests：\nredis-cli -c -h 127.0.0.1 -p 6002 127.0.0.1:6002\u0026gt; set foo bar -\u0026gt; Redirected to slot [0] located at 127.0.0.1:6003 OK 127.0.0.1:6003\u0026gt; get foo \u0026#34;bar\u0026#34; We can see that the data can be read and written normally，The current master is an instance of 6003. Since we have enabled the read data permission of all nodes, we can view data in other slave nodes.\nredis-cli -h 127.0.0.1 -p 6001 127.0.0.1:6001\u0026gt; get foo \u0026#34;bar\u0026#34; # Although we can read data in the slave node, we cannot write data directly on the slave node. 127.0.0.1:6001\u0026gt; set foo2 bar2 (error) MOVED 0 127.0.0.1:6003 Advanced Eco Tools # IceFireDB-Proxy: Intelligent network proxy # In the above case, we fully demonstrated the cluster construction and data read-write access, but the master-slave relationship between high-availability nodes, data read-write fault tolerance of the client, and the perception of the status of each node in the cluster are complex, so We launched the IceFireDB-Proxy software, which can shield users from understanding the complexity of the IceFireDB high-availability cluster, and use the IceFireDB cluster like a single IceFireDB.\n"},{"id":37,"href":"/icorer_docs/doccenter/redistun/quick_start/","title":"Quick Start","section":"RedisTun","content":" Quick Start Guide for the IceFireDB Database # This guide walks you through the quickest way to get started with IceFireDB. For non-production environments, you can deploy your IceFireDB database by either of the following methods:\nDeploy a local test cluster，Simulate production deployment on a single machine Deploy a local test cluster # Scenario: Quickly deploy a local IceFireDB cluster for testing using a single macOS or Linux server. As a distributed system, in the same availability zone network, a basic IceFireDB cluster usually consists of 3 IceFireDB instances.\n1.Download and compile the program # git clone https://github.com/IceFireDB/IceFireDB.git IceFireDB-NoSQL cd IceFireDB-NoSQL make \u0026amp;\u0026amp; ls ./bin/IceFireDB If the following message is displayed, you have build IceFireDB-NoSQL successfully:\nif [ ! -d \u0026#34;./bin/\u0026#34; ]; then \\ mkdir bin; \\ fi go build -ldflags \u0026#34;-s -w -X \\\u0026#34;main.BuildVersion=1c102f3\\\u0026#34; -X \\\u0026#34;main.BuildDate=2022-11-21 06:17:29\\\u0026#34;\u0026#34; -o bin/IceFireDB . ./bin/IceFireDB 2.Declare the global environment variable # IceFireDB implements many engines at the bottom, mainly including the following categories. The choice of the bottom engine is initialized through cmd variables.\nEngine type cmd key cmd value LevelDB storage-backend goleveldb Badger storage-backend badger IPFS storage-backend ipfs CRDT-KV storage-backend crdt IPFS-LOG storage-backend ipfs-log OrbitDB storage-backend orbitdb OSS storage-backend oss 3.Start the cluster in the current session # mkdir 6001 \u0026amp;\u0026amp; mkdir 6002 \u0026amp;\u0026amp; mkdir 6003 cp ./bin/IceFireDB ./6001 cp ./bin/IceFireDB ./6002 cp ./bin/IceFireDB ./6003 # start node1 /pwd/IceFireDB-NoSQL/6001/IceFireDB -storage-backend ipfs-log -n 1 -a 127.0.0.1:6001 --openreads # start node2 /pwd/IceFireDB-NoSQL/6002/IceFireDB -storage-backend ipfs-log -n 2 -a 127.0.0.1:6002 -j 127.0.0.1:6001 --openreads # start node3 /pwd/IceFireDB-NoSQL/6003/IceFireDB -storage-backend ipfs-log -n 3 -a 127.0.0.1:6003 -j 127.0.0.1:6001 --openreads In the same network availability zone, multiple IceFireDB instances can be added to the same raft network, and the same raft network exposes the standard Redis cluster access interface to meet the access requirements of the Redis client.\n4.Start a new session to access IceFireDB # The above steps start three IceFireDB nodes and form a highly available network with each other.We can use redis-cli to observe the cluster status\nsudo apt-get -y install redis-tools redis-cli cluster nodes We execute the cluster nodes command in the redis-cli terminal, and we can view the cluster status as follows:\n127.0.0.1:6002\u0026gt; cluster nodes 356a192b7913b04c54574d18c28d46e6395428ab 127.0.0.1:6001@6001 slave 77de68daecd823babbb58edb1c8e14d7106e83bb 0 0 connected 0-16383 da4b9237bacccdf19c0760cab7aec4a8359010b0 127.0.0.1:6002@6002 slave 77de68daecd823babbb58edb1c8e14d7106e83bb 0 0 connected 0-16383 77de68daecd823babbb58edb1c8e14d7106e83bb 127.0.0.1:6003@6003 master - 0 0 connected 0-16383 We use redis-cli for data read and write tests：\nredis-cli -c -h 127.0.0.1 -p 6002 127.0.0.1:6002\u0026gt; set foo bar -\u0026gt; Redirected to slot [0] located at 127.0.0.1:6003 OK 127.0.0.1:6003\u0026gt; get foo \u0026#34;bar\u0026#34; We can see that the data can be read and written normally，The current master is an instance of 6003. Since we have enabled the read data permission of all nodes, we can view data in other slave nodes.\nredis-cli -h 127.0.0.1 -p 6001 127.0.0.1:6001\u0026gt; get foo \u0026#34;bar\u0026#34; # Although we can read data in the slave node, we cannot write data directly on the slave node. 127.0.0.1:6001\u0026gt; set foo2 bar2 (error) MOVED 0 127.0.0.1:6003 Advanced Eco Tools # IceFireDB-Proxy: Intelligent network proxy # In the above case, we fully demonstrated the cluster construction and data read-write access, but the master-slave relationship between high-availability nodes, data read-write fault tolerance of the client, and the perception of the status of each node in the cluster are complex, so We launched the IceFireDB-Proxy software, which can shield users from understanding the complexity of the IceFireDB high-availability cluster, and use the IceFireDB cluster like a single IceFireDB.\n"},{"id":38,"href":"/icorer_docs/doccenter/redishub/designs/","title":"Designs","section":"RedisHub","content":" System Design # In order to build a decentralized database, the core of the IceFireDB system is to provide data decentralization and immutability for applications. Aiming at the above goals, we have designed the following core system levels.\n+-+---------------+----+---------------+----+---------------+-+ | Transport | | +---------------+ +---------------+ +---------------+ | | | Cluster | | Cluster | | Cluster | | | | communication | | communication | | communication | | | +---------------+ +---------------+ +---------------+ | +-+---------------+----+-------^-------+----+---------------+-+ | +------------------------------v------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +---+--------------------------+--------------------------+---+ | +------------------------------v------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv、list、hash、set | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ At the above system level, IceFireDB refines and implements the following important system components.\nSystem components describe technology used Network layer 1. RAFT guarantees data consistency within a single availability zone. 2. P2P network construction decentralized database communication. 3. NATS is a new network layer being built. P2P、RAFT、NATS Storage layer Many types of storage are currently supported. Under the codec computing layer, we abstract the KV storage driver layer, which is compatible with different storage engines of web2 and web3. goleveldb、badger、IPFS、CRDT、IPFS-LOG、OSS Protocol layer Based on the codec layer, we have built a protocol layer. A good communication protocol allows more applications to easily access the IceFireDB data network. Currently, we support the Redis-RESP NoSQL protocol and the MySQL protocol. RESP、SQL Codec layer The codec layer is the core of our system. For NoSQL scenarios, any data type will be abstracted into a KV storage model. With the flexible coding layer, we can build rich data operation structures and instructions, such as hash, sets, strings, etc. KV、Strings、Hashes、Lists、Sorted Sets、Sets、SQL、PubSub "},{"id":39,"href":"/icorer_docs/doccenter/redistun/designs/","title":"Designs","section":"RedisTun","content":" System Design # In order to build a decentralized database, the core of the IceFireDB system is to provide data decentralization and immutability for applications. Aiming at the above goals, we have designed the following core system levels.\n+-+---------------+----+---------------+----+---------------+-+ | Transport | | +---------------+ +---------------+ +---------------+ | | | Cluster | | Cluster | | Cluster | | | | communication | | communication | | communication | | | +---------------+ +---------------+ +---------------+ | +-+---------------+----+-------^-------+----+---------------+-+ | +------------------------------v------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +---+--------------------------+--------------------------+---+ | +------------------------------v------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv、list、hash、set | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ At the above system level, IceFireDB refines and implements the following important system components.\nSystem components describe technology used Network layer 1. RAFT guarantees data consistency within a single availability zone. 2. P2P network construction decentralized database communication. 3. NATS is a new network layer being built. P2P、RAFT、NATS Storage layer Many types of storage are currently supported. Under the codec computing layer, we abstract the KV storage driver layer, which is compatible with different storage engines of web2 and web3. goleveldb、badger、IPFS、CRDT、IPFS-LOG、OSS Protocol layer Based on the codec layer, we have built a protocol layer. A good communication protocol allows more applications to easily access the IceFireDB data network. Currently, we support the Redis-RESP NoSQL protocol and the MySQL protocol. RESP、SQL Codec layer The codec layer is the core of our system. For NoSQL scenarios, any data type will be abstracted into a KV storage model. With the flexible coding layer, we can build rich data operation structures and instructions, such as hash, sets, strings, etc. KV、Strings、Hashes、Lists、Sorted Sets、Sets、SQL、PubSub "},{"id":40,"href":"/icorer_docs/doccenter/redishub/tutorials/","title":"Tutorials","section":"RedisHub","content":" Tutorials # "},{"id":41,"href":"/icorer_docs/doccenter/redistun/tutorials/","title":"Tutorials","section":"RedisTun","content":" Tutorials # "},{"id":42,"href":"/icorer_docs/doccenter/","title":"项目总览","section":"工匠之芯-文档中心","content":" 项目总览 # RedisHub PulseFlow "},{"id":43,"href":"/icorer_docs/doccenter/redishub/deploy/","title":"Deploy","section":"RedisHub","content":" Deploy # "},{"id":44,"href":"/icorer_docs/doccenter/redistun/deploy/","title":"Deploy","section":"RedisTun","content":" Deploy # "}]