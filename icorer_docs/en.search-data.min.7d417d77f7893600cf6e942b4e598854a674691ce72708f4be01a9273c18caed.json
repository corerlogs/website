[{"id":0,"href":"/icorer_docs/doccenter/redishub/module_test/2.1/","title":"2.1 NetHandle组件测试","section":"系统模块测试","content":" NetHandle # This is a library for building high-performance TCP server applications, standing on the shoulders of Go giants and redcon\n1. Features # High performance, low resource consumption Very easy to use development interface 2. Install # go get -u uriModule/NetHandle\n3. Performance Testing: # 3.1 50*10000 (50 threads X 10000 requests) # 3.2 50*20000 (50 threads X 20000 requests) # 3.3 100*10000 (100 threads X 10000 requests) # "},{"id":1,"href":"/icorer_docs/doccenter/redishub/deploy/3.1/","title":"3.1 部署文档","section":"中间件部署","content":" 一. 系统设计 # 1.1 背景描述 # Redis单实例模式下，出现了机器存储过高,因此进行单实例向集群化的改造，但是由于存在以下两种原因，造成需要自研中间件：\nRedis标准集群不支持跨数据槽的MGET、MSET指令。 PHP生态缺乏并发支持，造成在M指令上性能很差。 最终我们设计RedisHub集群中间件，一方面用它来支持跨数据槽的MSET、MGET指令，一方面借助它的并发优势为PHP底层的M指令指令提高性能。\n1.2 总体设计 # 中间件的总体结构设计图如下所示:\n1.3 设计阐述 # 如图所示，PHP通过Unix Domain Socket途径和中间件进行IPC通信，然后中间件把PHP的redis请求进行处理，并转发到下游的Redis集群中，为PHP业务系统提供无缝的单实例到集群的改造，相关的网络模型如下：\n二. 组件部署文档 # RedisHub中间件 部署主要分为两个部分, 安装相关安装包 和 修改配置并运行.\n2.1 部署准备 # Linux 64bit 操作系统 RedisHub 二进制文件 调整Redishub配置文件：rh_config.yaml RedisHub进程监控工具 ,例如Supervisor 2.2 应用体安装 # 2.2.1 程序体运行 # 由于中间件是个可以方便运行的二进制体，但是我们在运行的时候需要制定配置文件路径，例如以下命令：\n./RedisHub -c /home/corerman/DATA/ICODE/GoLang/RedisHub/config/rh_config.yaml\n具体运行参数，运维伙伴可以自行调整。\n三. 配置文件详解 # 配置文件（rh_config.yaml）是这个系统运行的基础，所以这里将讲解配置文件的相关信息.\n#RedisHub YAML config file #RedisHub Net Config net: listen_uri: unix:/tmp/redis.sock #Unix Domain Socket的监听路径：PHP推荐使用这种模式 #listen_uri: tcp:10.100.183.180:16379 #TCP的监控IP及端口 #RedisHub cluster Config cluster: start_nodes: 10.54.2.8:9736,10.54.2.9:9736,10.54.2.10:9736 #Redis集群的节点，这里可以根据线上实际情况进行配置，多个只是为了保障高可用 conn_timeout: 50 #Redis节点的TCP连接超时时间 （单位：毫秒） conn_read_timeout: 50 #Redis节点的TCP读取超时时间 （单位：毫秒） conn_write_timeout: 50 #Redis节点的TCP写入超时时间 （单位：毫秒） conn_alive_timeout: 60 #Redis节点的TCP最大空闲时间 （单位：秒） conn_pool_size: 150 #针对每一个Redis集群节点的TCP连接池最大值 （单位：个） #RedisHub api config api: http_listen_address: #RedisHub log config # log 相关配置 log: # 日否开启日志 enable: true #是否开启日志功能（true 或 false） # 日志输出位置，支持std(终端) kafka # 注：std仅在调试时使用 output: \u0026#34;kafka\u0026#34; # kafka server的地址 需要修改到指定环境的kafka kafka_address: [\u0026#34;192.168.205.10:9092\u0026#34;, \u0026#34;192.168.205.11:9092\u0026#34;, \u0026#34;192.168.205.12:9092\u0026#34;] kafka_info_topic: \u0026#34;ltlog-info\u0026#34; kafka_error_topic: \u0026#34;ltlog-error\u0026#34; # 日志输出级别控制, 可省略, 默认输出到 error 级别 # 高级别的可以输出低级别的日志， 级别 trace \u0026gt; debug \u0026gt; error \u0026gt; warning \u0026gt; info # 例如 level = error时，不可输出trace和debug级别的日志 level: \u0026#34;error\u0026#34; # 日志中是否报告函数调用信息,可省略 默认为false report_call: false # 机器ip，可为空，默认自动查找 ip: \u0026#34;\u0026#34; # 机器hostname，可为空，默认自动查找 hostname: \u0026#34;\u0026#34; # app_name 默认为 RedisHub app_name: \u0026#34;RedisHub\u0026#34; # 周期上报redis执行信息 （当前是数量） heartbeat_report_second: 120 四 . 稳定性保障方案 # 4.1 进程稳定保障 # RedisHub中间件采用多线程-协程-多核编程,有很好的并发处理能力,经过较为严格的测试没出现过崩溃情况,但是对操作系统目前指抛出一个进程,为了保障RedisHub的运行稳定性 , 所以需要给予配置一个进程监控程序,可以使用Supervisor .\n4.2 更新配置 # 目前仅能通过修改配置文件来更新配置文件,如果需要更新配置,执行的操作有以下两步:\n更新配置文件 重启应用体 五. 运行校验 # 可以通过redis-cli工具连接redisHub中间件，然后测试set\\get\\mset\\mget指令的运行状况，从而判断中间件的运行状况。\n"},{"id":2,"href":"/icorer_docs/doccenter/redishub/system_access/4.1/","title":"4.1 phpredis对接redishub","section":"系统接入","content":" phpredis 对接 RedisHub 改造方案 # 一、背景描述 # phpredis进行Redis集群化改造，对接 RedisHub 中间件，并调整phpredis底层，增加业务降级逻辑。\n二、改造方案 # 2.1、Agent 连接 # 由于 RedisHub 采用 UNIX domain连接，本地部署 Agent 的方式，不存在 port端口，故 redis连接时，无法再传递该参数，需要去掉，具体连接代码如下\npublic function __construct($config, $prefix = \u0026#39;prefix:\u0026#39;) { $this-\u0026gt;prefixH = $prefix; $this-\u0026gt;redisconf = $config; $this-\u0026gt;redis = new \\Redis(); try { $this-\u0026gt;redis-\u0026gt;connect($this-\u0026gt;redisconf[\u0026#39;host\u0026#39;]); } catch (\\Exception $exception) { Logger::getInstance()-\u0026gt;error(\u0026#39;Redis agent connect error, \u0026#39; . $exception-\u0026gt;getMessage() . \u0026#39;, \u0026#39; . $this-\u0026gt;redisconf[\u0026#39;host\u0026#39;]); try { //redis集群中间件服务不可用，降级 $this-\u0026gt;redis = new \\RedisCluster(null, $this-\u0026gt;redisconf[\u0026#39;cluster\u0026#39;][\u0026#39;node\u0026#39;], $this-\u0026gt;redisconf[\u0026#39;cluster\u0026#39;][\u0026#39;timeout\u0026#39;], $this-\u0026gt;redisconf[\u0026#39;cluster\u0026#39;][\u0026#39;readTimeout\u0026#39;], $this-\u0026gt;redisconf[\u0026#39;cluster\u0026#39;][\u0026#39;persistent\u0026#39;]); } catch (\\Exception $exception) { Logger::getInstance()-\u0026gt;error(\u0026#39;RedisCluster connect error, \u0026#39; . $exception-\u0026gt;getMessage()); } } $this-\u0026gt;redis-\u0026gt;setOption(\\Redis::OPT_PREFIX, $prefix); } 我们可以看到，原先的连接方式几乎没有改动，只是去掉了port参数\n2.2、降级方案 # 针对 Agent可能产生异常的情形，我们做了降级方案，代码如上。当检测到连接异常时，采用 PHPRedis原生的集群方案，并发送错误日志，此降级对业务是无感知的\n三、过多 KEY 分片 # 为了避免 mget操作大数量级的 key，造成性能的下降，针对超过 200（可自定义）数量级的 key进行分片操作，代码如下\npublic function getMultiple($keys) { try { if (count($keys) \u0026gt; self::CHUNK_SIZE) { $data = array(); try { //针对200key以上分片读取 foreach (array_chunk($keys, self::CHUNK_SIZE) as $val) { $data = array_merge($data, $this-\u0026gt;redis-\u0026gt;mget($val)); } return $data; } catch (\\Exception $exception) { Logger::getInstance()-\u0026gt;warn(\u0026#39;redis getMultiple(mget) error, error message : \u0026#39; . $exception-\u0026gt;getMessage()); return false; } } return $this-\u0026gt;redis-\u0026gt;mget($keys); } catch (\\Exception $ex) { if (strpos($ex-\u0026gt;getMessage(), \u0026#39;protocol error\u0026#39;) !== false) { Logger::getInstance()-\u0026gt;warn(\u0026#39;protocol error : php redis getMultiple(mget) error, the key is \u0026#39; . implode($keys) . \u0026#39;. error message : \u0026#39; . $ex-\u0026gt;getMessage()); $this-\u0026gt;repconnect(); } return false; } } 我们定义了一个常量 CHUNK_SIZE，设置为 200，当 key数量大于 200 时，进行分片，每 200 key为单位，分批请求，再将结果进行合并返回，一旦中间出现任何错误，函数终止，返回 false\n四、配置文件 # 业务框架需要配合修改redis.php配置文件,deom 如下\n\u0026lt;?php // phpredis配置文件 return [ \u0026#39;cache\u0026#39; =\u0026gt; [ \u0026#39;host\u0026#39; =\u0026gt; \u0026#39;/tmp/redis.sock\u0026#39;, \u0026#39;port\u0026#39; =\u0026gt; \u0026#39;16379\u0026#39;, \u0026#39;cluster\u0026#39; =\u0026gt; [ \u0026#39;node\u0026#39; =\u0026gt; [ \u0026#39;node1\u0026#39;, \u0026#39;node2\u0026#39;, \u0026#39;node3\u0026#39;, \u0026#39;node4\u0026#39; ], \u0026#39;timeout\u0026#39; =\u0026gt; 1.5, \u0026#39;readTimeout\u0026#39; =\u0026gt; 1.5, \u0026#39;persistent\u0026#39; =\u0026gt; true ] ], \u0026#39;log\u0026#39; =\u0026gt; [ \u0026#39;host\u0026#39; =\u0026gt; \u0026#39;127.0.0.1\u0026#39;, \u0026#39;port\u0026#39; =\u0026gt; \u0026#39;6379\u0026#39; ], ]; host为Agent的UNIX domain文件地址\nport 可以不填\n新增 cluster 节点，node 为集群地址，timeout 为集群连接超时时间，readTimeout 为读取超时时间，persistent 为长连接参数\n以上参数请根据线上环境自行填写\n五、注意 # 5.1、为了应对降级方案，在使用 RedisCluster 对象的时候，目前线上版本的 PHPRdis为 5.0.0，此版本在集群模式下会有导致 php-fpm 进程崩溃的风险，需要升级到 5.0.2\n5.2、为了提升原生 RedisCluster 的性能，需要在 php.ini 文件中加上 redis.clusters.cache_slots = 1\n"},{"id":3,"href":"/icorer_docs/doccenter/redistun/develop/icefiredb-crdt-kv/","title":"icefiredb-crdt-kv","section":"Develop","content":" icefiredb-crdt-kv # Project introduction # The IceFireDB-CRDT-KV engine can support decentralized P2P networking, data synchronization and consistency between nodes. It is a component of the IceFireDB software ecosystem, thanks to the open source of IPFS.\nFeatures # Easy access to P2P data consistency function Stable decentralized networking function Friendly program access interface Installing # go get -u github.com/IceFireDB/icefiredb-crdt-kv Example # package main import ( \u0026#34;bufio\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; icefiredb_crdt_kv \u0026#34;github.com/IceFireDB/icefiredb-crdt-kv/kv\u0026#34; badger2 \u0026#34;github.com/dgraph-io/badger\u0026#34; \u0026#34;github.com/ipfs/go-datastore/query\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) func main() { ctx := context.TODO() log := logrus.New() db, err := icefiredb_crdt_kv.NewCRDTKeyValueDB(ctx, icefiredb_crdt_kv.Config{ NodeServiceName: \u0026#34;icefiredb-crdt-kv\u0026#34;, DataSyncChannel: \u0026#34;icefiredb-crdt-kv-data\u0026#34;, NetDiscoveryChannel: \u0026#34;icefiredb-crdt-kv-net\u0026#34;, Namespace: \u0026#34;test\u0026#34;, Logger: log, }) if err != nil { panic(err) } defer db.Close() fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { text := scanner.Text() fields := strings.Fields(text) if len(fields) == 0 { fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) continue } cmd := fields[0] switch cmd { case \u0026#34;exit\u0026#34;, \u0026#34;quit\u0026#34;: return case \u0026#34;get\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } val, err := db.Get(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(string(val)) case \u0026#34;put\u0026#34;: if len(fields) \u0026lt; 3 { printVal(\u0026#34;Missing parameters\u0026#34;) continue } printVal(db.Put(ctx, []byte(fields[1]), []byte(fields[2]))) case \u0026#34;delete\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } printVal(db.Delete(ctx, []byte(fields[1]))) case \u0026#34;has\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } is, err := db.Has(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(is) case \u0026#34;list\u0026#34;: result, err := db.Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;query\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } //fmt.Println(fields[1], len(fields[1])) q := query.Query{ //Prefix: fields[1], Filters: []query.Filter{ query.FilterKeyPrefix{ Prefix: fields[1], }, }, } result, err := db.Query(ctx, q) if err != nil { printVal(err) continue } //time.Sleep(time.Second) for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;connect\u0026#34;: // 主动连接 if len(fields) \u0026lt; 2 { printVal(\u0026#34;Missing connection address\u0026#34;) continue } err = db.Connect(fields[1]) if err == nil { printVal(\u0026#34;connection succeeded!\u0026#34;) } else { printVal(err) } case \u0026#34;slist\u0026#34;: result, err := db.Store().Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;bquery\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() prefix := []byte(fields[1]) for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) case \u0026#34;blist\u0026#34;: db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() for it.Rewind(); it.Valid(); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) default: printVal(\u0026#34;\u0026#34;) } } } func printVal(v interface{}) { fmt.Printf(\u0026#34;%v\\n\u0026gt; \u0026#34;, v) } RoadMap # Optimize project structure. Encapsulates the kv engine layer for easy reference by upper-layer applications. "},{"id":4,"href":"/icorer_docs/doccenter/redistun/project-comparison/orbitdb/","title":"OrbitDB","section":"Project Comparison","content":" Compared with OrbitDB # OrbitDB is a serverless, distributed, peer-to-peer database.\nOrbitDB uses IPFS as its data storage and IPFS Pubsub and uses CRDTs to automatically sync databases with peers, achieving strong eventual consistency - when all updates are eventually received, all nodes will have the same state.\nIceFireDB is a database built for web3 and web2,The core mission of the project is to help applications quickly achieve decentralization,built for Data dao.\nDatabase OrbitDB IceFireDB system target P2P Databases A decentralized database platform built for Data dao. storage engine support IPFS goleveldb、badger、IPFS、CRDT、IPFS-LOG、OSS network support type P2P P2P、RAFT、NATS Data type support KV、PubSub KV、Strings、Hashes、Lists、Sorted Sets、Sets、SQL、PubSub Software integration method Software library integration Software library integration, binary software integration、web3 platform integration web3 support No smart contract plan Smart contracts are being supported、Build data dao database platform computer language used to implement Javascript Go Ecological client language Javascript Any client that supports the redis、mysql protocol Thanks OrbitDB # During the construction of IceFireDB, we learned a lot of excellent ideas from OrbitDB, and we stood on the shoulders of OrbitDB giants to move forward.\n"},{"id":5,"href":"/icorer_docs/doccenter/ak-2019/architecture/overview/","title":"OverView","section":"Architecture","content":" OverView # "},{"id":6,"href":"/icorer_docs/doccenter/logdarts/architecture/overview/","title":"OverView","section":"Architecture","content":" OverView # "},{"id":7,"href":"/icorer_docs/doccenter/pulseflow/architecture/overview/","title":"OverView","section":"Architecture","content":" OverView # "},{"id":8,"href":"/icorer_docs/doccenter/redishub/designs/overview/","title":"OverView","section":"系统设计","content":" OverView # "},{"id":9,"href":"/icorer_docs/doccenter/redistun/designs/overview/","title":"OverView","section":"Designs","content":" OverView # "},{"id":10,"href":"/icorer_docs/doccenter/redishub/","title":"RedisHub","section":"项目总览","content":"RedisHub 是一款Redis集群中间件，帮助PHP环境平滑迁移Redis集群模式，通过中间件可以解耦业务代码和redis集群之间的关系，降低开发人员对于redis集群的理解心智。中间件实现高性能网络通信、RESP协议与Redis集群协议解析，实现集群模式下的MSET/MGET指令跨槽及并发流水吞吐，为PHP请求远程Redis集群提供更高的性能及稳定性。\n一. 总体设计 # RedisHub在组件设计上分为以下几部分:\nAgent配置解析器：负责对于配置文件进行解析（后续增加统一配置中心的支持） 通信组件: 包括UNIX本地监听组 和 远端Redis集群TCP长连接及连接池。 协议分析器: 提供稳定的Redis协议解析及组装功能组件。 协议拦截器: 主要对某些Redis命令进行拦截,调用协议插件组进行功能扩展. 协议插件组: 为协议分析器添加一系列插件,对通信进行优化和功能扩展，例如集群的mset、mget、del操作。 容灾器: 为Agent运行提供必要的安全保障,主要包括进程资源监控,迭代更新监控. 具体的组件总体架构如下图: 二. 网络代理设计 # RedisHub很重要的是网络代理部分,在网络代理方面由三部分组成.\n第一部分是对远程redis集群的连接池. 第二部分是对本地众多php-fpm客户端的UNIX请求连接管理. 第三部分是对这三端之间redis通信协议进行兼容. "},{"id":11,"href":"/icorer_docs/doccenter/redishub/designs/1.1/","title":"1.1基础运行框架","section":"系统设计","content":" 一. 基础运行框架 # 1.1 设计概述 # 基础运行框架也可以成为程序组织结构,一个项目必须要有足够清晰的工程文件结构. 在清晰的工程结构下,系统的各个模块才会有适合自己的存储位置,从存储位置可以上升为代码的调用路径,好的基础运行框架,既可以让系统的源码结构清楚很多,也可以降低模块间的耦合度,提高系统的稳定性.\n1.2 工程框架组成部分 # 在此项目工程结构中,最外层的文件结构主要包括以下几部分, 项目工程一级文件结构如下图所示:\ncmd目录 : 此目录是工程运行体的源码目录,也就是最终会编译出来的可执行体的源码目录,这个目录包括 proxy,daemon目录. config目录 : 配置文件存储目录. deploy目录 : 工程部署目录,里面会包含一些部署脚本和部署工具. doc目录 : 文档目录 ,里面包括一系列的子文件夹,用来保存每个子功能的描述文档. example目录 : 样例目录,对于一些对外提供的功能模块做一些功能示例. script目录 : 脚本目录,里面的脚本相对与deploy目录里的脚本更偏向于一些模块测试脚本. src目录 : 模块源代码目录, 这个目录是工程框架中最重要的一部分,主要包括工程的模块元素,其中包括models 、pkg、 proxy 、redishub、 redis、util目录,后面会做更加详细的阐述. vender目录 : 外部库目录,因为外部的包不断更新,这里的vender更偏向于把适合工程的外部依赖库进行镜像保存,主要是为了提高工程的稳定性. 二. 重要部分拆分设计 # 2.1 cmd目录 # 这个是运行体目录,主要存储main包结构的文件源码,目前里面包含两部分:\nRedisHub目录 : RedisHub主程序体 2.2 SRC目录结构 # SRC目录的结构如下图:\n这个目录会包含一些子目录文件夹,主要存储功能包级别的源码文件及源码目录,主要包括:\nmodels目录 : 数据结构目录,存储系统用到的数据结构相关定义 proxy目录 : 代理模块,这个模块专门储存redis代理相关的功能,既包括本地监听级别的backend,也包括协议解析部分的redis目录,还包括plugin插件目录. redhub目录: 这个目录是核心目录,主要的工作任务在于承上启下,因为在src目录下基础单元都是一个个独立的包,但是如何被cmd层的代码顺利调用,还需要redtun在src功能包基础上抽象cmd层调用的操作单元. utils目录 : 框架的套件目录,主要用来存储框架所使用的一系列套件包,例如errors , log , math , redis-client , trace , unsafe , rpc , sync , resolver , usage 等 pkg目录：包含codis redis协议解析器 redis目录：包含redis集群核心功能代码。 "},{"id":12,"href":"/icorer_docs/doccenter/redishub/module_test/2.2/","title":"2.2 中间件完整测试","section":"系统模块测试","content":" 零、亿级数据压测 # // 测试脚本 \u0026lt;?php $redis = new Redis(); try { //connect 是 异常级别 $redis-\u0026gt;connect(\u0026#39;/tmp/redis.socks\u0026#39;); $ret = $redis-\u0026gt;set(\u0026#34;hello\u0026#34;, \u0026#34;helloworld\u0026#34;,2); var_dump($ret); $ret = $redis-\u0026gt;get(\u0026#34;hello\u0026#34;); var_dump($ret); $ret = $redis-\u0026gt;del(\u0026#34;hello\u0026#34;); var_dump($ret); $ret = $redis-\u0026gt;mset([\u0026#34;test1\u0026#34; =\u0026gt; \u0026#39;value1\u0026#39;, \u0026#34;test2\u0026#34; =\u0026gt; \u0026#39;value2\u0026#39;]); var_dump($ret); $ret = $redis-\u0026gt;del(\u0026#34;test1\u0026#34;, \u0026#34;test2\u0026#34;, \u0026#34;test3\u0026#34;); var_dump($ret); $ret = $redis-\u0026gt;mget([\u0026#34;test1\u0026#34;, \u0026#34;test2\u0026#34;, \u0026#34;test3\u0026#34;, \u0026#34;test4\u0026#34;, \u0026#34;test5\u0026#34;, \u0026#34;test6\u0026#34;, \u0026#34;test7\u0026#34;, \u0026#34;test8\u0026#34;, \u0026#34;test9\u0026#34;, \u0026#34;test10\u0026#34;, \u0026#34;test11\u0026#34;, \u0026#34;test12\u0026#34;, \u0026#34;test13\u0026#34;, \u0026#34;test14\u0026#34;, \u0026#34;test15\u0026#34;, \u0026#34;test16\u0026#34;, \u0026#34;test17\u0026#34;, \u0026#34;test18\u0026#34;, \u0026#34;test19\u0026#34;, \u0026#34;test20\u0026#34;]); var_dump($ret); $ret = $redis-\u0026gt;rPush(\u0026#34;push\u0026#34;,\u0026#34;key1\u0026#34;,2019,\u0026#34;key2\u0026#34;); var_dump($ret); $val1 = $redis-\u0026gt;lPop(\u0026#34;push\u0026#34;); var_dump($val1); $val1 = $redis-\u0026gt;lPop(\u0026#34;push\u0026#34;); var_dump($val1); $val1 = $redis-\u0026gt;lPop(\u0026#34;push\u0026#34;); var_dump($val1); } catch (\\Exception $ex) { var_dump($ex-\u0026gt;getMessage()); } # 终端输出 ______ _ _ _ _ _ (_____ \\ | (_) (_) (_) | | _____) )_____ __| |_ ___ _______ _ _| |__ | __ /| ___ |/ _ | |/___) ___ | | | | _ \\ | | \\ \\| ____( (_| | |___ | | | | |_| | |_) ) |_| |_|_____)\\____|_(___/|_| |_|____/|____/ 2019/09/15 09:40:27 main.go:69: UNIX-Server: /tmp/redis.socks {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;RedisHub Heart-Beat-Report running.\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T09:40:27.222+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;RedisHub-UNIX begin running\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T09:40:27.222+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 1978910\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T09:42:27.222+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 3674420\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T09:44:27.222+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 7971749\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T09:46:27.223+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 13502078\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T09:48:27.223+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 19082608\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T09:50:27.224+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 24676646\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T09:52:27.224+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 30279296\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T09:54:27.225+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 35869386\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T09:56:27.225+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 41462169\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T09:58:27.226+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 47037700\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:00:27.227+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 52639532\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:02:27.227+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 58224711\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:04:27.229+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 63814850\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:06:27.229+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 69385735\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:08:27.229+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 74981851\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:10:27.230+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 80565352\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:12:27.230+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 86114343\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:14:27.231+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 91721982\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:16:27.231+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 97338179\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:18:27.231+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 102913090\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:20:27.231+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 102913090\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:22:27.232+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 102913090\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:24:27.232+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 102913090\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:26:27.233+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 102913100\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:28:27.233+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 102913270\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:30:27.340+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 102913270\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:32:27.341+0800\u0026#34;} {\u0026#34;appName\u0026#34;:\u0026#34;RedisHub\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;corerman-WorkStation\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;heartbeat report info: command parse count : 102913270\u0026#34;,\u0026#34;timestamp\u0026#34;:\u0026#34;2019-09-15T10:34:27.341+0800\u0026#34;} 一、Redis基准压测性能对比 # corerman@corerman-WorkStation:~$ redis-benchmark -p 6379 -t get,set,rpush,lpop,setex,psetex,del -n 200000 -c 50 -q SET: 129785.85 requests per second GET: 128865.98 requests per second RPUSH: 131839.16 requests per second LPOP: 131061.59 requests per second corerman@corerman-WorkStation:~$ redis-benchmark -s /tmp/redis.socks -t get,set,rpush,lpop,setex,psetex,del -n 200000 -c 50 -q SET: 116754.23 requests per second GET: 105932.20 requests per second RPUSH: 116686.12 requests per second LPOP: 105652.41 requests per second 通过上面可以看出，SET、GET、RPUSH、LPOP性能损耗不大\n二. 中间件和标准集群下，phpredis的表现及性能 # phpredis 5.0.2版本 需要在php.ini 中增加 redis.clusters.cache_slots = 1，开启redis集群的slots信息缓存，总而提升性能。\n2.1 SET指令 （OK） # 2.1.1 中间件 # \u0026lt;?php $redis = new Redis(); try{ //connect 是 异常级别 $redis-\u0026gt;connect(\u0026#39;/tmp/redis.socks\u0026#39;); $status = $redis-\u0026gt;set(\u0026#34;test\u0026#34;,\u0026#34;testval\u0026#34;); var_dump($status); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：bool(true)\n2.1.2 标准集群 # \u0026lt;?php try{ //connect 是 异常级别 $redisCluster = new RedisCluster(NULL, Array(\u0026#34;127.0.0.1:9001\u0026#34;), 1.5, 1.5, true); $status = $redisCluster-\u0026gt;set(\u0026#34;test\u0026#34;,\u0026#34;testval\u0026#34;); var_dump($status); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：bool(true)\n2.1.3 压测数据对比 # #中间件 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_agent.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_agent.php Document Length: 11 bytes Concurrency Level: 20 Time taken for tests: 6.229 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 19000000 bytes HTML transferred: 1100000 bytes Requests per second: 16054.15 [#/sec] (mean) Time per request: 1.246 [ms] (mean) Time per request: 0.062 [ms] (mean, across all concurrent requests) Transfer rate: 2978.80 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 2 Processing: 0 1 2.9 1 213 Waiting: 0 1 2.9 1 213 Total: 0 1 2.9 1 213 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 2 95% 2 98% 3 99% 4 100% 213 (longest request) # phpredis直连标准集群 ： 开启slot缓存 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_cluster.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_cluster.php Document Length: 11 bytes Concurrency Level: 20 Time taken for tests: 5.609 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 19000000 bytes HTML transferred: 1100000 bytes Requests per second: 17829.52 [#/sec] (mean) Time per request: 1.122 [ms] (mean) Time per request: 0.056 [ms] (mean, across all concurrent requests) Transfer rate: 3308.21 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 1 Processing: 0 1 2.0 1 213 Waiting: 0 1 2.0 1 212 Total: 0 1 2.0 1 213 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 2 95% 2 98% 2 99% 3 100% 213 (longest request) # phpredis直连标准集群 ： 未开启slot缓存 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_cluster.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_cluster.php Document Length: 11 bytes Concurrency Level: 20 Time taken for tests: 10.369 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 19000000 bytes HTML transferred: 1100000 bytes Requests per second: 9643.96 [#/sec] (mean) Time per request: 2.074 [ms] (mean) Time per request: 0.104 [ms] (mean, across all concurrent requests) Transfer rate: 1789.41 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 3.2 0 1018 Processing: 1 2 4.1 2 214 Waiting: 1 2 4.1 2 214 Total: 1 2 5.2 2 1021 Percentage of the requests served within a certain time (ms) 50% 2 66% 2 75% 2 80% 2 90% 2 95% 2 98% 3 99% 3 100% 1021 (longest request) 2.2 SETEX指令 （OK） # 2.2.1 中间件 # \u0026lt;?php $redis = new Redis(); try{ //connect 是 异常级别 $redis-\u0026gt;connect(\u0026#39;/tmp/redis.socks\u0026#39;); $status = $redis-\u0026gt;set(\u0026#34;test\u0026#34;,\u0026#34;testval\u0026#34;，5); //内部调用setex var_dump($status); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：bool(true)\n2.2.2 标准集群 # \u0026lt;?php try{ //connect 是 异常级别 $redis = new RedisCluster(NULL, Array(\u0026#34;127.0.0.1:9001\u0026#34;), 1.5, 1.5, true); $status = $redis-\u0026gt;set(\u0026#34;test\u0026#34;,\u0026#34;testval\u0026#34;，5); //内部调用setex var_dump($status); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：bool(true)\n2.2.3 压测数据对比 # # 中间件 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_agent.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_agent.php Document Length: 11 bytes Concurrency Level: 20 Time taken for tests: 6.258 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 19000000 bytes HTML transferred: 1100000 bytes Requests per second: 15979.34 [#/sec] (mean) Time per request: 1.252 [ms] (mean) Time per request: 0.063 [ms] (mean, across all concurrent requests) Transfer rate: 2964.92 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 1 Processing: 0 1 2.6 1 211 Waiting: 0 1 2.6 1 211 Total: 0 1 2.6 1 211 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 2 95% 2 98% 3 99% 3 100% 211 (longest request) # 标准集群 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_cluster.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_cluster.php Document Length: 11 bytes Concurrency Level: 20 Time taken for tests: 5.600 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 19000000 bytes HTML transferred: 1100000 bytes Requests per second: 17858.46 [#/sec] (mean) Time per request: 1.120 [ms] (mean) Time per request: 0.056 [ms] (mean, across all concurrent requests) Transfer rate: 3313.58 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 1 Processing: 0 1 2.0 1 209 Waiting: 0 1 2.0 1 209 Total: 0 1 2.0 1 210 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 2 95% 2 98% 2 99% 3 100% 210 (longest request) 2.3 PSETEX指令 （OK） # 2.3.1 中间件 # \u0026lt;?php $redis = new Redis(); try{ //connect 是 异常级别 $redis-\u0026gt;connect(\u0026#39;/tmp/redis.socks\u0026#39;); $status = $redis-\u0026gt;set(\u0026#34;test\u0026#34;,\u0026#34;testval\u0026#34;, [\u0026#39;xx\u0026#39;, \u0026#39;px\u0026#39;=\u0026gt;1000]); var_dump($status); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：bool(true)\n2.3.2 标准集群 # \u0026lt;?php try{ //connect 是 异常级别 $redisCluster = new RedisCluster(NULL, Array(\u0026#34;127.0.0.1:9001\u0026#34;), 1.5, 1.5, true); $status = $redisCluster-\u0026gt;set(\u0026#34;test\u0026#34;,\u0026#34;testval\u0026#34;, [\u0026#39;xx\u0026#39;, \u0026#39;px\u0026#39;=\u0026gt;1000]); var_dump($status); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：bool(true)\n2.3.3 压测数据对比 # # 中间件 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_agent.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_agent.php Document Length: 11 bytes Concurrency Level: 20 Time taken for tests: 6.265 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 19000000 bytes HTML transferred: 1100000 bytes Requests per second: 15962.77 [#/sec] (mean) Time per request: 1.253 [ms] (mean) Time per request: 0.063 [ms] (mean, across all concurrent requests) Transfer rate: 2961.84 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 1 Processing: 0 1 2.9 1 213 Waiting: 0 1 2.9 1 213 Total: 0 1 2.9 1 213 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 2 95% 2 98% 3 99% 4 100% 213 (longest request) # 标准集群 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_cluster.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_cluster.php Document Length: 11 bytes Concurrency Level: 20 Time taken for tests: 5.696 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 19000000 bytes HTML transferred: 1100000 bytes Requests per second: 17555.61 [#/sec] (mean) Time per request: 1.139 [ms] (mean) Time per request: 0.057 [ms] (mean, across all concurrent requests) Transfer rate: 3257.39 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 3 Processing: 0 1 1.8 1 214 Waiting: 0 1 1.8 1 214 Total: 0 1 1.8 1 214 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 2 95% 2 98% 3 99% 4 100% 214 (longest request) 2.4 GET指令 （数据存在：1KB 标准数据） （OK） # 2.4.1 中间件 # \u0026lt;?php $redis = new Redis(); try{ //connect 是 异常级别 $redis-\u0026gt;connect(\u0026#39;/tmp/redis.socks\u0026#39;); $ret = $redis-\u0026gt;get(\u0026#34;test\u0026#34;); var_dump($ret); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 能够成功获取到1KB数据。\n2.4.2 标准集群 # \u0026lt;?php try{ //connect 是 异常级别 $redis = new RedisCluster(NULL, Array(\u0026#34;127.0.0.1:9001\u0026#34;), 1.5, 1.5, true); $ret = $redis-\u0026gt;get(\u0026#34;test\u0026#34;); var_dump($ret); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 能够成功获取到1KB数据。\n2.4.3 压测数据对比 # # 中间件 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_agent.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_agent.php Document Length: 0 bytes Concurrency Level: 20 Time taken for tests: 6.025 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 17900000 bytes HTML transferred: 0 bytes Requests per second: 16597.96 [#/sec] (mean) Time per request: 1.205 [ms] (mean) Time per request: 0.060 [ms] (mean, across all concurrent requests) Transfer rate: 2901.40 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 2 Processing: 0 1 2.8 1 213 Waiting: 0 1 2.8 1 213 Total: 0 1 2.8 1 214 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 2 95% 2 98% 3 99% 3 100% 214 (longest request) # 标准集群 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_cluster.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_cluster.php Document Length: 0 bytes Concurrency Level: 20 Time taken for tests: 5.314 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 17900000 bytes HTML transferred: 0 bytes Requests per second: 18818.88 [#/sec] (mean) Time per request: 1.063 [ms] (mean) Time per request: 0.053 [ms] (mean, across all concurrent requests) Transfer rate: 3289.63 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 2 Processing: 0 1 1.8 1 213 Waiting: 0 1 1.8 1 212 Total: 0 1 1.8 1 213 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 1 95% 2 98% 2 99% 3 100% 213 (longest request) 2.5 GET指令 （数据不存在） （OK） # 2.5.1 中间件 # \u0026lt;?php $redis = new Redis(); try{ //connect 是 异常级别 $redis-\u0026gt;connect(\u0026#39;/tmp/redis.socks\u0026#39;); $ret = $redis-\u0026gt;get(\u0026#34;test\u0026#34;); var_dump($ret); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：bool(false)\n2.5.2 标准集群 # \u0026lt;?php try{ //connect 是 异常级别 $redis = new RedisCluster(NULL, Array(\u0026#34;127.0.0.1:9001\u0026#34;), 1.5, 1.5, true); $ret = $redis-\u0026gt;get(\u0026#34;test\u0026#34;); var_dump($ret); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：bool(false)\n2.5.3 压测数据对比 # # 中间件 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_agent.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_agent.php Document Length: 12 bytes Concurrency Level: 20 Time taken for tests: 6.012 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 19100000 bytes HTML transferred: 1200000 bytes Requests per second: 16632.40 [#/sec] (mean) Time per request: 1.202 [ms] (mean) Time per request: 0.060 [ms] (mean, across all concurrent requests) Transfer rate: 3102.33 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 2 Processing: 0 1 2.9 1 213 Waiting: 0 1 2.9 1 213 Total: 0 1 2.9 1 213 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 2 95% 2 98% 3 99% 3 100% 213 (longest request) # 标准集群 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_cluster.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_cluster.php Document Length: 12 bytes Concurrency Level: 20 Time taken for tests: 5.313 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 19100000 bytes HTML transferred: 1200000 bytes Requests per second: 18822.79 [#/sec] (mean) Time per request: 1.063 [ms] (mean) Time per request: 0.053 [ms] (mean, across all concurrent requests) Transfer rate: 3510.89 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 1 Processing: 0 1 2.0 1 212 Waiting: 0 1 2.0 1 212 Total: 0 1 2.0 1 213 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 1 95% 2 98% 2 99% 3 100% 213 (longest request) 2.6 RPUSH指令 （异常情况）（OK） # 2.6.1 中间件 # \u0026lt;?php $redis = new Redis(); try{ //connect 是 异常级别 $redis-\u0026gt;connect(\u0026#39;/tmp/redis.socks\u0026#39;); $ret = $redis-\u0026gt;rPush(\u0026#34;test\u0026#34;,\u0026#34;val1\u0026#34;); //由于test 键已经和上面的set冲突，所以会出错 var_dump($ret); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：bool(false)\n2.6.2 标准集群 # \u0026lt;?php try{ //connect 是 异常级别 $redis = new RedisCluster(NULL, Array(\u0026#34;127.0.0.1:9001\u0026#34;), 1.5, 1.5, true); $ret = $redis-\u0026gt;rPush(\u0026#34;test\u0026#34;,\u0026#34;val1\u0026#34;); var_dump($ret); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：bool(false)\n2.6.3 压测数据对比 # # 中间件 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_agent.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_agent.php Document Length: 12 bytes Concurrency Level: 20 Time taken for tests: 5.777 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 19100000 bytes HTML transferred: 1200000 bytes Requests per second: 17309.96 [#/sec] (mean) Time per request: 1.155 [ms] (mean) Time per request: 0.058 [ms] (mean, across all concurrent requests) Transfer rate: 3228.71 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 2 Processing: 0 1 2.4 1 213 Waiting: 0 1 2.4 1 213 Total: 0 1 2.4 1 213 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 2 95% 2 98% 2 99% 3 100% 213 (longest request) # 标准集群 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_cluster.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_cluster.php Document Length: 12 bytes Concurrency Level: 20 Time taken for tests: 5.570 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 19100000 bytes HTML transferred: 1200000 bytes Requests per second: 17952.40 [#/sec] (mean) Time per request: 1.114 [ms] (mean) Time per request: 0.056 [ms] (mean, across all concurrent requests) Transfer rate: 3348.54 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 1 Processing: 0 1 1.9 1 210 Waiting: 0 1 1.9 1 210 Total: 0 1 1.9 1 210 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 1 95% 2 98% 2 99% 3 100% 210 (longest request) 2.7 RPUSH指令 （成功情况）（OK） # 2.7.1 中间件 # \u0026lt;?php $redis = new Redis(); try{ //connect 是 异常级别 $redis-\u0026gt;connect(\u0026#39;/tmp/redis.socks\u0026#39;); $ret = $redis-\u0026gt;rPush(\u0026#34;pushtest1\u0026#34;,\u0026#34;val1\u0026#34;); var_dump($ret); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：int(1)\n2.7.2 标准集群 # \u0026lt;?php try{ //connect 是 异常级别 $redis = new RedisCluster(NULL, Array(\u0026#34;127.0.0.1:9001\u0026#34;), 1.5, 1.5, true); $ret = $redis-\u0026gt;rPush(\u0026#34;pushtest2\u0026#34;,\u0026#34;val1\u0026#34;); var_dump($ret); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：int(1)\n2.7.3 压测数据对比 # # 中间件 压测后计数正常 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_agent.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_agent.php Document Length: 7 bytes Concurrency Level: 20 Time taken for tests: 6.132 seconds Complete requests: 100000 Failed requests: 99992 (Connect: 0, Receive: 0, Length: 99992, Exceptions: 0) Total transferred: 18988900 bytes HTML transferred: 1088900 bytes Requests per second: 16307.69 [#/sec] (mean) Time per request: 1.226 [ms] (mean) Time per request: 0.061 [ms] (mean, across all concurrent requests) Transfer rate: 3024.07 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 1 Processing: 0 1 1.7 1 213 Waiting: 0 1 1.7 1 213 Total: 0 1 1.7 1 213 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 2 95% 2 98% 3 99% 3 100% 213 (longest request) # 标准集群 压测后计数正常 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_cluster.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_cluster.php Document Length: 7 bytes Concurrency Level: 20 Time taken for tests: 5.558 seconds Complete requests: 100000 Failed requests: 99991 (Connect: 0, Receive: 0, Length: 99991, Exceptions: 0) Total transferred: 18988895 bytes HTML transferred: 1088895 bytes Requests per second: 17992.50 [#/sec] (mean) Time per request: 1.112 [ms] (mean) Time per request: 0.056 [ms] (mean, across all concurrent requests) Transfer rate: 3336.50 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 3 Processing: 0 1 2.3 1 213 Waiting: 0 1 2.3 1 213 Total: 0 1 2.4 1 213 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 1 95% 2 98% 2 99% 3 100% 213 (longest request) 2.8 LPOP指令（OK） # 2.8.1 中间件 # \u0026lt;?php $redis = new Redis(); try{ //connect 是 异常级别 $redis-\u0026gt;connect(\u0026#39;/tmp/redis.socks\u0026#39;); $ret = $redis-\u0026gt;lPop(\u0026#34;pushtest1\u0026#34;); var_dump($ret); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 结果：可以正常消费，并且当没有数据后 返回false\n2.8.2 标准集群 # \u0026lt;?php try{ //connect 是 异常级别 $redis = new RedisCluster(NULL, Array(\u0026#34;127.0.0.1:9001\u0026#34;), 1.5, 1.5, true); $ret = $redis-\u0026gt;lPop(\u0026#34;pushtest2\u0026#34;); var_dump($ret); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 结果：可以正常消费，并且当没有数据后 返回false\n2.8.3 压测数据对比 # # 中间件 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_agent.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_agent.php Document Length: 17 bytes Concurrency Level: 20 Time taken for tests: 5.839 seconds Complete requests: 100000 Failed requests: 97496 (Connect: 0, Receive: 0, Length: 97496, Exceptions: 0) Total transferred: 19112520 bytes HTML transferred: 1212520 bytes Requests per second: 17126.52 [#/sec] (mean) Time per request: 1.168 [ms] (mean) Time per request: 0.058 [ms] (mean, across all concurrent requests) Transfer rate: 3196.59 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 1 Processing: 0 1 2.7 1 213 Waiting: 0 1 2.7 1 213 Total: 0 1 2.7 1 213 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 2 95% 2 98% 3 99% 3 100% 213 (longest request) # 标准集群 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_cluster.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_cluster.php Document Length: 17 bytes Concurrency Level: 20 Time taken for tests: 5.419 seconds Complete requests: 100000 Failed requests: 47803 (Connect: 0, Receive: 0, Length: 47803, Exceptions: 0) Total transferred: 19360985 bytes HTML transferred: 1460985 bytes Requests per second: 18453.22 [#/sec] (mean) Time per request: 1.084 [ms] (mean) Time per request: 0.054 [ms] (mean, across all concurrent requests) Transfer rate: 3488.99 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 1 Processing: 0 1 1.8 1 209 Waiting: 0 1 1.8 1 209 Total: 0 1 1.8 1 209 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 1 95% 2 98% 2 99% 3 100% 209 (longest request) 2.9 MGET指令（OK） # 2.9.1 中间件 # \u0026lt;?php $redis = new Redis(); try{ //connect 是 异常级别 $redis-\u0026gt;connect(\u0026#39;/tmp/redis.socks\u0026#39;); $ret = $redis-\u0026gt;mget([\u0026#34;test1\u0026#34;,\u0026#34;test2\u0026#34;,\u0026#34;test3\u0026#34;,\u0026#34;test4\u0026#34;,\u0026#34;test5\u0026#34;,\u0026#34;test6\u0026#34;,\u0026#34;test7\u0026#34;,\u0026#34;test8\u0026#34;,\u0026#34;test9\u0026#34;,\u0026#34;test10\u0026#34;,\u0026#34;test11\u0026#34;,\u0026#34;test12\u0026#34;,\u0026#34;test13\u0026#34;,\u0026#34;test14\u0026#34;,\u0026#34;test15\u0026#34;,\u0026#34;test16\u0026#34;,\u0026#34;test17\u0026#34;,\u0026#34;test18\u0026#34;,\u0026#34;test19\u0026#34;,\u0026#34;test20\u0026#34;]); var_dump($ret); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：\narray(20) { [0]=\u0026gt; string(6) \u0026#34;value1\u0026#34; [1]=\u0026gt; string(6) \u0026#34;value2\u0026#34; [2]=\u0026gt; bool(false) [3]=\u0026gt; bool(false) [4]=\u0026gt; bool(false) [5]=\u0026gt; bool(false) [6]=\u0026gt; bool(false) [7]=\u0026gt; bool(false) [8]=\u0026gt; bool(false) [9]=\u0026gt; bool(false) [10]=\u0026gt; bool(false) [11]=\u0026gt; bool(false) [12]=\u0026gt; bool(false) [13]=\u0026gt; bool(false) [14]=\u0026gt; bool(false) [15]=\u0026gt; bool(false) [16]=\u0026gt; bool(false) [17]=\u0026gt; bool(false) [18]=\u0026gt; bool(false) [19]=\u0026gt; bool(false) } 2.9.2 标准集群 # \u0026lt;?php try{ //connect 是 异常级别 $redis = new RedisCluster(NULL, Array(\u0026#34;127.0.0.1:9001\u0026#34;), 1.5, 1.5, true); $ret = $redis-\u0026gt;mget([\u0026#34;test1\u0026#34;,\u0026#34;test2\u0026#34;,\u0026#34;test3\u0026#34;,\u0026#34;test4\u0026#34;,\u0026#34;test5\u0026#34;,\u0026#34;test6\u0026#34;,\u0026#34;test7\u0026#34;,\u0026#34;test8\u0026#34;,\u0026#34;test9\u0026#34;,\u0026#34;test10\u0026#34;,\u0026#34;test11\u0026#34;,\u0026#34;test12\u0026#34;,\u0026#34;test13\u0026#34;,\u0026#34;test14\u0026#34;,\u0026#34;test15\u0026#34;,\u0026#34;test16\u0026#34;,\u0026#34;test17\u0026#34;,\u0026#34;test18\u0026#34;,\u0026#34;test19\u0026#34;,\u0026#34;test20\u0026#34;]); var_dump($ret); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：\narray(20) { [0]=\u0026gt; string(6) \u0026#34;value1\u0026#34; [1]=\u0026gt; string(6) \u0026#34;value2\u0026#34; [2]=\u0026gt; bool(false) [3]=\u0026gt; bool(false) [4]=\u0026gt; bool(false) [5]=\u0026gt; bool(false) [6]=\u0026gt; bool(false) [7]=\u0026gt; bool(false) [8]=\u0026gt; bool(false) [9]=\u0026gt; bool(false) [10]=\u0026gt; bool(false) [11]=\u0026gt; bool(false) [12]=\u0026gt; bool(false) [13]=\u0026gt; bool(false) [14]=\u0026gt; bool(false) [15]=\u0026gt; bool(false) [16]=\u0026gt; bool(false) [17]=\u0026gt; bool(false) [18]=\u0026gt; bool(false) [19]=\u0026gt; bool(false) } 2.9.3 压测数据对比 # # 中间件 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_agent.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_agent.php Document Length: 478 bytes Concurrency Level: 20 Time taken for tests: 8.120 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 65700000 bytes HTML transferred: 47800000 bytes Requests per second: 12315.93 [#/sec] (mean) Time per request: 1.624 [ms] (mean) Time per request: 0.081 [ms] (mean, across all concurrent requests) Transfer rate: 7901.92 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 1 Processing: 0 1 2.5 1 214 Waiting: 0 1 2.5 1 214 Total: 0 2 2.5 1 215 Percentage of the requests served within a certain time (ms) 50% 1 66% 2 75% 2 80% 2 90% 2 95% 3 98% 3 99% 4 100% 215 (longest request) # 标准集群 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_cluster.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_cluster.php Document Length: 478 bytes Concurrency Level: 20 Time taken for tests: 13.117 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 65700000 bytes HTML transferred: 47800000 bytes Requests per second: 7623.54 [#/sec] (mean) Time per request: 2.623 [ms] (mean) Time per request: 0.131 [ms] (mean, across all concurrent requests) Transfer rate: 4891.28 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.0 0 1 Processing: 1 3 2.4 2 213 Waiting: 1 3 2.4 2 213 Total: 1 3 2.4 2 213 Percentage of the requests served within a certain time (ms) 50% 2 66% 3 75% 3 80% 3 90% 4 95% 4 98% 5 99% 5 100% 213 (longest request) 2.10 DEL指令（OK） # 2.10.1 中间件 # \u0026lt;?php $redis = new Redis(); try{ //connect 是 异常级别 $redis-\u0026gt;connect(\u0026#39;/tmp/redis.socks\u0026#39;); $ret = $redis-\u0026gt;mset([\u0026#34;test1\u0026#34;=\u0026gt;\u0026#39;value1\u0026#39;,\u0026#34;test2\u0026#34;=\u0026gt;\u0026#39;value2\u0026#39;]); var_dump($ret); $ret = $redis-\u0026gt;del(\u0026#34;test1\u0026#34;,\u0026#34;test2\u0026#34;,\u0026#34;test3\u0026#34;); var_dump($ret); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：bool(true) int(2)\n2.10.2 标准集群 # \u0026lt;?php try{ //connect 是 异常级别 $redis = new RedisCluster(NULL, Array(\u0026#34;127.0.0.1:9001\u0026#34;), 1.5, 1.5, true); $ret = $redis-\u0026gt;mset([\u0026#34;test1\u0026#34;=\u0026gt;\u0026#39;value1\u0026#39;,\u0026#34;test2\u0026#34;=\u0026gt;\u0026#39;value2\u0026#39;]); var_dump($ret); $ret = $redis-\u0026gt;del(\u0026#34;test1\u0026#34;,\u0026#34;test2\u0026#34;,\u0026#34;test3\u0026#34;); var_dump($ret); }catch (\\Exception $ex){ var_dump($ex-\u0026gt;getMessage()); } 运行结果：bool(true) int(2)\n2.10.3 压测数据对比 # # 中间件 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_agent.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_agent.php Document Length: 7 bytes Concurrency Level: 20 Time taken for tests: 7.240 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 18600000 bytes HTML transferred: 700000 bytes Requests per second: 13811.28 [#/sec] (mean) Time per request: 1.448 [ms] (mean) Time per request: 0.072 [ms] (mean, across all concurrent requests) Transfer rate: 2508.69 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 1 Processing: 0 1 2.6 1 215 Waiting: 0 1 2.6 1 215 Total: 0 1 2.6 1 215 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 2 80% 2 90% 2 95% 2 98% 3 99% 4 100% 215 (longest request) # 标准集群 corerman@corerman-WorkStation:~$ ab -n 100000 -c 20 http://lab.test.local/redis_cluster.php This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1807734 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking lab.test.local (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: nginx Server Hostname: lab.test.local Server Port: 80 Document Path: /redis_cluster.php Document Length: 7 bytes Concurrency Level: 20 Time taken for tests: 6.447 seconds Complete requests: 100000 Failed requests: 0 Total transferred: 18600000 bytes HTML transferred: 700000 bytes Requests per second: 15511.04 [#/sec] (mean) Time per request: 1.289 [ms] (mean) Time per request: 0.064 [ms] (mean, across all concurrent requests) Transfer rate: 2817.44 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.1 0 1 Processing: 0 1 3.1 1 216 Waiting: 0 1 3.1 1 216 Total: 0 1 3.1 1 216 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 2 95% 2 98% 3 99% 4 100% 216 (longest request) RedisHub 与 phpredis集群客户端指令效果对比 # 一. set指令 （保持一致） # 正常：true 节点损坏：报异常 clusterDown：返回异常，clusterdown异常 二、mset （保持一致） # 正常 ：true 节点损坏：报异常 clusterDown：返回异常，clusterdown异常 三、get （保持一致） # 正常：数据 节点损坏：报异常 clusterDown：返回异常，clusterdown异常 四、mget （节点损坏时有区别，redishub更合理） # 正常：返回数据 ，无数据时为false 节点损坏： phpredis全部不返回，redishub针对存活的数据节点返回数据，不存活的节点返回nil及false clusterDown：返回异常，clusterdown异常 五、DEL # 正常：返回删除数据量\n节点损坏：phpredis报异常、redishub针对存活的数据节点进行数据删除，并统计数量返回，redishub不报异常\nclusterDown：返回异常，clusterdown异常\n六.rpush指令 （保持一致） # 正常 返回：统计量\n节点损坏：报异常\nclusterDown：返回异常，clusterdown异常\n七.lpop指令 （保持一致） # 正常 返回：统计量 节点损坏：报异常 clusterDown：返回异常，clusterdown异常 "},{"id":13,"href":"/icorer_docs/doccenter/redistun/develop/icefiredb-ipfs-log/","title":"icefiredb-ipfs-log","section":"Develop","content":" icefiredb-ipfs-log # Project introduction # icefiredb-ipfs-log is a distributed immutable, operation-based conflict-free replication data structure that relies on ipfs to store data and merges each peer node data based on pubsub conflict-free. You can easily implement custom data structures such as kv, event, nosql, etc. based on icefiredb-ipfs-log.\nConflict-free log replication model\nLog A Log B | | logA.append(\u0026#34;one\u0026#34;) logB.append(\u0026#34;hello\u0026#34;) | | v v +-----+ +-------+ |\u0026#34;one\u0026#34;| |\u0026#34;hello\u0026#34;| +-----+ +-------+ | | logA.append(\u0026#34;two\u0026#34;) logB.append(\u0026#34;world\u0026#34;) | | v v +-----------+ +---------------+ |\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;| |\u0026#34;hello\u0026#34;,\u0026#34;world\u0026#34;| +-----------+ +---------------+ | | | | logA.join(logB) \u0026lt;----------+ | v +---------------------------+ |\u0026#34;one\u0026#34;,\u0026#34;hello\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;world\u0026#34;| +---------------------------+ Features # Easy access to P2P \u0026amp;\u0026amp; ipfs-log data consistency function Stable decentralized networking function Friendly program access interface Installing # go get -u github.com/IceFireDB/icefiredb-ipfs-log Example # Example of building a key-value database using icefiredb-ipfs-log # memory key-value：memory-kv leveldb kv ：leveldb-kv Use of key-value databases # Detailed usage example reference\nfunc main() { ctx := context.TODO() // disk cache directory rootPath := \u0026#34;./kvdb\u0026#34; node, api, err := iflog.CreateNode(ctx, rootPath) if err != nil { panic(err) } hostAddr, _ := ma.NewMultiaddr(fmt.Sprintf(\u0026#34;/ipfs/%s\u0026#34;, node.PeerHost.ID().Pretty())) for _, a := range node.PeerHost.Addrs() { fmt.Println(a.Encapsulate(hostAddr).String()) } log := zap.NewNop() dbname := \u0026#34;iflog-event-kv\u0026#34; ev, err := iflog.NewIpfsLog(ctx, api, dbname, \u0026amp;iflog.EventOptions{ Directory: rootPath, Logger: log, }) if err != nil { panic(err) } if err := ev.AnnounceConnect(ctx, node); err != nil { panic(err) } kvdb, err := kv.NewKeyValueDB(ctx, ev, log) if err != nil { panic(err) } // Load old data from disk if err := ev.LoadDisk(ctx); err != nil { panic(err) } kvdb.Put(ctx, \u0026#34;one\u0026#34;, \u0026#34;one\u0026#34;) kvdb.Get(\u0026#34;one\u0026#34;) kvdb.Delete(ctx, \u0026#34;one\u0026#34;) } package main import ( \u0026#34;bufio\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; icefiredb_crdt_kv \u0026#34;github.com/IceFireDB/icefiredb-crdt-kv/kv\u0026#34; badger2 \u0026#34;github.com/dgraph-io/badger\u0026#34; \u0026#34;github.com/ipfs/go-datastore/query\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) func main() { ctx := context.TODO() log := logrus.New() db, err := icefiredb_crdt_kv.NewCRDTKeyValueDB(ctx, icefiredb_crdt_kv.Config{ NodeServiceName: \u0026#34;icefiredb-crdt-kv\u0026#34;, DataSyncChannel: \u0026#34;icefiredb-crdt-kv-data\u0026#34;, NetDiscoveryChannel: \u0026#34;icefiredb-crdt-kv-net\u0026#34;, Namespace: \u0026#34;test\u0026#34;, Logger: log, }) if err != nil { panic(err) } defer db.Close() fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { text := scanner.Text() fields := strings.Fields(text) if len(fields) == 0 { fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) continue } cmd := fields[0] switch cmd { case \u0026#34;exit\u0026#34;, \u0026#34;quit\u0026#34;: return case \u0026#34;get\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } val, err := db.Get(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(string(val)) case \u0026#34;put\u0026#34;: if len(fields) \u0026lt; 3 { printVal(\u0026#34;Missing parameters\u0026#34;) continue } printVal(db.Put(ctx, []byte(fields[1]), []byte(fields[2]))) case \u0026#34;delete\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } printVal(db.Delete(ctx, []byte(fields[1]))) case \u0026#34;has\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } is, err := db.Has(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(is) case \u0026#34;list\u0026#34;: result, err := db.Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;query\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } //fmt.Println(fields[1], len(fields[1])) q := query.Query{ //Prefix: fields[1], Filters: []query.Filter{ query.FilterKeyPrefix{ Prefix: fields[1], }, }, } result, err := db.Query(ctx, q) if err != nil { printVal(err) continue } //time.Sleep(time.Second) for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;connect\u0026#34;: // 主动连接 if len(fields) \u0026lt; 2 { printVal(\u0026#34;Missing connection address\u0026#34;) continue } err = db.Connect(fields[1]) if err == nil { printVal(\u0026#34;connection succeeded!\u0026#34;) } else { printVal(err) } case \u0026#34;slist\u0026#34;: result, err := db.Store().Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;bquery\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() prefix := []byte(fields[1]) for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) case \u0026#34;blist\u0026#34;: db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() for it.Rewind(); it.Valid(); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) default: printVal(\u0026#34;\u0026#34;) } } } func printVal(v interface{}) { fmt.Printf(\u0026#34;%v\\n\u0026gt; \u0026#34;, v) } Some code reference sources # go-ipfs-log License # icefiredb-ipfs-log is under the Apache 2.0 license. See the LICENSE directory for details.\n"},{"id":14,"href":"/icorer_docs/doccenter/ak-2019/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":15,"href":"/icorer_docs/doccenter/logdarts/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":16,"href":"/icorer_docs/doccenter/pulseflow/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":17,"href":"/icorer_docs/doccenter/redistun/designs/network/","title":"Network layer","section":"Designs","content":" Network layer design # The network layer undertakes the work of inter-node networking, inter-node data distribution, and inter-node data consistency consensus. The network layer of IceFireDB is divided into two layers according to the distance of the physical network link:\nData consistency network layer for short-distance networks. Decentralized database network layer for wide-distance network. The above two different network layers are supported by different technologies and have different requirements for data consistency sensitivity and timeliness.\nParallel cluster network # The smallest component unit of the IceFireDB network cluster is a parallel cluster network guaranteed by the RAFT algorithm, rather than a decentralized P2P network.\nThe IceFireDB parallel cluster uses the RAFT algorithm to form a data consistency layer. Each cluster has a master and multiple slave nodes. They synchronize with each other and maintain the master-slave state of the cluster. Each node stores the same data and writes data. The input is undertaken by the master node, and the master node is responsible for distributing data to the slave nodes. A RAFT cluster internally guarantees the consistency of each data write.\nDecentralized network # The data jumps out of the parallel cluster network and enters the decentralized network. The IceFireDB decentralized network mainly uses P2P technology for automatic networking, and relies on the P2P PubSub communication method to synchronize data commands.\nIceFireDB\u0026rsquo;s use of P2P networking is not only in node discovery, but also provides users with decentralized publish and subscribe middleware by bridging the PubSub network to the RESP protocol. Increase the decentralization capability of the SQLite database by bridging the PubSub network to the SQL protocol.\nDecentralized log synchronization network # Relying solely on P2P and PubSub technologies cannot meet the database requirements for data decentralized synchronization and decentralized data consistency.The current popular CRDT technology can meet the final data consistency, but it cannot guarantee the synchronization order of the data, so it will castrate some data instruction functions of IceFireDB-NoSQL, and the IPFS-LOG technology very well makes up for the functional gap of the decentralized log.\nIceFireDB-NoSQL has established a decentralized log synchronization network based on IPFS-LOG technology. The decentralized IceFireDB nodes broadcast database command logs and build orderly logs to complete the function of data decentralization and consistency.\nIceGiant Network structure # IceFireDB\u0026rsquo;s RAFT has exactly the same data set within the same group of nodes, and in a larger network, we are providing the IceGiant network structure, which aims to break up different tenant database data.\nAlthough all IceGiant nodes are in the same P2P network, the structure of the network can be decomposed according to the data set, which is the data tenant isolation area, which refers to the database area used by a specific application on top of the IceGiant protocol. Different from the traditional blockchain network, IceGiant nodes are only responsible for interacting with peers operating the same data set, and are not responsible for any data of other data sets. Peer IceGiant node sets form a high-availability data storage area.\nIn-process network IO model # IceFireDB supports the following two IO models:\nGolang classic netpoll model: goroutine-per-connection, suitable for situations where the number of connections is not a bottleneck.\nRawEpoll model: that is, Reactor mode, I/O multiplexing (I/O multiplexing) + non-blocking I/O (non-blocking I/O) mode. For scenarios where there are a large number of long links between the access layer and the gateway, it is more suitable for the RawEpoll model.\n"},{"id":18,"href":"/icorer_docs/doccenter/redistun/project-comparison/threaddb/","title":"ThreadDB","section":"Project Comparison","content":" Compared with ThreadDB # ThreadDB is a serverless, distributed, peer-to-peer database.\nIceFireDB is a database built for web3 and web2,The core mission of the project is to help applications quickly achieve decentralization,built for Data dao.\nDatabase ThreadDB IceFireDB system target P2P Databases A decentralized database platform built for Data dao. storage engine support IPFS goleveldb、badger、IPFS、CRDT、IPFS-LOG、OSS network support type P2P P2P、RAFT、NATS Data type support SQL KV、Strings、Hashes、Lists、Sorted Sets、Sets、SQL、PubSub Software integration method Binary software integration Software library integration, binary software integration、web3 platform integration web3 support No smart contract plan Smart contracts are being supported、Build data dao database platform computer language used to implement Go Go Ecological client language Go Any client that supports the redis、mysql protocol Thanks ThreadDB # Thanks to ThreadDB for letting us see the excellent implementation of decentralized SQL database.\n"},{"id":19,"href":"/icorer_docs/doccenter/redishub/designs/1.2/","title":"1.2配置解析器","section":"系统设计","content":" 1.2 配置解析器设计 # Agent配置器, 主要负责解析来自网络 或者 文件系统中的配置文件,目前提供配置文件解析模块,配置解析器设计主要包括以下部分:\n配置信息元素结构. 配置解析器相关解析函数. 配置解析器对外API接口服务,允许动态更新配置. 组成结构图如下: 1.2.1 配置信息元素 # 随着系统功能的不断丰富,系统的配置项目也会越来越多,目前配置解析器主要解析的数据对象包括:\n网络配置项 : 针对中间件网络监听进行配置。 redis集群配置项：针对Redis集群进行配置。 日志配置项：目前提供kafka或日志输出，kafka支持公司标准日志。 1.2.2 配置样例 # #RedisHub YAML config file #RedisHub Net Config net: listen_uri: unix:/var/run/redis.sock #Unix Domain Socket的监听路径：PHP推荐使⽤这种模式 # listen_uri: tcp:10.100.183.180:16379 #TCP的监控IP及端⼝ #RedisHub cluster Config cluster: start_nodes:node1,node2,node3 #Redis集群的节点，这⾥可以根据线上实际情况进⾏配置，多个只是为了保障⾼可⽤ conn_timeout: 200 #Redis节点的TCP连接超时时间 （单位：毫秒） conn_read_timeout: 50 #Redis节点的TCP读取超时时间 （单位：毫秒） conn_write_timeout: 50 #Redis节点的TCP写⼊超时时间 （单位：毫秒） conn_alive_timeout: 60 #Redis节点的TCP最⼤空闲时间 （单位：秒） conn_pool_size: 200 #针对每⼀个Redis集群节点的TCP连接池最⼤值 （单位：个） #RedisHub api config api: http_listen_address: #RedisHub log config # log 相关配置 log: # 日否开启日志 enable: true # 日志输出位置，支持std(终端) kafka # 注：std仅在调试时使用 output: \u0026#34;kafka\u0026#34; # kafka server的地址 需要修改到指定环境的kafka kafka_address: [\u0026#34;10.166.7.139:9092\u0026#34;, \u0026#34;10.166.7.139:9093\u0026#34;, \u0026#34;10.166.7.139:9094\u0026#34;] kafka_info_topic: \u0026#34;ltlog-info\u0026#34; kafka_error_topic: \u0026#34;ltlog-error\u0026#34; # 日志输出级别控制, 可省略, 默认输出到 error 级别 # 高级别的可以输出低级别的日志， 级别 trace \u0026gt; debug \u0026gt; error \u0026gt; warning \u0026gt; info # 例如 level = error时，不可输出trace和debug级别的日志 level: \u0026#34;debug\u0026#34; # 日志中是否报告函数调用信息,可省略 默认为false report_call: false # 机器ip，可为空，默认自动查找 ip: \u0026#34;\u0026#34; # 机器hostname，可为空，默认自动查找 hostname: \u0026#34;\u0026#34; # app_name 默认为 RedisHub app_name: \u0026#34;RedisHub\u0026#34; # 周期上报redis执行信息 （单位：秒） heartbeat_report_second: 120 "},{"id":20,"href":"/icorer_docs/doccenter/redistun/develop/icefiredb_proxy/","title":"icefiredb-proxy","section":"Develop","content":" IceFireDB-Proxy # Project introduction # IceFireDB-Proxy is a high-performance, high-availability, and user-friendly Resp protocol cluster proxy solution. It is supporting P2P networking and is a network component in the IceFireDB ecosystem.\nFeatures # Complete data source mode support: stand-alone, cluster mode Rich command support Excellent cluster state management and failover Excellent traffic control policies: Traffic read/write separation and multi-tenant data isolation Excellent command telemetry features Bottom-fishing use of mind and base abilities that are closer to cloud native Supports P2P automatic networking, and Proxy helps traditional Redis databases achieve data decentralization. New framework for faster network, will be upgraded soon. redhub Architecture # Network Communication Model # Installing # 1. Install Go 2. git clone https://github.com/IceFireDB/IceFireDB-Proxy.git $GOPATH/src/github.com/IceFireDB/IceFireDB-Proxy 3. cd $GOPATH/src/github.com/IceFireDB/IceFireDB-Proxy 4. make Usage # Run a binary file directly, if you need to run in the background can be added to the systemd system management\n./bin/Icefiredb-proxy -c ./config/config.yaml Command support # String # APPEND BITCOUNT BITPOS DECR DECRBY DEL EXISTS GET GETBIT SETBIT GETRANGE GETSET INCR INCRBY MGET MSET SET SETEX SETEXAT SETRANGE EXPIRE EXPIREAT TTL Set # SADD SCARD SETBIT SISMEMBER SMEMBERS SPOP SRANDMEMBER SREM SSCAN List # LINDEX LINSERT LLEN LPOP LPUSH LPUSHX LRANGE LREM LSET LTRIM RPOP RPUSH RPUSHX Hash # HDEL HEXISTS HGET HGETALL HINCRBY HINCRBYFLOAT HKEYS HLEN HMGET HMSET HSCAN HSET HSETNX HSTRLEN HVALS Sorted Sets # ZADD ZCARD ZCOUNT ZINCRBY ZLEXCOUNT ZPOPMAX ZPOPMIN ZLEXCOUNT ZRANGE ZRANGEBYLEX ZRANGEBYSCORE ZRANK ZREM ZREMRANGEBYLEX ZREMRANGEBYRANK ZREMRANGEBYSCORE ZREVRANGE ZREVRANGEBYLEX ZREVRANGEBYSCORE ZREVRANK ZSCAN ZSCORE Stream # XACK XADD XCLAIM XDEL XLEN XINFO XPENDING XRANGE XREADGROUP XREVRANGE XTRIM XGROUP Others # COMMAND PING QUIT "},{"id":21,"href":"/icorer_docs/doccenter/ak-2019/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":22,"href":"/icorer_docs/doccenter/logdarts/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":23,"href":"/icorer_docs/doccenter/pulseflow/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":24,"href":"/icorer_docs/doccenter/redistun/designs/storage/","title":"Storage layer","section":"Designs","content":" Storage layer design # The storage layer is responsible for data storage, and the data storage here includes different storage media of web2 and web3. For web2, the storage media we face includes disk, OSS, and for web3, the storage media we face includes IPFS, blockchain, and smart contracts.Currently, the storage types supported by IceFireDB mainly include the following.\nEngine type describe Driver directory LevelDB LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values. Default Badger BadgerDB is an embeddable, persistent and fast key-value (KV) database written in pure Go. Badger OSS Object storage is a technology that stores and manages data in an unstructured format called objects. OSS IPFS IPFS (the InterPlanetary File System) is a hypermedia distribution protocol addressed by content and identities. It enables the creation of completely distributed applications, and in doing so aims to make the web faster, safer, and more open. IPFS CRDT-KV The IceFireDB-CRDT-KV engine can support decentralized P2P networking, data synchronization and consistency between nodes. It is a component of the IceFireDB software ecosystem, thanks to the open source of IPFS. CRDT-KV IPFS-LOG icefiredb-ipfs-log is a distributed immutable, operation-based conflict-free replication data structure that relies on ipfs to store data and merges each peer node data based on pubsub conflict-free. You can easily implement custom data structures such as kv, event, nosql, etc. based on icefiredb-ipfs-log. IPFS-LOG OrbitDB OrbitDB is a serverless, distributed, peer-to-peer database. OrbitDB uses IPFS as its data storage and IPFS Pubsub to automatically sync databases with peers. OrbitDB Storage model # The NoSQL storage layer of each individual IceGiant mainly includes the codec layer and the underlying KV storage layer. the underlying KV engine currently supports levelDB, badgerDB, IPFS and OSS, and the main data storage includes two ways:\ninstruction broadcast model based on IPFS-LOG\\CRDT_KV\\OrbitDB\nNative data storage model based on LevelDB\\Badger\\OSS\\IPFS\nMultiple IceFireDB nodes will be divided into groups according to data sets, and each group will form a highly available storage area structure.\nNoSQL storage engine # The core of each node is the database engine. By default, IceGiant node integrates KV storage engines such as levelDB, badgerDB, IPFS, OSS, etc., and implements the protocol coding layer of NoSQL on the KV storage relationship. Currently, the data storage of NoSQL mainly includes the following two ways:\nInstruction broadcast model # Based on ipfs-log,crdt and libp2p(pubsub), an immutable and operation-based conflict-free replication data model for distributed systems is implemented. Based on ipfs-log, various data structures such as event and kv are encapsulated, and multi-node database instruction broadcast is implemented based on this engine;At that bottom of IceFireDB, we abstract the variable kv engine base on badgerdb and leveldb. any node will broadcast the whole network when it is writing instruction, and the bottom driver of IceFireDB of each node will execute the broadcast instruction to ensure the final consistency of data.\nLog A Log B | | logA.append(\u0026#34;one\u0026#34;) logB.append(\u0026#34;hello\u0026#34;) | | v v +-----+ +-------+ |\u0026#34;one\u0026#34;| |\u0026#34;hello\u0026#34;| +-----+ +-------+ | | logA.append(\u0026#34;two\u0026#34;) logB.append(\u0026#34;world\u0026#34;) | | v v +-----------+ +---------------+ |\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;| |\u0026#34;hello\u0026#34;,\u0026#34;world\u0026#34;| +-----------+ +---------------+ | | | | logA.join(logB) \u0026lt;----------+ | v +---------------------------+ |\u0026#34;one\u0026#34;,\u0026#34;hello\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;world\u0026#34;| +---------------------------+ Full storage model # In addition to the first implementation mode, we are also building the structure of the second type of data, so that the complete data will grow on ipfs. At first, there is an ipfs driver in the IceFireDB driver layer, which will encode and process the upper-level commands into a unified kv data structure, store and change the value, and the generated new cid will be connected with key. However, at present, there is no key broadcast network with multiple nodes and data synchronization. When we connect with the broadcast network, we can build a data model originally grown on LevelDB\\Badger\\OSS\\IPFS.\n+-------------------------------------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv、list、hash、set | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ IceGiant Synchronizer # The storage layer of IceFireDB not only includes a complete storage server, but IceGiant Synchronizer, which is currently under construction, also belongs to the ecological software layer of the storage layer.\nIceGiant Synchronizer is an application directly above the database engine. All incoming database requests pass through IceGiant Synchronizer, which determines whether the requests should be processed, whether data writes should be propagated to other parts of the network, and whether local data should be written and customer requests should be responded to.\nIceGiant Synchronizer can also provide data write aggregation function, allowing multiple data requests to be merged and written into a single network storage request. It also allows users to cross-mix data sets between different nodes, encouraging further data decentralization, while keeping the operation overhead low.\n"},{"id":25,"href":"/icorer_docs/doccenter/redishub/designs/1.3/","title":"1.3通信组件","section":"系统设计","content":" 1.3 通信组件设计 # 通信组件目前不包括协议解析部分,这里的通信组件就是纯粹的网络通信层设计.\n1.3.1 通信组件组成部分 # 目前网络通信层分为两部分:\n本地UNIX监听组,用于高速的IPC通信 每个UNIX监听组所对应一组远端redis服务器,分别对于主从进行连接池连接. 1.3.1 通信组件链路结构 # RedisHub很重要的是网络代理部分,在网络代理方面由三部分组成.\n第一部分是对远程redis集群的连接池. 第二部分是对本地众多php-fpm客户端的UNIX请求连接管理. 第三部分是对这三端之间redis通信协议进行兼容. "},{"id":26,"href":"/icorer_docs/doccenter/ak-2019/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":27,"href":"/icorer_docs/doccenter/logdarts/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":28,"href":"/icorer_docs/doccenter/pulseflow/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":29,"href":"/icorer_docs/doccenter/redistun/designs/protocol/","title":"Protocol layer","section":"Designs","content":" Protocol layer design # A good access method of the application can accelerate the growth of the application ecology, and a good protocol design can reduce the transformation cost of the stock application, so the protocol layer is an important component of the IceFireDB software stack. The communication protocol of IceFireDB-NoSQL fully integrates the Redis RESP protocol, which mainly includes the following two parts of the protocol:\nData control protocol: Complete support for RESP clients, supporting functional requirements for database data access.\nCluster control protocol: Satisfy the client\u0026rsquo;s command protocol for controlling the nodes of the IceFireDB cluster and viewing the status and availability status of the cluster nodes.\n+-+---------------+----+---------------+----+---------------+-+ | Transport | | +---------------+ +---------------+ +---------------+ | | | Cluster | | Cluster | | Cluster | | | | communication | | communication | | communication | | | +---------------+ +---------------+ +---------------+ | +-+---------------+----+-------^-------+----+---------------+-+ | +------------------------------v------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +-------------------------------------------------------------+ As can be seen from the figure above, the cluster control protocol is located above the data read and write protocol. The client (redis cluster client, IceFireDB-Proxy) obtains the master-slave structure of the cluster nodes according to the cluster status, and selects the relevant master and slave nodes Perform data read and write operations. For the request traffic carried by IceFireDB, it will enter the request processing cycle. During the request processing cycle, it will analyze and optimize the client request.\nProtocol advantages # At present, the web2 ecology of RESP protocol clients is very rich, and mainstream computer languages have been fully covered. IceFireDB is firstly compatible with the RESP NoSQL protocol, which can quickly meet the needs of existing application systems to access IceFireDB-NoSQL.\nAs a decentralized database, IceFireDB is compatible with the RESP protocol, and can hide high technical intelligence from the user layer. Users do not need to understand P2P, RAFT, CRDT, IPFS-LOG and other technologies, and only need to follow the RESP protocol to choose the appropriate client for application You can operate the database to read and write, and quickly meet the access and transformation of the business system.\nData control protocol # Strings Hashes Lists Sets Sorted Sets APPEND HSET RPUSH SADD ZADD BITCOUNT HGET LPOP SCARD ZCARD BITOP HDEL LINDEX SDIFF ZCOUNT BITPOS HEXISTS LPUSH SDIFFSTORE ZREM DECR HGETALL RPOP SINTER ZCLEAR DECRBY HINCRBY LRANGE SINTERSTORE ZRANK DEL HKEYS LSET SISMEMBER ZRANGE EXISTS HLEN LLEN SMEMBERS ZREVRANGE GET HMGET RPOPLPUSH SREM ZSCORE GETBIT HMSET LCLEAR SUNION ZINCRBY SETBIT HSETEX LCLEAR SUNIONSTORE ZREVRANK GETRANGE HSTRLEN LMCLEAR SCLEAR ZRANGEBYSCORE GETSET HVALS LEXPIRE SMCLEAR ZREVRANGEBYSCORE INCR HCLEAR LEXPIREAT SEXPIRE ZREMRANGEBYSCORE EXISTS HMCLEAR LKEYEXISTS SEXPIRE ZREMRANGEBYRANK GET HEXPIRE LTRIM SEXPIREAT GETBIT HEXPIREAT LTTL STTL SETBIT HKEYEXIST SPERSIST GETRANGE HTTL SKEYEXISTS GETSET INCRBY GET MGET MSET SET SETEX SETEXAT SETRANGE EXPIRE EXPIREAT TTL Cluster control protocol # IceFireDB-NoSQL integrates some Redis cluster status instructions in the RAFT parallel database scenario.\nVERSION # show the application version MACHINE # show information about the state machine RAFT LEADER # show the address of the current raft leader RAFT INFO [pattern] # show information about the raft server and cluster RAFT SERVER LIST # show all servers in cluster RAFT SERVER ADD id address # add a server to cluster RAFT SERVER REMOVE id # remove a server from the cluster RAFT SNAPSHOT NOW # make a snapshot of the data RAFT SNAPSHOT LIST # show a list of all snapshots on server RAFT SNAPSHOT FILE id # show the file path of a snapshot on server RAFT SNAPSHOT READ id [RANGE start end] # download all or part of a snapshot "},{"id":30,"href":"/icorer_docs/doccenter/redistun/develop/redhub/","title":"redhub-frame","section":"Develop","content":" redhub-frame # Project introduction # High-performance Redis-Server multi-threaded framework, based on RawEpoll model.\nFeatures # Ultra high performance Fully multi-threaded support Low CPU resource consumption Compatible with redis protocol Create a Redis compatible server with RawEpoll model in Go Installing # go get -u github.com/IceFireDB/redhub Example # Here is a simple framework usage example,support the following redis commands:\nSET key value GET key DEL key PING QUIT You can run this example in terminal:\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;net/http\u0026#34; _ \u0026#34;net/http/pprof\u0026#34; \u0026#34;github.com/IceFireDB/redhub\u0026#34; \u0026#34;github.com/IceFireDB/redhub/pkg/resp\u0026#34; ) func main() { var mu sync.RWMutex var items = make(map[string][]byte) var network string var addr string var multicore bool var reusePort bool var pprofDebug bool var pprofAddr string flag.StringVar(\u0026amp;network, \u0026#34;network\u0026#34;, \u0026#34;tcp\u0026#34;, \u0026#34;server network (default \\\u0026#34;tcp\\\u0026#34;)\u0026#34;) flag.StringVar(\u0026amp;addr, \u0026#34;addr\u0026#34;, \u0026#34;127.0.0.1:6380\u0026#34;, \u0026#34;server addr (default \\\u0026#34;:6380\\\u0026#34;)\u0026#34;) flag.BoolVar(\u0026amp;multicore, \u0026#34;multicore\u0026#34;, true, \u0026#34;multicore\u0026#34;) flag.BoolVar(\u0026amp;reusePort, \u0026#34;reusePort\u0026#34;, false, \u0026#34;reusePort\u0026#34;) flag.BoolVar(\u0026amp;pprofDebug, \u0026#34;pprofDebug\u0026#34;, false, \u0026#34;open pprof\u0026#34;) flag.StringVar(\u0026amp;pprofAddr, \u0026#34;pprofAddr\u0026#34;, \u0026#34;:8888\u0026#34;, \u0026#34;pprof address\u0026#34;) flag.Parse() if pprofDebug { go func() { http.ListenAndServe(pprofAddr, nil) }() } protoAddr := fmt.Sprintf(\u0026#34;%s://%s\u0026#34;, network, addr) option := redhub.Options{ Multicore: multicore, ReusePort: reusePort, } rh := redhub.NewRedHub( func(c *redhub.Conn) (out []byte, action redhub.Action) { return }, func(c *redhub.Conn, err error) (action redhub.Action) { return }, func(cmd resp.Command, out []byte) ([]byte, redhub.Action) { var status redhub.Action switch strings.ToLower(string(cmd.Args[0])) { default: out = resp.AppendError(out, \u0026#34;ERR unknown command \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39;\u0026#34;) case \u0026#34;ping\u0026#34;: out = resp.AppendString(out, \u0026#34;PONG\u0026#34;) case \u0026#34;quit\u0026#34;: out = resp.AppendString(out, \u0026#34;OK\u0026#34;) status = redhub.Close case \u0026#34;set\u0026#34;: if len(cmd.Args) != 3 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.Lock() items[string(cmd.Args[1])] = cmd.Args[2] mu.Unlock() out = resp.AppendString(out, \u0026#34;OK\u0026#34;) case \u0026#34;get\u0026#34;: if len(cmd.Args) != 2 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.RLock() val, ok := items[string(cmd.Args[1])] mu.RUnlock() if !ok { out = resp.AppendNull(out) } else { out = resp.AppendBulk(out, val) } case \u0026#34;del\u0026#34;: if len(cmd.Args) != 2 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.Lock() _, ok := items[string(cmd.Args[1])] delete(items, string(cmd.Args[1])) mu.Unlock() if !ok { out = resp.AppendInt(out, 0) } else { out = resp.AppendInt(out, 1) } case \u0026#34;config\u0026#34;: // This simple (blank) response is only here to allow for the // redis-benchmark command to work with this example. out = resp.AppendArray(out, 2) out = resp.AppendBulk(out, cmd.Args[2]) out = resp.AppendBulkString(out, \u0026#34;\u0026#34;) } return out, status }, ) log.Printf(\u0026#34;started redhub server at %s\u0026#34;, addr) err := redhub.ListendAndServe(protoAddr, option, rh) if err != nil { log.Fatal(err) } } Benchmarks # Machine information OS : Debian Buster 10.6 64bit CPU : 8 CPU cores Memory : 64.0 GiB Go Version : go1.16.5 linux/amd64 【Redis-server5.0.3】 Single-threaded, no disk persistence. # $ ./redis-server --port 6380 --appendonly no $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2306060.50 requests per second GET: 3096742.25 requests per second 【Redis-server6.2.5】 Single-threaded, no disk persistence. # $ ./redis-server --port 6380 --appendonly no $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2076325.75 requests per second GET: 2652801.50 requests per second 【Redis-server6.2.5】 Multi-threaded, no disk persistence. # io-threads-do-reads yes io-threads 8 $ ./redis-server redis.conf $ redis-benchmark -h 127.0.0.1 -p 6379 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 1944692.88 requests per second GET: 2375184.00 requests per second 【RedCon】 Multi-threaded, no disk persistence # $ go run example/clone.go $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2332742.25 requests per second GET: 14654162.00 requests per second 【RedHub】 Multi-threaded, no disk persistence # $ go run example/server.go $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 4087305.00 requests per second GET: 16490765.00 requests per second "},{"id":31,"href":"/icorer_docs/doccenter/redishub/designs/1.4/","title":"1.4 协议解析器与拦截器","section":"系统设计","content":" 1.4 协议解析器与拦截器 # 作为网络中间件,我们需要针对网络数据包进行读取之外,还需要对数据包进行解析工作,基于redis协议做了解析之后,我们还需要在解析器的基础上做拦截器,针对不同的redis做不同的处理流程.\n1.4.1 协议解析器设计 # 协议解析器,是一个单独的模块,它主要针对网络数据包进行redis协议的解析工作,虽然redis协议具有统一的数据格式,但是还需要对每种协议命令做兼容操作.\n1.4.2 协议解析器构成成分 # redis协议解析器 : 主要用来解析redis协议 (decoder) redis协议编码器 : 主要用来组装redis协议 (encoder) 1.4.3 解析器 和 拦截器 的工作流程 配合 # 这两个重要组件的功能流程配合如下:\n[PHP-FPM请求] \u0026ndash;\u0026gt;** [RedisHub中间件]** \u0026ndash;\u0026gt; (读取数据包) \u0026ndash;\u0026gt; (解析数据包协议) \u0026ndash;\u0026gt; (对指定命令进行拦截操作) \u0026mdash;\u0026gt; (转发下游redis服务) \u0026ndash;\u0026gt; [下游redis集群] \u0026ndash;\u0026gt; (处理命令并响应) \u0026ndash;\u0026gt; [RedisHub中间件] \u0026mdash;\u0026gt; (响应请求) \u0026mdash;\u0026gt;** [PHP-FPM]**\n"},{"id":32,"href":"/icorer_docs/doccenter/pulseflow/architecture/codec/","title":"Codec","section":"Architecture","content":" Codec Engine Details # "},{"id":33,"href":"/icorer_docs/doccenter/redistun/designs/codec/","title":"Codec layer","section":"Designs","content":" Codec layer design # The codec layer is the glue of the IceFireDB data expression layer, because the bottom layer of IceFireDB supports many storage engines, including centralized storage such as web2 disk, OSS, leveldb, and badger, as well as web3\u0026rsquo;s IPFS, crdt-kv, and IPFS-LOG For this kind of decentralized storage, the storage interface provided by any kind of storage is simple and not standardized. The codec layer of IceFireDB-NoSQL is abstracted through a unified driver layer, and by encoding and decoding many instruction semantics into a KV model, a richer data expression layer is built to support more data scenarios, such as Strings\\Hashs\\Sets\\Lists \\Sorted Sets.\n+-------------------------------------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +---+--------------------------+--------------------------+---+ | +------------------------------v------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv、list、hash、set | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ As an important glue layer, the codec layer connects the network layer, request layer, and KV storage layer.\nCodec advantages # In the decentralized database scenario, although the community also has a decentralized storage solution similar to the KV model, the KV model cannot meet the complex use of upper-level applications. We believe that rich data structures need to be supported to support the decentralized development of applications, including NoSQL data models (such as strings\\hashs\\lists\\sets\\sorted sets data structures). Just as the prosperity of the web2 application ecology is inseparable from the contribution of database infrastructure such as memcached, redis, and mysql, IceFireDB provides richer data types and can support data decentralization in more complex scenarios.\nThe encoding and decoding layer encodes data commands and parameters to meet the conversion of many rich instruction data models to KV models. Under the function of this conversion layer, the complex shielding layer of the data model layer and the storage engine layer is effectively constructed.\nThe data instruction layer does not need to care about the underlying storage engine, so it can adapt to various storage engine drivers with the help of the codec layer.\nThe underlying storage engine continues to provide a simple data operation interface (put\\get\\del\\iterator), without direct coupling and customization with the data presentation layer, and the abstract storage driver layer of the codec layer maintains the easy work of the two layers.\nAny problem in computer science can be solved by another layer of indirection.\nCodec Layer value # In addition to adding a richer data model and isolating the complexity of the request layer and storage layer, the codec layer of IceFireDB also has the following functions:\nImprove data access speed: A memory buffer layer is added to the encoding layer to effectively improve the performance of data access. Calculate the middle layer：The support of complex data structures often requires a certain amount of calculation. At present, the coding layer mainly performs CPU calculations, and subsequent calculations can be combined with GPU and FPGA, which can expand high-performance data calculations and add more possibilities for decentralized databases. blockchain/web3 connection layer: At present, whether it is a web2 or web3 database scenario, data access is the focus of attention, but with the development of blockchain and web3, trusted computing of data and non-tamperable proofs are becoming more and more important. The codec layer of IceFireDB is playing an important role , can be used to combine the blockchain data structure to build web3 side chain infrastructure, reducing the calculation and storage burden of the blockchain layer. "},{"id":34,"href":"/icorer_docs/doccenter/redishub/designs/1.5/","title":"1.5 NetHandle组件","section":"系统设计","content":" NetHandle # 这是一个库,方便构建高性能TCP服务端应用程序,站在Go语言的巨人肩膀上\n项目地址: uriModule/NetHandle\n一. 特点 # 高性能,低资源消耗 非常简单易用的开发接口 支持众多协议,TCP,UDP,UNIX 二. 安装 # go get -u moduleUri/NetHandle\n三. 性能测试: # 3.1 50*10000 (50线程 X 10000请求) # 3.2 50*20000 (50线程 X 20000请求) # 3.3 100*10000 (100线程 X 10000请求) # 四. 样例代码: # 使用这个库的时候,只需要自定义简单的回调函数,即可构造出性能强悍的网络监听.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;moduleUri/NetHandle\u0026#34; \u0026#34;log\u0026#34; \u0026#34;sync\u0026#34; ) var addrTcp = \u0026#34;127.0.0.1:10000\u0026#34; func main() { log.SetFlags(log.Lshortfile | log.LstdFlags) var mu sync.RWMutex count := 0 go log.Printf(\u0026#34;started server at %s\u0026#34;, addrTcp) err := NetHandle.ListenAndServe(\u0026#34;tcp\u0026#34;, addrTcp, func(conn NetHandle.Conn) { requestData := make([]byte, 512) _, err := conn.NetConn().Read(requestData) if err != nil { conn.Close() return } mu.Lock() count++ mu.Unlock() countCurrent := 0 mu.RLock() countCurrent = count mu.RUnlock() replyData := fmt.Sprintf(\u0026#34;%d\\r\\n\u0026#34;, countCurrent) conn.NetConn().Write(append([]byte(replyData))) }, func(conn NetHandle.Conn) bool { // use this function to accept or deny the connection. log.Printf(\u0026#34;accept: %s\u0026#34;, conn.RemoteAddr()) return true }, func(conn NetHandle.Conn, err error) { // this is called when the connection has been closed log.Printf(\u0026#34;closed: %s, err: %v\u0026#34;, conn.RemoteAddr(), err) }, ) if err != nil { log.Fatal(err) } } "},{"id":35,"href":"/icorer_docs/doccenter/redishub/designs/1.6/","title":"1.5 集群 MSET 指令支持","section":"系统设计","content":" 集群 MSET 指令支持 # "},{"id":36,"href":"/icorer_docs/doccenter/redishub/designs/1.7/","title":"1.7 集群 MGET 指令支持","section":"系统设计","content":" 1.7 集群 MGET 指令支持 # 业务系统为了提高多个key的数据获取吞吐，经常会采用mget指令，这个指令在单实例模式的redis场景下能够完美支持，但是redis标准集群不支持跨槽位执行mget指令。\n但是mget针对业务环境是大量使用的，所以中间件必须完成mget指令在集群中的命令支持。\n1.7.1 指令转换 # 由于Redis不支持跨槽位执行mget指令，因此我们可以变相思维，总体的指令转换如下：\n针对MGET指令的keys进行拆分。 对keys进行hash计算 =\u0026gt; 选择key所对应的node 。 针对node组上的指令进行并发 pipline命令发送。 聚合各个并发线程的指令结果，把远端redis集群内部的数据返回给业务调用方。 1.7.2 MGET指令设计图 # 针对MGET的具体工作情况，我这边绘制了一个图，说明MGET的具体工作原理，红色线路代表一次完整的MGET执行周期。 从上面的图形可以看出，中间件接受到MGET请求后，会把指令中的KYES进行hash计算、并结合node信息选择数据存储的node，针对每个node进行进行pipeline 指令流水吞吐、保障单node下的吞吐高性能，针对不同的node进行并发执行，通过并发技术提高MGET在集群环境下的整体响应速度。\n"},{"id":37,"href":"/icorer_docs/doccenter/redishub/designs/1.8/","title":"1.9 Redis集群状态更新","section":"系统设计","content":" 1.9 Redis集群状态更新 # RedisHub中间件除了要沟通客户端 与 服务端 这两方，还需要及时获取Redis集群的状态信息，并把状态信息转换或更新必要的内存数据结构，内存数据结构最终会被中间件用来承载redis请求。\n1.9.1 状态信息来源 # 集群状态信息的来源主要包括：\n被动方式：集群内部指令执行周期内的MOVE、ASK信号 主动方式：集群 CLUSTER SLOTS 指令 1.9.2 集群槽位状态信息更新 # 关于集群槽位的信息更新，我们主要包括以下两方面策略：\n为了具备更好的性能，我们采用chan 共享内存监听 采用读写锁，提高关键内存共享区的读写安全性及性能 针对更新操作做了秒级限流操作，保障中间件的稳定性。 核心源码分析 如下：\nfunc (cluster *Cluster) handleUpdate() { for { //获取chan列表的更新信号 msg := \u0026lt;-cluster.updateList // TODO: control update frequency by updateTime and movedTime? cluster.rwLock.RLock() clusterLastUpdateTime := cluster.updateTime cluster.rwLock.RUnlock() //如果集群的上一次更新时间 加上窗口值（1s） 小于 此次集群更新指令产生的时间: 证明集群更新频率过高，控制频率。 if clusterLastUpdateTime.Add(1 * time.Second).Before(msg.movedTime) { //针对集群进行状态更新 err := cluster.Update(msg.node) if err != nil { log.Printf(\u0026#34;handleUpdate: %v\\n\u0026#34;, err) go KafkaLoger.Errorf(\u0026#34;redistun handleUpdate wrong. err: %s\u0026#34;, err.Error()) } } } } 1.9.2 指令MOVE 、ASK 信号跟踪 # 如果客户端在请求redis集群的过程中、redis集群出现集群槽位重新分配，会对于请求产生MOVE、ASK信号，为了让中间件支持数据的无缝迁移，我们针对MOVE、ASK进行了特殊处理。\n核心源码分析 如下：\n// 检查回复类型 resp := checkReply(reply) switch (resp) { case kRespOK, kRespError: return reply, nil case kRespMove: //此处在高并发+slots循环多次集中迁移时，会出现数据的多级别MOVE，对于多级别MOVE 要进行到底，一般频率为20万次中出现10次 //所以采用循环进行多级MOVE处理 for { //尝试第一次MOVE，并对结果进行判断，如果reply类型不再是MOVE类型，则证明摆脱多级MOVE，则把结果返回出去 //由于结果可能会发生变化，因此再进行判断 reply, err = cluster.handleMove(node, reply.(redisError).Error(), cmd, args) respType := checkReply(reply) //如果reply类型不是MOVE类型，则 准备跳出循环、对结果进行判断，选择条件返回 if respType != kRespMove { switch (respType) { case kRespOK, kRespError: return reply, nil case kRespAsk: return cluster.handleAsk(node, reply.(redisError).Error(), cmd, args) case kRespConnTimeout: return cluster.handleConnTimeout(node, cmd, args) case kRespClusterDown: //如果redis集群宕机，则返回宕机错误 //选取可用的节点 更新集群状态信息 cluster.UpdateSlotsInfoByRandomNode(node) return reply, Cluster_Down_Error } //此处return为了跳出多级MOVE的for循环 return reply, err } } //return cluster.handleMove(node, reply.(redisError).Error(), cmd, args) case kRespAsk: return cluster.handleAsk(node, reply.(redisError).Error(), cmd, args) case kRespConnTimeout: return cluster.handleConnTimeout(node, cmd, args) case kRespClusterDown: //如果redis集群宕机，则返回宕机错误 //选取可用的节点 更新集群状态信息 cluster.UpdateSlotsInfoByRandomNode(node) return reply, Cluster_Down_Error } "},{"id":38,"href":"/icorer_docs/doccenter/redishub/designs/1.9/","title":"1.8 TCP连接池","section":"系统设计","content":" 1.8 TCP连接池 # 1.8.1 连接池是什么 # 我们常见的池很多，比如内存池，线程池，对象池，连接池等。顾名思义，池子干的事情都是一样的，把一类相同的事物放到一个池里面，已备不时之需，好比我们的蓄水池一样，把平日多余的水储蓄起来，一方面防止洪水到来时候对下游造成洪涝灾害，另一方面还可以合理灌溉资源利用，比如还可以水力发电。同样连接池是把已经已经建立好的连接放入一个池子，当请求到来可以直接拿来使用，这样就即解决了频繁的创建关闭连接带来的开销，也保护了后端服务，防止同时有大量连接涌入，造成危害。\n1.8.2 连接池的种类 # 其实也就是连接池的使用场景\n可以是一个独立部署的服务，通过套接字提供代理服务。例如我们的常用的dbproxy。\n可以是一个服务内部进程间共享的连接池，这种相对更加轻量，可以理解为项目级别，只对内提供服务。\n进程内的连接池，更加轻量，当前进程内的线程或者协程可以使用。\nRedisHub 实现的连接池就是 进程内的连接池，使用连接池有以下好处：\n减少客户端使用连接时，创建和销毁连接的时间和系统资源开销，这里涉及到TCP的三次握手也四次挥手，还有TCP的慢启动预热。 避免极端情况大量连接直接涌入后端服务，对整个系统服务造成危害。 但同时也有一些缺点，比如空闲状态下也要维护一定数量的连接，占用客户端和服务端的资源，这里可以根据实际需求动态调配连接数，达到效率和资源利用的平衡。哪有一点资源不占用，还想系统高效稳定的事情，建个水坝还得占片地，护坝人间断性的职守呢。\n1.8.3 TCP连接池初始化方式 # TCP连接池的最终目标就是对程序体内部需要使用的TCP连接进行池化管理，连接不够使用时自动扩容、连接过剩的时候能够自动回收，所以 我们首先需要考虑TCP连接池的初始化方式。连接池的初始化方式主要包括如下两种：\n当请求到来的时候，尝试从连接池中获取连接对象，如果连接池为空，创建连接对象，请求结束的时候，归还至连接池.\n进程启动的时候，创建固定数量的连接对象，当请求到来的时候，尝试从连接池中获取连接对象，如果连接池为空，继续等待或者服务降级; 不为空的话正常服务，请求结束的时候，归还至连接池.\nRedisHub 中间件的TCP连接池初始化方式选择第一种。针对第二种连接池的实现方案，可以查阅本人另外一个项目RedisTun。\n1.8.4 连接池源码分析 - 获取一个可用链接 # func (node *redisNode) getConn() (*redisConn, error) { //需要针对当前的redis-node的tcp连接池进行内存操作，在并发场景下，首先先上锁。 node.mutex.Lock() //如果当前node已经进入不可用状态。 if node.closed { node.mutex.Unlock() return nil, fmt.Errorf(\u0026#34;getConn: connection has been closed\u0026#34;) } //从TCP连接池中清理陈旧的TCP连接，这里面使用了LIST数据结构，可以把陈旧连接进行归并、一并处理。 //如果连接远程node节点时候设置了TCP连接存活时间 则 进行检验。 if node.aliveTime \u0026gt; 0 { for { //从list中选择一个元素，如果conns列表为空 则跳出检查 elem := node.conns.Back() if elem == nil { break } //成功获取到一条TCP连接，进行生命期时间校验 conn := elem.Value.(*redisConn) //如果当前获取的TCP连接是在合法生命周期内部的，立刻退出，但是这个元素还在list中，下次获取仍然能够获取到 if conn.t.Add(node.aliveTime).After(time.Now()) { break } //运行到这里，代表TCP连接生命期超时，删除此元素 node.conns.Remove(elem) } } //经过前面的操作，前面目的在于清理超时TCP连接 if node.conns.Len() \u0026lt;= 0 { //没有TCP连接可用，所以需要新建连接，立刻需要释放锁 node.mutex.Unlock() c, err := net.DialTimeout(\u0026#34;tcp\u0026#34;, node.address, node.connTimeout) if err != nil { return nil, err } //var writerMemory bytes.Buffer //创建新的redis连接内存对象 conn := \u0026amp;redisConn{ c: c, br: bufio.NewReader(c), bw: bufio.NewWriter(c), readTimeout: node.readTimeout, writeTimeout: node.writeTimeout, //writerMemory: \u0026amp;writerMemory, } //设置内存缓冲区 //conn.bwm = RedSHandle.NewWriterHandle(conn.writerMemory) //conn.readerParser = RedisFastParser.NewParserHandle(conn.c) return conn, nil } //获取到一条已经存在的存活TCP连接，这条TCP的生命周期也在合法时间内，所以： // 1.取出元素 // 2.删除元素在list中的位置 // 3.立刻解锁 elem := node.conns.Back() node.conns.Remove(elem) node.mutex.Unlock() //重置内存缓冲区 //elem.Value.(*redisConn).writerMemory.Reset() return elem.Value.(*redisConn), nil } 1.8.5 连接池源码分析 - 放回可用连接进入池 # func (node *redisNode) releaseConn(conn *redisConn) { //需要针对当前的redis-node的tcp连接池进行内存操作，在并发场景下，首先先上锁。 node.mutex.Lock() defer node.mutex.Unlock() //连接仍然有待处理的回复，只需将其关闭即可，避免可能的TCP粘包连接。 if conn.pending \u0026gt; 0 || node.closed { conn.shutdown() return } //如果连接池的当前长度已经超过池的最高界限，或者node没有开启tcp存活时间选项。 if node.conns.Len() \u0026gt;= node.keepAlive || node.aliveTime \u0026lt;= 0 { conn.shutdown() return } //更新当前conn的时间，并放入LIST数据结构 conn.t = time.Now() node.conns.PushFront(conn) //重置内存缓冲区 //conn.writerMemory.Reset() } "},{"id":39,"href":"/icorer_docs/doccenter/redishub/designs/1.10/","title":"1.10 RESP协议解析","section":"系统设计","content":" 1.10 RESP协议解析 # 1.10.1 RESP协议介绍 # Redis 协议在以下三个目标之间进行折中：\n易于实现 可以高效地被计算机分析（parse） 可以很容易地被人类读懂 1.10.2 请求 # Redis 服务器接受命令以及命令的参数。\n服务器会在接到命令之后，对命令进行处理，并将命令的回复传送回客户端。\n1.10.3 回复 # Redis 命令会返回多种不同类型的回复。\n通过检查服务器发回数据的第一个字节， 可以确定这个回复是什么类型：\n状态回复（status reply）的第一个字节是 \u0026ldquo;+\u0026rdquo;\n错误回复（error reply）的第一个字节是 \u0026ldquo;-\u0026rdquo;\n整数回复（integer reply）的第一个字节是 \u0026ldquo;:\u0026rdquo;\n批量回复（bulk reply）的第一个字节是 \u0026ldquo;$\u0026rdquo;\n多条批量回复（multi bulk reply）的第一个字节是 \u0026ldquo;*\u0026rdquo;\n1.10.4 状态回复 # 一个状态回复（或者单行回复，single line reply）是一段以 \u0026ldquo;+\u0026rdquo; 开始、 \u0026ldquo;\\r\\n\u0026rdquo; 结尾的单行字符串。\n以下是一个状态回复的例子：\n+OK\n客户端库应该返回 \u0026ldquo;+\u0026rdquo; 号之后的所有内容。 比如在在上面的这个例子中， 客户端就应该返回字符串 \u0026ldquo;OK\u0026rdquo; 。\n状态回复通常由那些不需要返回数据的命令返回，这种回复不是二进制安全的，它也不能包含新行。\n状态回复的额外开销非常少，只需要三个字节（开头的 \u0026ldquo;+\u0026rdquo; 和结尾的 CRLF）。\n1.10.5 错误回复 # 错误回复和状态回复非常相似， 它们之间的唯一区别是， 错误回复的第一个字节是 \u0026ldquo;-\u0026rdquo; ， 而状态回复的第一个字节是 \u0026quot;+\u0026quot; 。\n错误回复只在某些地方出现问题时发送： 比如说， 当用户对不正确的数据类型执行命令， 或者执行一个不存在的命令， 等等。\n一个客户端库应该在收到错误回复时产生一个异常。\n以下是两个错误回复的例子：\n-ERR unknown command \u0026lsquo;foobar\u0026rsquo; -WRONGTYPE Operation against a key holding the wrong kind of value\n在 \u0026quot;-\u0026quot; 之后，直到遇到第一个空格或新行为止，这中间的内容表示所返回错误的类型。\nERR 是一个通用错误，而 WRONGTYPE 则是一个更特定的错误。 一个客户端实现可以为不同类型的错误产生不同类型的异常， 或者提供一种通用的方式， 让调用者可以通过提供字符串形式的错误名来捕捉（trap）不同的错误。\n不过这些特性用得并不多， 所以并不是特别重要， 一个受限的（limited）客户端可以通过简单地返回一个逻辑假（false）来表示一个通用的错误条件。\n1.10.6 整数回复 # 整数回复就是一个以 \u0026quot;:\u0026quot; 开头， CRLF 结尾的字符串表示的整数。\n比如说， \u0026quot;:0\\r\\n\u0026quot; 和 \u0026quot;:1000\\r\\n\u0026quot; 都是整数回复。\n1.10.7 RESP协议解析- 源码分析 # 这里首先感谢Codis 和 Redigo 这两大开源项目， 这里只进行概括性源码解析。\nfunc (d *Decoder) decodeResp() (*Resp, error) { b, err := d.br.ReadByte() //从缓冲区读取1byte，这个是RESP首特征byte if err != nil { return nil, errors.Trace(err) } r := \u0026amp;Resp{} r.Type = RespType(b) switch r.Type { default: //如果不在可控范围，则代表请求不是RESP协议 return nil, errors.Errorf(\u0026#34;bad resp type %s\u0026#34;, r.Type) case TypeString, TypeError, TypeInt: //如果是字符串、错误、整形响应 r.Value, err = d.decodeTextBytes() case TypeBulkBytes: //如果是Bulk类型 r.Value, err = d.decodeBulkBytes() case TypeArray: //如果数据是数组类型 r.Array, err = d.decodeArray() } return r, err } //解析非二进制安全的字符串、错误、整形响应，必须按照\\r\\n分割。 func (d *Decoder) decodeTextBytes() ([]byte, error) { b, err := d.br.ReadBytes(\u0026#39;\\n\u0026#39;) if err != nil { return nil, errors.Trace(err) } if n := len(b) - 2; n \u0026lt; 0 || b[n] != \u0026#39;\\r\u0026#39; { return nil, errors.Trace(ErrBadCRLFEnd) } else { return b[:n], nil } } //解析数组类型响应。 func (d *Decoder) decodeArray() ([]*Resp, error) { //解析数组长度 n, err := d.decodeInt() if err != nil { return nil, err } switch { case n \u0026lt; -1: return nil, errors.Trace(ErrBadArrayLen) case n \u0026gt; MaxArrayLen: return nil, errors.Trace(ErrBadArrayLenTooLong) case n == -1: return nil, nil } //根据数组长度创建RESP数组 array := make([]*Resp, n) for i := range array { //针对每个数组元素进行解析，此处类似递归调用，但是，借助go的栈逃逸、堆区内存分配，可以避免循环栈消耗。 r, err := d.decodeResp() if err != nil { return nil, err } array[i] = r } return array, nil } "},{"id":40,"href":"/icorer_docs/doccenter/redistun/quick_start/","title":"Quick Start","section":"RedisTun","content":" Quick Start Guide for the IceFireDB Database # This guide walks you through the quickest way to get started with IceFireDB. For non-production environments, you can deploy your IceFireDB database by either of the following methods:\nDeploy a local test cluster，Simulate production deployment on a single machine Deploy a local test cluster # Scenario: Quickly deploy a local IceFireDB cluster for testing using a single macOS or Linux server. As a distributed system, in the same availability zone network, a basic IceFireDB cluster usually consists of 3 IceFireDB instances.\n1.Download and compile the program # git clone https://github.com/IceFireDB/IceFireDB.git IceFireDB-NoSQL cd IceFireDB-NoSQL make \u0026amp;\u0026amp; ls ./bin/IceFireDB If the following message is displayed, you have build IceFireDB-NoSQL successfully:\nif [ ! -d \u0026#34;./bin/\u0026#34; ]; then \\ mkdir bin; \\ fi go build -ldflags \u0026#34;-s -w -X \\\u0026#34;main.BuildVersion=1c102f3\\\u0026#34; -X \\\u0026#34;main.BuildDate=2022-11-21 06:17:29\\\u0026#34;\u0026#34; -o bin/IceFireDB . ./bin/IceFireDB 2.Declare the global environment variable # IceFireDB implements many engines at the bottom, mainly including the following categories. The choice of the bottom engine is initialized through cmd variables.\nEngine type cmd key cmd value LevelDB storage-backend goleveldb Badger storage-backend badger IPFS storage-backend ipfs CRDT-KV storage-backend crdt IPFS-LOG storage-backend ipfs-log OrbitDB storage-backend orbitdb OSS storage-backend oss 3.Start the cluster in the current session # mkdir 6001 \u0026amp;\u0026amp; mkdir 6002 \u0026amp;\u0026amp; mkdir 6003 cp ./bin/IceFireDB ./6001 cp ./bin/IceFireDB ./6002 cp ./bin/IceFireDB ./6003 # start node1 /pwd/IceFireDB-NoSQL/6001/IceFireDB -storage-backend ipfs-log -n 1 -a 127.0.0.1:6001 --openreads # start node2 /pwd/IceFireDB-NoSQL/6002/IceFireDB -storage-backend ipfs-log -n 2 -a 127.0.0.1:6002 -j 127.0.0.1:6001 --openreads # start node3 /pwd/IceFireDB-NoSQL/6003/IceFireDB -storage-backend ipfs-log -n 3 -a 127.0.0.1:6003 -j 127.0.0.1:6001 --openreads In the same network availability zone, multiple IceFireDB instances can be added to the same raft network, and the same raft network exposes the standard Redis cluster access interface to meet the access requirements of the Redis client.\n4.Start a new session to access IceFireDB # The above steps start three IceFireDB nodes and form a highly available network with each other.We can use redis-cli to observe the cluster status\nsudo apt-get -y install redis-tools redis-cli cluster nodes We execute the cluster nodes command in the redis-cli terminal, and we can view the cluster status as follows:\n127.0.0.1:6002\u0026gt; cluster nodes 356a192b7913b04c54574d18c28d46e6395428ab 127.0.0.1:6001@6001 slave 77de68daecd823babbb58edb1c8e14d7106e83bb 0 0 connected 0-16383 da4b9237bacccdf19c0760cab7aec4a8359010b0 127.0.0.1:6002@6002 slave 77de68daecd823babbb58edb1c8e14d7106e83bb 0 0 connected 0-16383 77de68daecd823babbb58edb1c8e14d7106e83bb 127.0.0.1:6003@6003 master - 0 0 connected 0-16383 We use redis-cli for data read and write tests：\nredis-cli -c -h 127.0.0.1 -p 6002 127.0.0.1:6002\u0026gt; set foo bar -\u0026gt; Redirected to slot [0] located at 127.0.0.1:6003 OK 127.0.0.1:6003\u0026gt; get foo \u0026#34;bar\u0026#34; We can see that the data can be read and written normally，The current master is an instance of 6003. Since we have enabled the read data permission of all nodes, we can view data in other slave nodes.\nredis-cli -h 127.0.0.1 -p 6001 127.0.0.1:6001\u0026gt; get foo \u0026#34;bar\u0026#34; # Although we can read data in the slave node, we cannot write data directly on the slave node. 127.0.0.1:6001\u0026gt; set foo2 bar2 (error) MOVED 0 127.0.0.1:6003 Advanced Eco Tools # IceFireDB-Proxy: Intelligent network proxy # In the above case, we fully demonstrated the cluster construction and data read-write access, but the master-slave relationship between high-availability nodes, data read-write fault tolerance of the client, and the perception of the status of each node in the cluster are complex, so We launched the IceFireDB-Proxy software, which can shield users from understanding the complexity of the IceFireDB high-availability cluster, and use the IceFireDB cluster like a single IceFireDB.\n"},{"id":41,"href":"/icorer_docs/doccenter/redistun/designs/","title":"Designs","section":"RedisTun","content":" System Design # In order to build a decentralized database, the core of the IceFireDB system is to provide data decentralization and immutability for applications. Aiming at the above goals, we have designed the following core system levels.\n+-+---------------+----+---------------+----+---------------+-+ | Transport | | +---------------+ +---------------+ +---------------+ | | | Cluster | | Cluster | | Cluster | | | | communication | | communication | | communication | | | +---------------+ +---------------+ +---------------+ | +-+---------------+----+-------^-------+----+---------------+-+ | +------------------------------v------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +---+--------------------------+--------------------------+---+ | +------------------------------v------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv、list、hash、set | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ At the above system level, IceFireDB refines and implements the following important system components.\nSystem components describe technology used Network layer 1. RAFT guarantees data consistency within a single availability zone. 2. P2P network construction decentralized database communication. 3. NATS is a new network layer being built. P2P、RAFT、NATS Storage layer Many types of storage are currently supported. Under the codec computing layer, we abstract the KV storage driver layer, which is compatible with different storage engines of web2 and web3. goleveldb、badger、IPFS、CRDT、IPFS-LOG、OSS Protocol layer Based on the codec layer, we have built a protocol layer. A good communication protocol allows more applications to easily access the IceFireDB data network. Currently, we support the Redis-RESP NoSQL protocol and the MySQL protocol. RESP、SQL Codec layer The codec layer is the core of our system. For NoSQL scenarios, any data type will be abstracted into a KV storage model. With the flexible coding layer, we can build rich data operation structures and instructions, such as hash, sets, strings, etc. KV、Strings、Hashes、Lists、Sorted Sets、Sets、SQL、PubSub "},{"id":42,"href":"/icorer_docs/doccenter/redishub/designs/","title":"系统设计","section":"RedisHub","content":" 一. 总体设计 # RedisHub在组件设计上分为以下几部分:\nAgent配置解析器：负责对于配置文件进行解析（后续增加统一配置中心的支持） 通信组件: 包括UNIX本地监听组 和 远端Redis集群TCP长连接及连接池。 协议分析器: 提供稳定的Redis协议解析及组装功能组件。 协议拦截器: 主要对某些Redis命令进行拦截,调用协议插件组进行功能扩展. 协议插件组: 为协议分析器添加一系列插件,对通信进行优化和功能扩展，例如集群的mset、mget、del操作。 容灾器: 为Agent运行提供必要的安全保障,主要包括进程资源监控,迭代更新监控. 具体的组件总体架构如下图: "},{"id":43,"href":"/icorer_docs/doccenter/redistun/tutorials/","title":"Tutorials","section":"RedisTun","content":" Tutorials # "},{"id":44,"href":"/icorer_docs/doccenter/","title":"项目总览","section":"工匠之芯-文档中心","content":" 项目总览 # RedisHub PulseFlow "},{"id":45,"href":"/icorer_docs/doccenter/redistun/deploy/","title":"Deploy","section":"RedisTun","content":" Deploy # "},{"id":46,"href":"/icorer_docs/doccenter/redishub/system_access/","title":"系统接入","section":"RedisHub","content":" Tutorials # "}]